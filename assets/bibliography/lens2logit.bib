@article{taqi2018review,
  title={A review of artifacts in histopathology},
  author={Taqi, Syed Ahmed and Sami, Syed Abdus and Sami, Lateef Begum and Zaki, Syed Ahmed},
  journal={Journal of oral and maxillofacial pathology: JOMFP},
  volume={22},
  number={2},
  pages={279},
  year={2018},
  publisher={Wolters Kluwer--Medknow Publications}
}

@article{chlap2021review,
  title={A review of medical image data augmentation techniques for deep learning applications},
  author={Chlap, Phillip and Min, Hang and Vandenberg, Nym and Dowling, Jason and Holloway, Lois and Haworth, Annette},
  journal={Journal of Medical Imaging and Radiation Oncology},
  year={2021},
  publisher={Wiley Online Library}
}

@misc{dietrich2021machine,
      title={Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis}, 
      author={Maximilian Dietrich and Silvia Seidlitz and Nicholas Schreck and Manuel Wiesenfarth and Patrick Godau and Minu Tizabi and Jan Sellner and Sebastian Marx and Samuel Knödler and Michael M. Allers and Leonardo Ayala and Karsten Schmidt and Thorsten Brenner and Alexander Studier-Fischer and Felix Nickel and Beat P. Müller-Stich and Annette Kopp-Schneider and Markus A. Weigand and Lena Maier-Hein},
      year={2021},
      eprint={2106.08445},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@book{kandel2000principles,
  title={Principles of neural science},
  author={Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Siegelbaum, Steven and Hudspeth, A James and Mack, Sarah},
  volume={4},
  year={2000},
  publisher={McGraw-hill New York}
}

@article{felleman1991distributed,
  title={Distributed hierarchical processing in the primate cerebral cortex.},
  author={Felleman, Daniel J and Van Essen, David C},
  journal={Cerebral cortex (New York, NY: 1991)},
  volume={1},
  number={1},
  pages={1--47},
  year={1991}
}

@article{zheng2014marginal,
  title={Marginal space learning for medical image analysis},
  author={Zheng, Yefeng and Comaniciu, Dorin},
  journal={Springer},
  volume={2},
  number={3},
  pages={6},
  year={2014},
  publisher={Springer}
}

@article{marcus2018deep,
  title={Deep learning: A critical appraisal},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:1801.00631},
  year={2018}
}

@article{mualla2013automatic,
  title={Automatic cell detection in bright-field microscope images using SIFT, random forests, and hierarchical clustering},
  author={Mualla, Firas and Sch{\"o}ll, Simon and Sommerfeldt, Bj{\"o}rn and Maier, Andreas and Hornegger, Joachim},
  journal={IEEE transactions on medical imaging},
  volume={32},
  number={12},
  pages={2274--2286},
  year={2013},
  publisher={IEEE}
}

@article{bellmund2018navigating,
  title={Navigating cognition: Spatial codes for human thinking},
  author={Bellmund, Jacob LS and G{\"a}rdenfors, Peter and Moser, Edvard I and Doeller, Christian F},
  journal={Science},
  volume={362},
  number={6415},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={201--208},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{krauss2019recurrence,
  title={Recurrence resonance” in three-neuron motifs},
  author={Krauss, Patrick and Prebeck, Karin and Schilling, Achim and Metzner, Claus},
  journal={Frontiers in computational neuroscience},
  volume={13},
  pages={64},
  year={2019},
  publisher={Frontiers}
}

@article{benzi1982stochastic,
  title={Stochastic resonance in climatic change},
  author={Benzi, Roberto and Parisi, Giorgio and Sutera, Alfonso and Vulpiani, Angelo},
  journal={Tellus},
  volume={34},
  number={1},
  pages={10--16},
  year={1982},
  publisher={Wiley Online Library}
}

@article{krauss2016stochastic,
  title={Stochastic resonance controlled upregulation of internal noise after hearing loss as a putative cause of tinnitus-related neuronal hyperactivity},
  author={Krauss, Patrick and Tziridis, Konstantin and Metzner, Claus and Schilling, Achim and Hoppe, Ulrich and Schulze, Holger},
  journal={Frontiers in neuroscience},
  volume={10},
  pages={597},
  year={2016},
  publisher={Frontiers}
}

@article{krauss2018cross,
  title={Cross-modal stochastic resonance as a universal principle to enhance sensory processing},
  author={Krauss, Patrick and Tziridis, Konstantin and Schilling, Achim and Schulze, Holger},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={578},
  year={2018},
  publisher={Frontiers}
}

@article{krauss2017adaptive,
  title={Adaptive stochastic resonance for unknown and variable input signals},
  author={Krauss, Patrick and Metzner, Claus and Schilling, Achim and Sch{\"u}tz, Christian and Tziridis, Konstantin and Fabry, Ben and Schulze, Holger},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{mcdonnell2009stochastic,
  title={What is stochastic resonance? Definitions, misconceptions, debates, and its relevance to biology},
  author={McDonnell, Mark D and Abbott, Derek},
  journal={PLoS computational biology},
  volume={5},
  number={5},
  pages={e1000348},
  year={2009},
  publisher={Public Library of Science San Francisco, USA}
}

@article{moss2004stochastic,
  title={Stochastic resonance and sensory information processing: a tutorial and review of application},
  author={Moss, Frank and Ward, Lawrence M and Sannita, Walter G},
  journal={Clinical neurophysiology},
  volume={115},
  number={2},
  pages={267--281},
  year={2004},
  publisher={Elsevier}
}

@article{krauss2021simulated,
  title={Simulated transient hearing loss improves auditory sensitivity},
  author={Krauss, Patrick and Tziridis, Konstantin},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--8},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{schilling2020intrinsic,
  title={Intrinsic noise improves speech recognition in a computational model of the auditory pathway},
  author={Schilling, Achim and Gerum, Richard and Zankl, Alexandra and Schulze, Holger and Metzner, Claus and Krauss, Patrick},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{park2020map,
  title={Map making: constructing, combining, and inferring on abstract cognitive maps},
  author={Park, Seongmin A and Miller, Douglas S and Nili, Hamed and Ranganath, Charan and Boorman, Erie D},
  journal={Neuron},
  volume={107},
  number={6},
  pages={1226--1238},
  year={2020},
  publisher={Elsevier}
}

@article{momennejad2020learning,
  title={Learning structures: Predictive representations, replay, and generalization},
  author={Momennejad, Ida},
  journal={Current Opinion in Behavioral Sciences},
  volume={32},
  pages={155--166},
  year={2020},
  publisher={Elsevier}
}

@article{miller2012prolonged,
  title={Prolonged myelination in human neocortical evolution},
  author={Miller, Daniel J and Duka, Tetyana and Stimpson, Cheryl D and Schapiro, Steven J and Baze, Wallace B and McArthur, Mark J and Fobbs, Archibald J and Sousa, Andr{\'e} MM and {\v{S}}estan, Nenad and Wildman, Derek E and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={41},
  pages={16480--16485},
  year={2012},
  publisher={National Acad Sciences}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{krauss2020will,
  title={Will we ever have Conscious Machines?},
  author={Krauss, Patrick and Maier, Andreas},
  journal={Frontiers in computational neuroscience},
  volume={14},
  pages={116},
  year={2020},
  publisher={Frontiers}
}

@article{stachenfeld2017hippocampus,
  title={The hippocampus as a predictive map},
  author={Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  journal={Nature neuroscience},
  volume={20},
  number={11},
  pages={1643--1653},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{rosenblatt1960perceptron,
  title={Perceptron simulation experiments},
  author={Rosenblatt, Frank},
  journal={Proceedings of the IRE},
  volume={48},
  number={3},
  pages={301--309},
  year={1960},
  publisher={IEEE}
}

@article{maier2019gentle,
  title={A gentle introduction to deep learning in medical image processing},
  author={Maier, Andreas and Syben, Christopher and Lasser, Tobias and Riess, Christian},
  journal={Zeitschrift f{\"u}r Medizinische Physik},
  volume={29},
  number={2},
  pages={86--101},
  year={2019},
  publisher={Elsevier}
}

@article{box1976science,
  title={Science and statistics},
  author={Box, George EP},
  journal={Journal of the American Statistical Association},
  volume={71},
  number={356},
  pages={791--799},
  year={1976},
  publisher={Taylor \& Francis}
}

@book{maier2018medical,
  title={Medical imaging systems: An introductory guide},
  author={Maier, Andreas and Steidl, Stefan and Christlein, Vincent and Hornegger, Joachim},
  year={2018},
  publisher={Springer}
}

@inproceedings{meister2018towards,
  title={Towards fast biomechanical modeling of soft tissue using neural networks},
  author={Meister, Felix and Passerini, Tiziano and Mihalef, Viorel and Tuysuzoglu, Ahmet and Maier, Andreas and Mansi, Tommaso},
  booktitle={Medical Imaging meets NeurIPS Workshop, NeurIPS 2018},
  pages={no pagination},
  year={2018},
  organization={MED-NEURIPS}
}
@book{hart2000pattern,
  title={Pattern classification},
  author={Hart, Peter E and Stork, David G and Duda, Richard O},
  year={2000},
  publisher={Wiley Hoboken}
}

@book{hamming2012numerical,
  title={Numerical methods for scientists and engineers},
  author={Hamming, Richard},
  year={2012},
  publisher={Courier Corporation}
}

@article{simidjievski2020equation,
  title={Equation discovery for nonlinear system identification},
  author={Simidjievski, Nikola and Todorovski, Ljup{\v{c}}o and Kocijan, Ju{\v{s}} and D{\v{z}}eroski, Sa{\v{s}}o},
  journal={IEEE Access},
  volume={8},
  pages={29930--29943},
  year={2020},
  publisher={IEEE}
}

@ARTICLE{8633930,
  author={Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R. and Guttag, John and Dalca, Adrian V.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={VoxelMorph: A Learning Framework for Deformable Medical Image Registration}, 
  year={2019},
  volume={38},
  number={8},
  pages={1788-1800},
  doi={10.1109/TMI.2019.2897538}}


@ARTICLE{Schlemper2018,
  author={Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony N. and Rueckert, Daniel},
  journal={IEEE Transactions on Medical Imaging}, 
  title={A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image Reconstruction}, 
  year={2018},
  volume={37},
  number={2},
  pages={491-503},
  doi={10.1109/TMI.2017.2760978}}


@inproceedings{NEURIPS2020_43e4e6a6,
 author = {Um, Kiwon and Brand, Robert and Fei, Yun (Raymond) and Holl, Philipp and Thuerey, Nils},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {6111--6122},
 publisher = {Curran Associates, Inc.},
 title = {Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers},
 url = {https://proceedings.neurips.cc/paper/2020/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf},
 volume = {33},
 year = {2020}
}


@misc{biamonte2017tensor,
      title={Tensor Networks in a Nutshell}, 
      author={Jacob Biamonte and Ville Bergholm},
      year={2017},
      eprint={1708.00006},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@inproceedings{selvan2020tensor,
  title={Tensor networks for medical image classification},
  author={Selvan, Raghavendra and Dam, Erik B},
  booktitle={Medical Imaging with Deep Learning},
  pages={721--732},
  year={2020},
  organization={PMLR}
}

@inproceedings{yang2020d3vo,
  title={D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry},
  author={Yang, Nan and Stumberg, Lukas von and Wang, Rui and Cremers, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1281--1292},
  year={2020}
}

@misc{bronstein2021geometric,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{manzanera2021patient,
  title={Patient-Specific 3d Cellular Automata Nodule Growth Synthesis In Lung Cancer Without The Need Of External Data},
  author={Manzanera, Octavio E Martinez and Ellis, Sam and Baltatzis, Vasileios and Nair, Arjun and Le Folgoc, Loic and Desai, Sujal and Glocker, Ben and Schnabel, Julia A},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  pages={5--9},
  year={2021},
  organization={IEEE}
}

@inproceedings{tewari2020state,
  title={State of the art on neural rendering},
  author={Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and Martin-Brualla, Ricardo and Simon, Tomas and Saragih, Jason and Nie{\ss}ner, Matthias and others},
  booktitle={Computer Graphics Forum},
  volume={39},
  number={2},
  pages={701--727},
  year={2020},
  organization={Wiley Online Library}
}

@article{schwab2019deep,
  title={Deep null space learning for inverse problems: convergence analysis and rates},
  author={Schwab, Johannes and Antholzer, Stephan and Haltmeier, Markus},
  journal={Inverse Problems},
  volume={35},
  number={2},
  pages={025008},
  year={2019},
  publisher={IOP Publishing}
}

@article{li2020nett,
  title={NETT: Solving inverse problems with deep neural networks},
  author={Li, Housen and Schwab, Johannes and Antholzer, Stephan and Haltmeier, Markus},
  journal={Inverse Problems},
  volume={36},
  number={6},
  pages={065005},
  year={2020},
  publisher={IOP Publishing}
}

@article{lim2020cyclegan,
  title={Cyclegan with a blur kernel for deconvolution microscopy: Optimal transport geometry},
  author={Lim, Sungjun and Park, Hyoungjun and Lee, Sang-Eun and Chang, Sunghoe and Sim, Byeongsu and Ye, Jong Chul},
  journal={IEEE Transactions on Computational Imaging},
  volume={6},
  pages={1127--1138},
  year={2020},
  publisher={IEEE}
}

@article{oh2020unpaired,
  title={Unpaired deep learning for accelerated MRI using optimal transport driven cycleGAN},
  author={Oh, Gyutaek and Sim, Byeongsu and Chung, HyungJin and Sunwoo, Leonard and Ye, Jong Chul},
  journal={IEEE Transactions on Computational Imaging},
  volume={6},
  pages={1285--1296},
  year={2020},
  publisher={IEEE}
}

@inproceedings{chen2020mri,
  title={Mri image reconstruction via learning optimization using neural odes},
  author={Chen, Eric Z and Chen, Terrence and Sun, Shanhui},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={83--93},
  year={2020},
  organization={Springer}
}

@inproceedings{zhang2020deep,
  title={Deep active contour network for medical image segmentation},
  author={Zhang, Mo and Dong, Bin and Li, Quanzheng},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={321--331},
  year={2020},
  organization={Springer}
}

@inproceedings{wickramasinghe2020voxel2mesh,
  title={Voxel2mesh: 3d mesh model generation from volumetric data},
  author={Wickramasinghe, Udaranga and Remelli, Edoardo and Knott, Graham and Fua, Pascal},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={299--308},
  year={2020},
  organization={Springer}
}

@article{cranmer2019learning,
  title={Learning symbolic physics with graph networks},
  author={Cranmer, Miles D and Xu, Rui and Battaglia, Peter and Ho, Shirley},
  journal={arXiv preprint arXiv:1909.05862},
  year={2019}
}

@inproceedings{yang2020extreme,
  title={Extreme relative pose network under hybrid representations},
  author={Yang, Zhenpei and Yan, Siming and Huang, Qixing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2455--2464},
  year={2020}
}

@inproceedings{rematas2020neural,
  title={Neural voxel renderer: Learning an accurate and controllable rendering tool},
  author={Rematas, Konstantinos and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5417--5427},
  year={2020}
}

@inproceedings{dai2020neural,
  title={Neural point cloud rendering via multi-plane projection},
  author={Dai, Peng and Zhang, Yinda and Li, Zhuwen and Liu, Shuaicheng and Zeng, Bing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7830--7839},
  year={2020}
}

@article{tukuljac2019spectrobank,
  title={SpectroBank: A filter-bank convolutional layer for CNN-based audio applications},
  author={Tukuljac, Helena Peic and Ricaud, Benjamin and Aspert, Nicolas and Vandergheynst, Pierre},
  year={2019}
}

@inproceedings{schroter2020clcnet,
  title={Clcnet: Deep learning-based noise reduction for hearing aids using complex linear coding},
  author={Schr{\"o}ter, Hendrik and Rosenkranz, Tobias and Escalante-B, Alberto N and Aubreville, Marc and Maier, Andreas},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6949--6953},
  year={2020},
  organization={IEEE}
}

@article{engel2020ddsp,
  title={DDSP: Differentiable digital signal processing},
  author={Engel, Jesse and Hantrakul, Lamtharn and Gu, Chenjie and Roberts, Adam},
  journal={arXiv preprint arXiv:2001.04643},
  year={2020}
}

@article{willard2020integrating,
  title={Integrating physics-based modeling with machine learning: A survey},
  author={Willard, Jared and Jia, Xiaowei and Xu, Shaoming and Steinbach, Michael and Kumar, Vipin},
  journal={arXiv preprint arXiv:2003.04919},
  year={2020}
}

@article{filan2020pruned,
  title={Pruned neural networks are surprisingly modular},
  author={Filan, Daniel and Hod, Shlomi and Wild, Cody and Critch, Andrew and Russell, Stuart},
  journal={arXiv preprint arXiv:2003.04881},
  year={2020}
}

@article{li2021kohn,
  title={Kohn-Sham equations as regularizer: Building prior knowledge into machine-learned physics},
  author={Li, Li and Hoyer, Stephan and Pederson, Ryan and Sun, Ruoxi and Cubuk, Ekin D and Riley, Patrick and Burke, Kieron and others},
  journal={Physical review letters},
  volume={126},
  number={3},
  pages={036401},
  year={2021},
  publisher={APS}
}

@misc{hutson2018has,
  title={Has artificial intelligence become alchemy?},
  author={Hutson, Matthew},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{gencoglu2019hark,
  title={HARK Side of Deep Learning--From Grad Student Descent to Automated Machine Learning},
  author={Gencoglu, Oguzhan and van Gils, Mark and Guldogan, Esin and Morikawa, Chamin and S{\"u}zen, Mehmet and Gruber, Mathias and Leinonen, Jussi and Huttunen, Heikki},
  journal={arXiv preprint arXiv:1904.07633},
  year={2019}
}


@article{singh2017learning,
  title={Learning to play Go from scratch},
  author={Singh, Satinder and Okun, Andy and Jackson, Andrew},
  journal={Nature},
  volume={550},
  number={7676},
  pages={336--337},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{christlein2018encoding,
  title={Encoding CNN activations for writer recognition},
  author={Christlein, Vincent and Maier, Andreas},
  booktitle={2018 13th IAPR International Workshop on Document Analysis Systems (DAS)},
  pages={169--174},
  year={2018},
  organization={IEEE}
}

@article{yang2019self,
  title={Self-imitating feedback generation using GAN for computer-assisted pronunciation training},
  author={Yang, Seung Hee and Chung, Minhwa},
  journal={arXiv preprint arXiv:1904.09407},
  year={2019}
}

@inproceedings{yang2018introducing,
  title={Introducing smart pillow using actuator mechanism, pressure sensors, and deep learning-based asr},
  author={Yang, Seung Hee and Park, Sangwoo and Yang, Taemyung and Jin, Ilhyung and Kim, Wooil and Liu, Chingwei and Kim, Seong-Woo and Eune, Juhyun},
  booktitle={Proceedings of the 9th Augmented Human International Conference},
  pages={1--2},
  year={2018}
}

@article{crego2016systran,
  title={Systran's pure neural machine translation systems},
  author={Crego, Josep and Kim, Jungi and Klein, Guillaume and Rebollo, Anabel and Yang, Kathy and Senellart, Jean and Akhanov, Egor and Brunelle, Patrice and Coquard, Aurelien and Deng, Yongchao and others},
  journal={arXiv preprint arXiv:1610.05540},
  year={2016}
}

@article{barron2018approximation,
  title={Approximation and estimation for high-dimensional deep learning networks},
  author={Barron, Andrew R and Klusowski, Jason M},
  journal={arXiv preprint arXiv:1809.03090},
  year={2018}
}

@article{barron1994approximation,
  title={Approximation and estimation bounds for artificial neural networks},
  author={Barron, Andrew R},
  journal={Machine learning},
  volume={14},
  number={1},
  pages={115--133},
  year={1994},
  publisher={Springer}
}


@misc{sutton2019bitter,
  title={The Bitter Lesson,},
  author={Sutton, Richard},
  url={http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  year={2019},
  note = {Accessed: 2021-07-27}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@misc{maier2019bittersweet,
  title={Artificial Intelligence — A Bitter-Sweet Symphony in Modelling,},
  author={Maier, Andreas},
  url={https://towardsdatascience.com/artificial-intelligence-a-bitter-sweet-symphony-in-modelling-bc7107225b44},
  year={2019},
  note = {Accessed: 2021-07-27}
}

@misc{Aggarwal2018birth,
  title={The Birth of AI and The First AI Hype Cycle,},
  author={Alok Aggarwal},
  url={https://www.kdnuggets.com/2018/02/birth-ai-first-hype-cycle.html},
  year={2018},
  note = {Accessed: 2021-07-27}
}

@article{nguyen2018rendernet,
  title={Rendernet: A deep convolutional network for differentiable rendering from 3d shapes},
  author={Nguyen-Phuoc, Thu and Li, Chuan and Balaban, Stephen and Yang, Yong-Liang},
  journal={arXiv preprint arXiv:1806.06575},
  year={2018}
}

@inproceedings{syben2018deriving,
  title={Deriving Neural Network Architectures using Precision Learning: Parallel-to-fan beam Conversion},
  author={Syben, Christopher and Stimpel, Bernhard and Lommen, Jonathan and W{\"u}rfl, Tobias and D{\"o}rfler, Arnd and Maier, Andreas},
  booktitle={German Conference on Pattern Recognition (GCPR)},
  year={2018}
}

@article{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  journal={arXiv preprint},
  year={2017}
}

@InProceedings{huang2018considerations,
author="Huang, Yixing
and W{\"u}rfl, Tobias
and Breininger, Katharina
and Liu, Ling
and Lauritsch, G{\"u}nter
and Maier, Andreas",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="Some Investigations on Robustness of Deep Learning in Limited Angle Tomography",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="145--153",
abstract="In computed tomography, image reconstruction from an insufficient angular range of projection data is called limited angle tomography. Due to missing data, reconstructed images suffer from artifacts, which cause boundary distortion, edge blurring, and intensity biases. Recently, deep learning methods have been applied very successfully to this problem in simulation studies. However, the robustness of neural networks for clinical applications is still a concern. It is reported that most neural networks are vulnerable to adversarial examples. In this paper, we aim to investigate whether some perturbations or noise will mislead a neural network to fail to detect an existing lesion. Our experiments demonstrate that the trained neural network, specifically the U-Net, is sensitive to Poisson noise. While the observed images appear artifact-free, anatomical structures may be located at wrong positions, e.g. the skin shifted by up to 1 cm. This kind of behavior can be reduced by retraining on data with simulated Poisson noise. However, we demonstrate that the retrained U-Net model is still susceptible to adversarial examples. We conclude the paper with suggestions towards robust deep-learning-based reconstruction.",
isbn="978-3-030-00928-1"
}

@incollection{stimpel2019multi,
  title={Multi-modal super-resolution with deep guided filtering},
  author={Stimpel, Bernhard and Syben, Christopher and Schirrmacher, Franziska and Hoelter, Philip and D{\"o}rfler, Arnd and Maier, Andreas},
  booktitle={Bildverarbeitung f{\"u}r die Medizin 2019},
  pages={110--115},
  year={2019},
  publisher={Springer}
}

@article{pro2012interactive,
  title={Interactive Disassembler},
  author={Pro, IDA},
  journal={URl: http://www. hex-rays. com/products/ida/index. shtml},
  year={2012}
}

@inproceedings{rohleder2019hands,
  title={Hands-on ghidra-a tutorial about the software reverse engineering framework},
  author={Rohleder, Roman},
  booktitle={Proceedings of the 3rd ACM Workshop on Software Protection},
  pages={77--78},
  year={2019}
}

@inproceedings{gamma1993design,
  title={Design patterns: Abstraction and reuse of object-oriented design},
  author={Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
  booktitle={European Conference on Object-Oriented Programming},
  pages={406--431},
  year={1993},
  organization={Springer}
}

@article{syben2019pyro,
  title={PYRO-NN: Python reconstruction operators in neural networks},
  author={Syben, Christopher and Michen, Markus and Stimpel, Bernhard and Seitz, Stephan and Ploner, Stefan and Maier, Andreas K},
  journal={Medical physics},
  volume={46},
  number={11},
  pages={5110--5115},
  year={2019},
  publisher={Wiley Online Library}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{isensee2018nnu,
  title={nnu-net: Self-adapting framework for u-net-based medical image segmentation},
  author={Isensee, Fabian and Petersen, Jens and Klein, Andre and Zimmerer, David and Jaeger, Paul F and Kohl, Simon and Wasserthal, Jakob and Koehler, Gregor and Norajitra, Tobias and Wirkert, Sebastian and others},
  journal={arXiv preprint arXiv:1809.10486},
  year={2018}
}

@article{ye2018deep,
  title={Deep convolutional framelets: A general deep learning framework for inverse problems},
  author={Ye, Jong Chul and Han, Yoseob and Cha, Eunju},
  journal={SIAM Journal on Imaging Sciences},
  volume={11},
  number={2},
  pages={991--1048},
  year={2018},
  publisher={SIAM}
}

@incollection{hammernik2017deep,
  title={A deep learning architecture for limited-angle computed tomography reconstruction},
  author={Hammernik, Kerstin and W{\"u}rfl, Tobias and Pock, Thomas and Maier, Andreas},
  booktitle={Bildverarbeitung f{\"u}r die Medizin 2017},
  pages={92--97},
  year={2017},
  publisher={Springer}
}

@inproceedings{kobler2017variational,
  title={Variational networks: connecting variational methods and deep learning},
  author={Kobler, Erich and Klatzer, Teresa and Hammernik, Kerstin and Pock, Thomas},
  booktitle={German conference on pattern recognition},
  pages={281--293},
  year={2017},
  organization={Springer}
}

@article{hammernik2018learning,
  title={Learning a variational network for reconstruction of accelerated MRI data},
  author={Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
  journal={Magnetic resonance in medicine},
  volume={79},
  number={6},
  pages={3055--3071},
  year={2018},
  publisher={Wiley Online Library}
}

@article{maier2019learning,
  title={Learning with known operators reduces maximum error bounds},
  author={Maier, Andreas K and Syben, Christopher and Stimpel, Bernhard and W{\"u}rfl, Tobias and Hoffmann, Mathis and Schebesch, Frank and Fu, Weilin and Mill, Leonid and Kling, Lasse and Christiansen, Silke},
  journal={Nature machine intelligence},
  volume={1},
  number={8},
  pages={373--380},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{miccai:schirrmacher,
  title={{QuaSI: Quantile Sparse Image Prior for Spatio-Temporal Denoising of Retinal OCT Data}},
  author={Schirrmacher, Franziska and K{\"o}hler, Thomas and Husvogt, Lennart and Fujimoto, James G and Hornegger, Joachim and Maier, Andreas K},
  booktitle={Medical Image Computing and Computer-Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings},
  volume={10434},
  pages={83},
  year={2017},
  organization={Springer}
}


@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}


@incollection{andermatt2016multi,
  title={Multi-dimensional gated recurrent units for the segmentation of biomedical 3D-data},
  author={Andermatt, Simon and Pezold, Simon and Cattin, Philippe},
  booktitle={Deep Learning and Data Labeling for Medical Applications},
  pages={142--151},
  year={2016},
  publisher={Springer}
}

@inproceedings{milletari2016v,
  title={V-net: Fully convolutional neural networks for volumetric medical image segmentation},
  author={Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  booktitle={3D Vision (3DV), 2016 Fourth International Conference on},
  pages={565--571},
  year={2016},
  organization={IEEE}
}

@article{litjens2017survey,
  title={A survey on deep learning in medical image analysis},
  author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen AWM and Van Ginneken, Bram and S{\'a}nchez, Clara I},
  journal={Medical image analysis},
  volume={42},
  pages={60--88},
  year={2017},
  publisher={Elsevier}
}

@article{ker2018deep,
  title={Deep learning applications in medical image analysis},
  author={Ker, Justin and Wang, Lipo and Rao, Jai and Lim, Tchoyoson},
  journal={IEEE Access},
  volume={6},
  pages={9375--9389},
  year={2018},
  publisher={IEEE}
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@misc{gottschalk2021learningbased,
      title={Learning-Based Patch-Wise Metal Segmentation with Consistency Check}, 
      author={Tristan M. Gottschalk and Andreas Maier and Florian Kordon and Björn W. Kreher},
      year={2021},
      eprint={2101.10914},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@inproceedings{kofler2018u,
  title={A U-Nets Cascade for Sparse View Computed Tomography},
  author={Kofler, Andreas and Haltmeier, Markus and Kolbitsch, Christoph and Kachelrie{\ss}, Marc and Dewey, Marc},
  booktitle={International Workshop on Machine Learning for Medical Image Reconstruction},
  pages={91--99},
  year={2018},
  organization={Springer}
}

@ARTICLE{9410231,
  author={Roser, Philipp and Birkhold, Annette and Preuhs, Alexander and Syben, Christopher and Felsner, Lina and Hoppe, Elisabeth and Strobel, Norbert and Kowarschik, Markus and Fahrig, Rebecca and Maier, Andreas},
  journal={IEEE Transactions on Medical Imaging}, 
  title={X-ray Scatter Estimation Using Deep Splines}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMI.2021.3074712}}


@misc{fu2019lesson,
      title={Lesson Learnt: Modularization of Deep Networks Allow Cross-Modality Reuse}, 
      author={Weilin Fu and Lennart Husvogt and Stefan Ploner James G. Fujimoto Andreas Maier},
      year={2019},
      eprint={1911.02080},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@InProceedings{10.1007/978-3-030-32239-7_21,
author="Fu, Weilin
and Breininger, Katharina
and Schaffert, Roman
and Ravikumar, Nishant
and Maier, Andreas",
editor="Shen, Dinggang
and Liu, Tianming
and Peters, Terry M.
and Staib, Lawrence H.
and Essert, Caroline
and Zhou, Sean
and Yap, Pew-Thian
and Khan, Ali",
title="A Divide-and-Conquer Approach Towards Understanding Deep Networks",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="183--191",
abstract="Deep neural networks have achieved tremendous success in various fields including medical image segmentation. However, they have long been criticized for being a black-box, in that interpretation, understanding and correcting architectures is difficult as there is no general theory for deep neural network design. Previously, precision learning was proposed to fuse deep architectures and traditional approaches. Deep networks constructed in this way benefit from the original known operator, have fewer parameters, and improved interpretability. However, they do not yield state-of-the-art performance in all applications. In this paper, we propose to analyze deep networks using known operators, by adopting a divide-and-conquer strategy to replace network components, whilst retaining networks performance. The task of retinal vessel segmentation is investigated for this purpose. We start with a high-performance U-Net and show by step-by-step conversion that we are able to divide the network into modules of known operators. The results indicate that a combination of a trainable guided filter and a trainable version of the Frangi filter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with a tremendous reduction in parameters (111, 536 vs. 9, 575). In addition, the trained layers can be mapped back into their original algorithmic interpretation and analyzed using standard tools of signal processing.",
isbn="978-3-030-32239-7"
}



@inproceedings{ArXivWeilin,
	booktitle={{Bildverarbeitung f{\"u}r die Medizin 2018}},
	year={2018},
	title={{Frangi-Net: A Neural Network Approach to Vessel Segmentation}},
	location={Erlangen},
	author={Weilin Fu and Katharina Breininger and Roman Schaffert and Nishant Ravikumar and Tobias W{\"u}rfl and Jim  Fujimoto and Eric  Moult and Andreas Maier},
	pages={341--346},
	editor={Andreas Maier and  Deserno T. and  Handels H. and  Maier-Hein K. and  Palm C. and  Tolxdorff T.},
	bibsource = {UnivIS, http://univis.uni-erlangen.de/prg?search=publications&id=91813115&show=elong},
}

@article{yang2017quicksilver,
  title={Quicksilver: Fast predictive image registration--a deep learning approach},
  author={Yang, Xiao and Kwitt, Roland and Styner, Martin and Niethammer, Marc},
  journal={NeuroImage},
  volume={158},
  pages={378--396},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{liao2017artificial,
  title={An Artificial Agent for Robust Image Registration.},
  author={Liao, Rui and Miao, Shun and de Tournemire, Pierre and Grbic, Sasa and Kamen, Ali and Mansi, Tommaso and Comaniciu, Dorin},
  booktitle={AAAI},
  pages={4168--4175},
  year={2017}
}

@misc{zhou2021metalearning,
      title={Meta-Learning Symmetries by Reparameterization}, 
      author={Allan Zhou and Tom Knowles and Chelsea Finn},
      year={2021},
      eprint={2007.02933},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{snell2017prototypical,
      title={Prototypical Networks for Few-shot Learning}, 
      author={Jake Snell and Kevin Swersky and Richard S. Zemel},
      year={2017},
      eprint={1703.05175},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{marcus2020decade,
      title={The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence}, 
      author={Gary Marcus},
      year={2020},
      eprint={2002.06177},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@InProceedings{pmlr-v70-finn17a, title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, author = {Chelsea Finn and Pieter Abbeel and Sergey Levine}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1126--1135}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf}, url = { http://proceedings.mlr.press/v70/finn17a.html }, abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.} }

@InProceedings{Sung_2018_CVPR,
author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H.S. and Hospedales, Timothy M.},
title = {Learning to Compare: Relation Network for Few-Shot Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@misc{zoph2017neural,
      title={Neural Architecture Search with Reinforcement Learning}, 
      author={Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1611.01578},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{triantafillou2020metadataset,
      title={Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples}, 
      author={Eleni Triantafillou and Tyler Zhu and Vincent Dumoulin and Pascal Lamblin and Utku Evci and Kelvin Xu and Ross Goroshin and Carles Gelada and Kevin Swersky and Pierre-Antoine Manzagol and Hugo Larochelle},
      year={2020},
      eprint={1903.03096},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NIPS2015_33ceb07b,
 author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Spatial Transformer Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf},
 volume = {28},
 year = {2015}
}



@InProceedings{10.1007/978-3-030-59716-0_32,
author="Gao, Cong
and Liu, Xingtong
and Gu, Wenhao
and Killeen, Benjamin
and Armand, Mehran
and Taylor, Russell
and Unberath, Mathias",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Generalizing Spatial Transformers to Projective Geometry with Applications to 2D/3D Registration",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="329--339",
abstract="Differentiable rendering is a technique to connect 3D scenes with corresponding 2D images. Since it is differentiable, processes during image formation can be learned. Previous approaches to differentiable rendering focus on mesh-based representations of 3D scenes, which is inappropriate for medical applications where volumetric, voxelized models are used to represent anatomy. We propose a novel Projective Spatial Transformer module that generalizes spatial transformers to projective geometry, thus enabling differentiable volume rendering. We demonstrate the usefulness of this architecture on the example of 2D/3D registration between radiographs and CT scans. Specifically, we show that our transformer enables end-to-end learning of an image processing and projection model that approximates an image similarity function that is convex with respect to the pose parameters, and can thus be optimized effectively using conventional gradient descent. To the best of our knowledge, we are the first to describe the spatial transformers in the context of projective transmission imaging, including rendering and pose estimation. We hope that our developments will benefit related 3D research applications. The source code is available at https://github.com/gaocong13/Projective-Spatial-Transformers.",
isbn="978-3-030-59716-0"
}



% Encoding: UTF-8

@InProceedings{10.1007/978-3-030-32254-0_2,
author="Zaech, Jan-Nico
and Gao, Cong
and Bier, Bastian
and Taylor, Russell
and Maier, Andreas
and Navab, Nassir
and Unberath, Mathias",
editor="Shen, Dinggang
and Liu, Tianming
and Peters, Terry M.
and Staib, Lawrence H.
and Essert, Caroline
and Zhou, Sean
and Yap, Pew-Thian
and Khan, Ali",
title="Learning to Avoid Poor Images: Towards Task-aware C-arm Cone-beam CT Trajectories",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="11--19",
abstract="Metal artifacts in computed tomography (CT) arise from a mismatch between physics of image formation and idealized assumptions during tomographic reconstruction. These artifacts are particularly strong around metal implants, inhibiting widespread adoption of 3D cone-beam CT (CBCT) despite clear opportunity for intra-operative verification of implant positioning, e. g. in spinal fusion surgery. On synthetic and real data, we demonstrate that much of the artifact can be avoided by acquiring better data for reconstruction in a task-aware and patient-specific manner, and describe the first step towards the envisioned task-aware CBCT protocol. The traditional short-scan CBCT trajectory is planar, with little room for scene-specific adjustment. We extend this trajectory by autonomously adjusting out-of-plane angulation. This enables C-arm source trajectories that are scene-specific in that they avoid acquiring ``poor images'', characterized by beam hardening, photon starvation, and noise. The recommendation of ideal out-of-plane angulation is performed on-the-fly using a deep convolutional neural network that regresses a detectability-rank derived from imaging physics.",
isbn="978-3-030-32254-0"
}




@article{faucris.250564935,
 abstract = {In this study, we propose a novel point cloud based 3D registration and segmentation framework using reinforcement learning. An artificial agent, implemented as a distinct actor based on value networks, is trained to predict the optimal piece-wise linear transformation of a point cloud for the joint tasks of registration and segmentation. The actor network estimates a set of plausible actions and the value network aims to select the optimal action for the current observation. Point-wise features that comprise spatial positions (and surface normal vectors in the case of structured meshes), and their corresponding image features, are used to encode the observation and represent the underlying 3D volume. The actor and value networks are applied iteratively to estimate a sequence of transformations that enable accurate delineation of object boundaries. The proposed approach was extensively evaluated in both segmentation and registration tasks using a variety of challenging clinical datasets. Our method has fewer trainable parameters and lower computational complexity compared to the 3D U-Net, and it is independent of the volume resolution. We show that the proposed method is applicable to mono- and multi-modal segmentation tasks, achieving significant improvements over the state-of-the-art for the latter. The flexibility of the proposed framework is further demonstrated for a multi-modal registration application. As we learn to predict actions rather than a target, the proposed method is more robust compared to the 3D U-Net when dealing with previously unseen datasets, acquired using different protocols or modalities. As a result, the proposed method provides a promising multi-purpose segmentation and registration framework, particular in the context of image-guided interventions.},
 author = {Zhong, Xia and Amrehn, Mario and Ravikumar, Nishant and Chen, Shuqing and Strobel, Norbert and Birkhold, Annette and Kowarschik, Markus and Fahrig, Rebecca and Maier, Andreas},
 doi = {10.1038/s41598-021-82370-6},
 faupublication = {yes},
 journal = {Scientific Reports},
 note = {CRIS-Team Scopus Importer:2021-02-26},
 peerreviewed = {Yes},
 title = {{Deep} action learning enables robust {3D} segmentation of body organs in various {CT} and {MRI} images},
 volume = {11},
 year = {2021}
}



@article{mill2021,
author = {Mill, Leonid and Wolff, David and Gerrits, Nele and Philipp, Patrick and Kling, Lasse and Vollnhals, Florian and Ignatenko, Andrew and Jaremenko, Christian and Huang, Yixing and De Castro, Olivier and Audinot, Jean-Nicolas and Nelissen, Inge and Wirtz, Tom and Maier, Andreas and Christiansen, Silke},
title = {Synthetic Image Rendering Solves Annotation Problem in Deep Learning Nanoparticle Segmentation},
journal = {Small Methods},
volume = {5},
number = {7},
pages = {2100223},
keywords = {helium ion microscopy, image analysis, machine learning, nanoparticles, segmentation, toxicology},
doi = {https://doi.org/10.1002/smtd.202100223},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smtd.202100223},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smtd.202100223},
abstract = {Abstract Nanoparticles occur in various environments as a consequence of man-made processes, which raises concerns about their impact on the environment and human health. To allow for proper risk assessment, a precise and statistically relevant analysis of particle characteristics (such as size, shape, and composition) is required that would greatly benefit from automated image analysis procedures. While deep learning shows impressive results in object detection tasks, its applicability is limited by the amount of representative, experimentally collected and manually annotated training data. Here, an elegant, flexible, and versatile method to bypass this costly and tedious data acquisition process is presented. It shows that using a rendering software allows to generate realistic, synthetic training data to train a state-of-the art deep neural network. Using this approach, a segmentation accuracy can be derived that is comparable to man-made annotations for toxicologically relevant metal-oxide nanoparticle ensembles which were chosen as examples. The presented study paves the way toward the use of deep learning for automated, high-throughput particle detection in a variety of imaging techniques such as in microscopies and spectroscopies, for a wide range of applications, including the detection of micro- and nanoplastic particles in water and tissue samples.},
year = {2021}
}

@article{faucris.242204248,
 abstract = {Purpose: During spinal fusion surgery, screws are placed close to critical nerves suggesting the need for highly accurate screw placement. Verifying screw placement on high-quality tomographic imaging is essential. C-arm cone-beam CT (CBCT) provides intraoperative 3D tomographic imaging which would allow for immediate verification and, if needed, revision. However, the reconstruction quality attainable with commercial CBCT devices is insufficient, predominantly due to severe metal artifacts in the presence of pedicle screws. These artifacts arise from a mismatch between the true physics of image formation and an idealized model thereof assumed during reconstruction. Prospectively acquiring views onto anatomy that are least affected by this mismatch can, therefore, improve reconstruction quality. Methods: We propose to adjust the C-arm CBCT source trajectory during the scan to optimize reconstruction quality with respect to a certain task, i.e., verification of screw placement. Adjustments are performed on-the-fly using a convolutional neural network that regresses a quality index over all possible next views given the current X-ray image. Adjusting the CBCT trajectory to acquire the recommended views results in non-circular source orbits that avoid poor images, and thus, data inconsistencies. Results: We demonstrate that convolutional neural networks trained on realistically simulated data are capable of predicting quality metrics that enable scene-specific adjustments of the CBCT source trajectory. Using both realistically simulated data as well as real CBCT acquisitions of a semianthropomorphic phantom, we show that tomographic reconstructions of the resulting scene-specific CBCT acquisitions exhibit improved image quality particularly in terms of metal artifacts. Conclusion: The proposed method is a step toward online patient-specific C-arm CBCT source trajectories that enable high-quality tomographic imaging in the operating room. Since the optimization objective is implicitly encoded in a neural network trained on large amounts of well-annotated projection images, the proposed approach overcomes the need for 3D information at run-time.},
 author = {Thies, Mareike and Zäch, Jan Nico and Gao, Cong and Taylor, Russell and Navab, Nassir and Maier, Andreas and Unberath, Mathias},
 doi = {10.1007/s11548-020-02249-1},
 faupublication = {yes},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 keywords = {Deep learning; Image-guided surgery; Metal artifact reduction; Tomographic reconstruction},
 note = {CRIS-Team Scopus Importer:2020-09-04},
 pages = {1787-1796},
 peerreviewed = {Yes},
 title = {{A} learning-based method for online adjustment of {C}-arm {Cone}-beam {CT} source trajectories for artifact avoidance},
 volume = {15},
 year = {2020}
}

@InProceedings{pineda,
author="Pineda, Luis
and Basu, Sumana
and Romero, Adriana
and Calandra, Roberto
and Drozdzal, Michal",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Active MR k-space Sampling with Reinforcement Learning",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="23--33",
abstract="Deep learning approaches have recently shown great promise in accelerating magnetic resonance image (MRI) acquisition. The majority of existing work have focused on designing better reconstruction models given a pre-determined acquisition trajectory, ignoring the question of trajectory optimization. In this paper, we focus on learning acquisition trajectories given a fixed image reconstruction model. We formulate the problem as a sequential decision process and propose the use of reinforcement learning to solve it. Experiments on a large scale public MRI dataset of knees show that our proposed models significantly outperform the state-of-the-art in active MRI acquisition, over a large range of acceleration factors.",
isbn="978-3-030-59713-9"
}



@article{mrzero,
author = {Loktyushin, A. and Herz, K. and Dang, N. and Glang, F. and Deshmane, A. and Weinmüller, S. and Doerfler, A. and Schölkopf, B. and Scheffler, K. and Zaiss, M.},
title = {MRzero - Automated discovery of MRI sequences using supervised learning},
journal = {Magnetic Resonance in Medicine},
volume = {86},
number = {2},
pages = {709-724},
keywords = {MR simulation, differentiable Bloch equation, AUTOSEQ, automatic MR, machine learning},
doi = {https://doi.org/10.1002/mrm.28727},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.28727},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.28727},
abstract = {Purpose A supervised learning framework is proposed to automatically generate MR sequences and corresponding reconstruction based on the target contrast of interest. Combined with a flexible, task-driven cost function this allows for an efficient exploration of novel MR sequence strategies. Methods The scanning and reconstruction process is simulated end-to-end in terms of RF events, gradient moment events in x and y, and delay times, acting on the input model spin system given in terms of proton density, and , and . As a proof of concept, we use both conventional MR images and maps as targets and optimize from scratch using the loss defined by data fidelity, SAR penalty, and scan time. Results In a first attempt, MRzero learns gradient and RF events from zero, and is able to generate a target image produced by a conventional gradient echo sequence. Using a neural network within the reconstruction module allows arbitrary targets to be learned successfully. Experiments could be translated to image acquisition at the real system (3T Siemens, PRISMA) and could be verified in the measurements of phantoms and a human brain in vivo. Conclusions Automated MR sequence generation is possible based on differentiable Bloch equation simulations and a supervised learning approach.},
year = {2021}
}


@inproceedings{schaffert2018metric,
  title={Metric-Driven Learning of Correspondence Weighting for 2-D/3-D Image Registration},
  author={Schaffert, Roman and Wang, Jian and Fischer, Peter and Borsdorf, Anja and Maier, Andreas},
  booktitle={German Conference on Pattern Recognition (GCPR)},
  year={2018}
}

@inproceedings{univis91731175,
	author={Julian Krebs and Tommaso Mansi and Herve Delingette and Li Zhang and Florin Cristian Ghesu and Shun Miao and Andreas Maier and Nicholas Ayache and Rui Liao and Ali Kamen},
	title={{Robust Non-Rigid Registration through Agent-Based Action Learning}},
	location={Quebec, Canada},
	year={2017},
	booktitle={{Medical Image Computing and Computer-Assisted Intervention; MICCAI 2017}},
	editor={ Springer},
	pages={344--352},
	bibsource = {UnivIS, http://univis.uni-erlangen.de/prg?search=publications&id=91731175&show=elong},
}

@InProceedings{10.1007/978-3-030-00937-3_12,
author="Unberath, Mathias
and Zaech, Jan-Nico
and Lee, Sing Chun
and Bier, Bastian
and Fotouhi, Javad
and Armand, Mehran
and Navab, Nassir",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="DeepDRR -- A Catalyst for Machine Learning in Fluoroscopy-Guided Procedures",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="98--106",
abstract="Machine learning-based approaches outperform competing methods in most disciplines relevant to diagnostic radiology. Interventional radiology, however, has not yet benefited substantially from the advent of deep learning, in particular because of two reasons: (1) Most images acquired during the procedure are never archived and are thus not available for learning, and (2) even if they were available, annotations would be a severe challenge due to the vast amounts of data. When considering fluoroscopy-guided procedures, an interesting alternative to true interventional fluoroscopy is in silico simulation of the procedure from 3D diagnostic CT. In this case, labeling is comparably easy and potentially readily available, yet, the appropriateness of resulting synthetic data is dependent on the forward model. In this work, we propose DeepDRR, a framework for fast and realistic simulation of fluoroscopy and digital radiography from CT scans, tightly integrated with the software platforms native to deep learning. We use machine learning for material decomposition and scatter estimation in 3D and 2D, respectively, combined with analytic forward projection and noise injection to achieve the required performance. On the example of anatomical landmark detection in X-ray images of the pelvis, we demonstrate that machine learning models trained on DeepDRRs generalize to unseen clinically acquired data without the need for re-training or domain adaptation. Our results are promising and promote the establishment of machine learning in fluoroscopy-guided procedures.",
isbn="978-3-030-00937-3"
}

@InProceedings{schiffers2018cyclegan,
author="Schiffers, Florian
and Yu, Zekuan
and Arguin, Steve
and Maier, Andreas
and Ren, Qiushi",
editor="Maier, Andreas
and Deserno, Thomas M.
and Handels, Heinz
and Maier-Hein, Klaus Hermann
and Palm, Christoph
and Tolxdorff, Thomas",
title="Synthetic Fundus Fluorescein Angiography using Deep Neural Networks",
booktitle="Bildverarbeitung f{\"u}r die Medizin 2018",
year="2018",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="234--238",
abstract="Fundus fluorescein angiography yields complementary image information when compared to conventional fundus imaging. Angiographic imaging, however, may pose risks of harm to the patient. The output from both types of imaging have different characteristics, but the most prominent features of the fundus are shared in both images. Thus, the question arises if conventional fundus images alone provide enough information to synthesize an angiographic image. Our research analyzes the capacity of deep neural networks to synthesize virtual angiographic images from their conventional fundus counterparts.",
isbn="978-3-662-56537-7"
}

@inproceedings{vishnevskiy2018image,
  title={Image Reconstruction via Variational Network for Real-Time Hand-Held Sound-Speed Imaging},
  author={Vishnevskiy, Valery and Sanabria, Sergio J and Goksel, Orcun},
  booktitle={International Workshop on Machine Learning for Medical Image Reconstruction},
  pages={120--128},
  year={2018},
  organization={Springer}
}

@InProceedings{Cohen2018distribution,
author="Cohen, Joseph Paul
and Luck, Margaux
and Honari, Sina",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="Distribution Matching Losses Can Hallucinate Features in Medical Image Translation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="529--536",
abstract="This paper discusses how distribution matching losses, such as those used in CycleGAN, when used to synthesize medical images can lead to mis-diagnosis of medical conditions. It seems appealing to use these new image synthesis methods for translating images from a source to a target domain because they can produce high quality images and some even do not require paired data. However, the basis of how these image translation models work is through matching the translation output to the distribution of the target domain. This can cause an issue when the data provided in the target domain has an over or under representation of some classes (e.g. healthy or sick). When the output of an algorithm is a transformed image there are uncertainties whether all known and unknown class labels have been preserved or changed. Therefore, we recommend that these translated images should not be used for direct interpretation (e.g. by doctors) because they may lead to misdiagnosis of patients based on hallucinated image features by an algorithm that matches a distribution. However there are many recent papers that seem as though this is the goal.",
isbn="978-3-030-00928-1"
}

@article{adler2018learned,
  title={Learned primal-dual reconstruction},
  author={Adler, Jonas and {\"O}ktem, Ozan},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={6},
  pages={1322--1332},
  year={2018},
  publisher={IEEE}
}


@article{han2017mr,
  title={MR-based synthetic CT generation using a deep convolutional neural network method},
  author={Han, Xiao},
  journal={Medical physics},
  volume={44},
  number={4},
  pages={1408--1419},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{maier2018deep,
  title={Deep scatter estimation (DSE): feasibility of using a deep convolutional neural network for real-time x-ray scatter prediction in cone-beam CT},
  author={Maier, Joscha and Berker, Yannick and Sawall, Stefan and Kachelrie{\ss}, Marc},
  booktitle={Medical Imaging 2018: Physics of Medical Imaging},
  volume={10573},
  pages={105731L},
  year={2018},
  organization={International Society for Optics and Photonics}
}

@inproceedings{vesselness-frangi,
  title={Multiscale vessel enhancement filtering},
  author={Frangi, Alejandro F and Niessen, Wiro J and Vincken, Koen L and Viergever, Max A},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={130--137},
  year={1998},
  organization={Springer}
}


@inproceedings{maier2018precision,
  title={Precision learning: towards use of known operators in neural networks},
  author={Maier, Andreas and Schebesch, Frank and Syben, Christopher and W{\"u}rfl, Tobias and Steidl, Stefan and Choi, Jang-Hwan and Fahrig, Rebecca},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},
  pages={183--188},
  year={2018},
  organization={IEEE}
}

@article{li2018differentiable,
  title={Differentiable programming for image processing and deep learning in halide},
  author={Li, Tzu-Mao and Gharbi, Micha{\"e}l and Adams, Andrew and Durand, Fr{\'e}do and Ragan-Kelley, Jonathan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={139},
  year={2018},
  publisher={ACM}
}

@article{zhu2018image,
  title={Image reconstruction by domain-transform manifold learning},
  author={Zhu, Bo and Liu, Jeremiah Z and Cauley, Stephen F and Rosen, Bruce R and Rosen, Matthew S},
  journal={Nature},
  volume={555},
  number={7697},
  pages={487},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{wurfl2016deep,
  title={Deep learning computed tomography},
  author={W{\"u}rfl, Tobias and Ghesu, Florin C and Christlein, Vincent and Maier, Andreas},
  booktitle={International conference on medical image computing and computer-assisted intervention},
  pages={432--440},
  year={2016},
  organization={Springer}
}

@misc{meister18:TFB,
    title={Towards Fast Biomechanical Modeling of Soft Tissue Using Neural Networks},
    author={Meister, F. and Passerini, T. and Mihalef, V. and Tuysuzoglu, A. and Maier, A. and Mansi, T.},
    year={2018},
	howpublished={Medical Imaging meets NeurIPS Workshop at 32nd Conference on Neural Information Processing Systems (NeurIPS)}
}

@article{wurfl2018deep,
  title={Deep learning computed tomography: Learning projection-domain weights from image domain in limited angle problems},
  author={W{\"u}rfl, Tobias and Hoffmann, Mathis and Christlein, Vincent and Breininger, Katharina and Huang, Yixin and Unberath, Mathias and Maier, Andreas K},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={6},
  pages={1454--1463},
  year={2018},
  publisher={IEEE}
}

@article{ghesu2017multi,
  title={Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans},
  author={Ghesu, Florin Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@misc{Auditing17:online,
author = {{The Supreme Audit Institutions of Finland, Germany, the Netherlands, Norway and the UK}},
title = {Auditing machine learning algorithms},
howpublished = {\url{https://auditingalgorithms.net/}},
month = {},
year = {2020},
note = {(Accessed on 07/02/2021)}
}

@article{neves2021interpretable,
  title={Interpretable heartbeat classification using local model-agnostic explanations on ECGs},
  author={Neves, In{\^e}s and Folgado, Duarte and Santos, Sara and Barandas, Mar{\'\i}lia and Campagner, Andrea and Ronzio, Luca and Cabitza, Federico and Gamboa, Hugo},
  journal={Computers in Biology and Medicine},
  volume={133},
  pages={104393},
  year={2021},
  publisher={Elsevier}
}

@article{DBLP:journals/corr/abs-2104-03624,
  author    = {Kurt Willis and
               Luis Oala},
  title     = {Post-Hoc Domain Adaptation via Guided Data Homogenization},
  journal   = {CoRR},
  volume    = {abs/2104.03624},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.03624},
  archivePrefix = {arXiv},
  eprint    = {2104.03624},
  timestamp = {Tue, 13 Apr 2021 16:46:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-03624.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{saleiro2018aequitas,
  title={Aequitas: A bias and fairness audit toolkit},
  author={Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T and Ghani, Rayid},
  journal={arXiv preprint arXiv:1811.05577},
  year={2018}
}

@article{SCHWENDICKE2021103610,
title = {Artificial intelligence in dental research: Checklist for authors, reviewers, readers},
journal = {Journal of Dentistry},
volume = {107},
pages = {103610},
year = {2021},
issn = {0300-5712},
doi = {https://doi.org/10.1016/j.jdent.2021.103610},
url = {https://www.sciencedirect.com/science/article/pii/S0300571221000312},
author = {Falk Schwendicke and Tarry Singh and Jae-Hong Lee and Robert Gaudin and Akhilanand Chaurasia and Thomas Wiegand and Sergio Uribe and Joachim Krois},
keywords = {Artificial intelligence, Checklist, Deep learning, Dental, Machine learning, Teeth},
abstract = {Objectives
The number of studies employing artificial intelligence (AI), specifically machine and deep learning, is growing fast. The majority of studies suffer from limitations in planning, conduct and reporting, resulting in low robustness, reproducibility and applicability. We here present a consented checklist on planning, conducting and reporting of AI studies for authors, reviewers and readers in dental research.
Methods
Lending from existing reviews, standards and other guidance documents, an initial draft of the checklist and an explanatory document were derived and discussed among the members of IADR’s e-oral network and the ITU/WHO focus group “Artificial Intelligence for Health (AI4H)”. The checklist was consented by 27 group members via an e-Delphi process.
Results
Thirty-one items on planning, conducting and reporting of AI studies were agreed on. These involve items on the studies’ wider goal, focus, design and specific aims, data sampling and reporting, sample estimation, reference test construction, model parameters, training and evaluation, uncertainty and explainability, performance metrics and data partitions.
Conclusion
Authors, reviewers and readers should consider this checklist when planning, conducting, reporting and evaluating studies on AI in dentistry.
Clinical significance
Current studies on AI in dentistry show considerable weaknesses, hampering their replication and application. This checklist may help to overcome this issue and advance AI research as well as facilitate a debate on standards in this fields.}
}

@article{koshiyama2021towards,
  title={Towards Algorithm Auditing: A Survey on Managing Legal, Ethical and Technological Risks of AI, ML and Associated Algorithms},
  author={Koshiyama, Adriano and Kazim, Emre and Treleaven, Philip and Rai, Pete and Szpruch, Lukasz and Pavey, Giles and Ahamat, Ghazi and Leutner, Franziska and Goebel, Randy and Knight, Andrew and others},
  year={2021}
}

@misc{EURLex5218:online,
author = {EUROPEAN-COMMISSION},
title = {EUR-Lex - 52021PC0206 - EN - EUR-Lex},
howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206}},
month = {},
year = {2021},
note = {(Accessed on 07/01/2021)}
}

@misc{AIMLSaMD44:online,
author = {US-FDA},
title = {AIML\_SaMD\_Action\_Plan},
howpublished = {\url{https://www.fda.gov/media/145022/download?utm_medium=email&utm_source=govdelivery}},
month = {},
year = {2021},
note = {(Accessed on 07/01/2021)}
}

@misc{MEDDEV2741:online,
author = {EUROPEAN-COMMISSION},
title = {MEDDEV 2.7/1 revision 4, Clinical evaluation: a guide for manufacturers and notified bodies},
howpublished = {\url{https://ec.europa.eu/docsroom/documents/17522/attachments/1/translations/en/renditions/native}},
month = {},
year = {2016},
note = {(Accessed on 07/01/2021)}
}

@misc{minderer2021revisiting,
      title={Revisiting the Calibration of Modern Neural Networks}, 
      author={Matthias Minderer and Josip Djolonga and Rob Romijnders and Frances Hubis and Xiaohua Zhai and Neil Houlsby and Dustin Tran and Mario Lucic},
      year={2021},
      eprint={2106.07998},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{9445026,
  author={Calderon-Ramirez, Saul and Yang, Shengxiang and Moemeni, Armaghan and Colreavy-Donnelly, Simon and Elizondo, David A. and Oala, Luis and Rodríguez-Capitán, Jorge and Jiménez-Navarro, Manuel and López-Rubio, Ezequiel and Molina-Cabello, Miguel A.},
  journal={IEEE Access}, 
  title={Improving Uncertainty Estimation With Semi-Supervised Deep Learning for COVID-19 Detection Using Chest X-Ray Images}, 
  year={2021},
  volume={9},
  number={},
  pages={85442-85454},
  doi={10.1109/ACCESS.2021.3085418}}

@InProceedings{10.1007/978-3-658-33198-6_79,
author="Macdonald, Jan
and M{\"a}rz, Maximilian
and Oala, Luis
and Samek, Wojciech",
editor="Palm, Christoph
and Deserno, Thomas M.
and Handels, Heinz
and Maier, Andreas
and Maier-Hein, Klaus
and Tolxdorff, Thomas",
title="Interval Neural Networks as Instability Detectors for Image Reconstructions",
booktitle="Bildverarbeitung f{\"u}r die Medizin 2021",
year="2021",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="324--329",
abstract="This work investigates the detection of instabilities that may occur when utilizing deep learning models for image reconstruction tasks. Although neural networks often empirically outperform traditional reconstruction methods, their usage for sensitive medical applications remains controversial. Indeed, in a recent series of works, it has been demonstrated that deep learning approaches are susceptible to various types of instabilities, caused for instance by adversarial noise or out-ofdistribution features. It is argued that this phenomenon can be observed regardless of the underlying architecture and that there is no easy remedy. Based on this insight, the present work demonstrates, how uncertainty quantification methods can be employed as instability detectors. In particular, it is shown that the recently proposed Interval Neural Networks are highly effective in revealing instabilities of reconstructions. Such an ability is crucial to ensure a safe use of deep learning-based methods for medical image reconstruction.",
isbn="978-3-658-33198-6"
}


@article{DBLP:journals/corr/abs-2003-11566,
  author    = {Luis Oala and
               Cosmas Hei{\ss} and
               Jan MacDonald and
               Maximilian M{\"{a}}rz and
               Wojciech Samek and
               Gitta Kutyniok},
  title     = {Interval Neural Networks: Uncertainty Scores},
  journal   = {CoRR},
  volume    = {abs/2003.11566},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.11566},
  archivePrefix = {arXiv},
  eprint    = {2003.11566},
  timestamp = {Wed, 01 Apr 2020 17:39:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-11566.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{calderon2021more,
  author    = {Sa{\'{u}}l Calder{\'{o}}n Ram{\'{\i}}rez and
               Luis Oala},
  title     = {More Than Meets The Eye: Semi-supervised Learning Under Non-IID Data},
  journal   = {CoRR},
  volume    = {abs/2104.10223},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.10223},
  archivePrefix = {arXiv},
  eprint    = {2104.10223},
  timestamp = {Thu, 10 Jun 2021 17:01:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10223.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/1500774.1500823,
author = {Ryan, John R.},
title = {Software Product Quality Assurance},
year = {1982},
isbn = {088283035X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1500774.1500823},
doi = {10.1145/1500774.1500823},
abstract = {Providing clear objectives, guidelines, and requirements in an environment conducive to high productivity is absolutely essential to designing and producing high-quality software. The Software Quality Branch of the Computer Systems Division of Texas Instruments is tasked with providing support functions that are vital to producing high-quality software.This paper explains the role of the Software Quality Branch in administering the development methodology of the Computer Systems Division. The paper also describes our participation in a corporate effort to define and monitor quality indices and our use of a software quality circle to encourage commitment to quality goals and to develop solutions to quality problems.},
booktitle = {Proceedings of the June 7-10, 1982, National Computer Conference},
pages = {393–398},
numpages = {6},
location = {Houston, Texas},
series = {AFIPS '82}
}



@article {Shneiderman13538,
	author = {Shneiderman, Ben},
	title = {Opinion: The dangers of faulty, biased, or malicious algorithms requires independent oversight},
	volume = {113},
	number = {48},
	pages = {13538--13540},
	year = {2016},
	doi = {10.1073/pnas.1618211113},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/113/48/13538},
	eprint = {https://www.pnas.org/content/113/48/13538.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{10.1145/3399579.3399867,
author = {Chen, Andrew and Chow, Andy and Davidson, Aaron and DCunha, Arjun and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Mewald, Clemens and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Singh, Avesh and Xie, Fen and Zaharia, Matei and Zang, Richard and Zheng, Juntai and Zumar, Corey},
title = {Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle},
year = {2020},
isbn = {9781450380232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399579.3399867},
doi = {10.1145/3399579.3399867},
abstract = {MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments.},
booktitle = {Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning},
articleno = {5},
numpages = {4},
location = {Portland, OR, USA},
series = {DEEM'20}
}

@article{DBLP:journals/corr/abs-1902-03570,
  author    = {Deshraj Yadav and
               Rishabh Jain and
               Harsh Agrawal and
               Prithvijit Chattopadhyay and
               Taranjeet Singh and
               Akash Jain and
               Shivkaran Singh and
               Stefan Lee and
               Dhruv Batra},
  title     = {EvalAI: Towards Better Evaluation Systems for {AI} Agents},
  journal   = {CoRR},
  volume    = {abs/1902.03570},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03570},
  archivePrefix = {arXiv},
  eprint    = {1902.03570},
  timestamp = {Sat, 23 Jan 2021 01:11:39 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-03570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{reportingcards,
title =   {DAISAM Audit Reporting Template},
  author =       {Verks, Boris and Oala, Luis},
  booktitle =   {ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) - Meeting J },
  year =   {2020},
  volume =   {J},
  publisher =    {ITU},
  pdf =   {https://aiaudit.org/assets/pdf/standards/FGAI4H-J-048.pdf},
  url =   {https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx}
}


@InProceedings{goodpractices,
title =   {Good practices for health applications of machine learning: Considerations for manufacturers and regulators},
  author =       {Johner, Christian and Balachandran, Pradeep and Oala, Luis and Lee, Aaron .Y. and Werneck Leite, Alixandro and Murchison, Andrew and Lin, Anle and Molnar, Christoph and Rumball-Smith, Juliet and Baird, Pat and Goldschmidt, Peter G. and Quartarolo, Pierre and Xu, Shan and Piechottka, Sven and Hornberger, Zack},
  booktitle =   {ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) - Meeting K},
  year =   {2021},
  volume =   {K},
editor = {Luis Oala},
  publisher =    {ITU},
  pdf =   {https://aiaudit.org/assets/pdf/standards/FGAI4H-K-039.pdf},
  url =   {https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx}
}

@article{roberts2021common,
  title={Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans},
  author={Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I and Etmann, Christian and McCague, Cathal and Beer, Lucian and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={3},
  pages={199--217},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{dietterich2019robust,
  title={Robust artificial intelligence and robust human organizations},
  author={Dietterich, Thomas G},
  journal={Frontiers of Computer Science},
  volume={13},
  number={1},
  pages={1--3},
  year={2019},
  publisher={Springer}
}

@article{wiegand2019and,
  title={WHO and ITU establish benchmarking process for artificial intelligence in health},
  author={Wiegand, Thomas and Krishnamurthy, Ramesh and Kuglitsch, Monique and Lee, Naomi and Pujari, Sameer and Salath{\'e}, Marcel and Wenzel, Markus and Xu, Shan},
  journal={The Lancet},
  volume={394},
  number={10192},
  pages={9--11},
  year={2019},
  publisher={Elsevier}
}

@misc{Googles52:online,
author = {Will Douglas Heaven},
title = {Google’s medical AI was super accurate in a lab. Real life was a different story. | MIT Technology Review},
howpublished = {\url{https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/}},
month = {},
year = {},
note = {(Accessed on 06/10/2021)}
}

@misc{CTscanni48:online,
author = {Luke Oakden-Rayner},
title = {CT scanning is just awful for diagnosing Covid-19 – Luke Oakden-Rayner},
howpublished = {\url{https://lukeoakdenrayner.wordpress.com/2020/03/23/ct-scanning-is-just-awful-for-diagnosing-covid-19/}},
month = {},
year = {},
note = {(Accessed on 06/10/2021)}
}

@inproceedings{NIPS2017_2650d608,
 author = {Kendall, Alex and Gal, Yarin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
 url = {https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{d2020underspecification,
  title={Underspecification presents challenges for credibility in modern machine learning},
  author={D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
  journal={arXiv preprint arXiv:2011.03395},
  year={2020}
}

@article{lapuschkin2019unmasking,
  title={Unmasking clever hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature Communications},
  volume={10},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{nalisnick2018deep,
  title={Do deep generative models know what they don't know?},
  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1810.09136},
  year={2018}
}

@inproceedings{gilmer2019adversarial,
  title={Adversarial examples are a natural consequence of test error in noise},
  author={Gilmer, Justin and Ford, Nicolas and Carlini, Nicholas and Cubuk, Ekin},
  booktitle={International Conference on Machine Learning},
  pages={2280--2289},
  year={2019},
  organization={PMLR}
}

@article{hendrycks2020many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  journal={arXiv preprint arXiv:2006.16241},
  year={2020}
}

@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International Conference on Machine Learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{raji2020closing,
  title={Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing},
  author={Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={33--44},
  year={2020}
}

@incollection{zhou2018unet++,
  title={Unet++: A nested u-net architecture for medical image segmentation},
  author={Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  booktitle={Deep learning in medical image analysis and multimodal learning for clinical decision support},
  pages={3--11},
  year={2018},
  publisher={Springer}
}

@article{wagner2020ptb,
  title={PTB-XL, a large publicly available electrocardiography dataset},
  author={Wagner, Patrick and Strodthoff, Nils and Bousseljot, Ralf-Dieter and Kreiseler, Dieter and Lunze, Fatima I and Samek, Wojciech and Schaeffter, Tobias},
  journal={Scientific Data},
  volume={7},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{bubba2019learning,
  title={Learning the invisible: a hybrid deep learning-shearlet framework for limited angle computed tomography},
  author={Bubba, Tatiana A and Kutyniok, Gitta and Lassas, Matti and M{\"a}rz, Maximilian and Samek, Wojciech and Siltanen, Samuli and Srinivasan, Vignesh},
  journal={Inverse Problems},
  volume={35},
  number={6},
  pages={064002},
  year={2019},
  publisher={IOP Publishing}
}

@article{senior2020improved,
  title={Improved protein structure prediction using potentials from deep learning},
  author={Senior, Andrew W and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'\i}dek, Augustin and Nelson, Alexander WR and Bridgland, Alex and others},
  journal={Nature},
  volume={577},
  number={7792},
  pages={706--710},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{hagele2020resolving,
  title={Resolving challenges in deep learning-based analyses of histopathological images using explanation methods},
  author={H{\"a}gele, Miriam and Seegerer, Philipp and Lapuschkin, Sebastian and Bockmayr, Michael and Samek, Wojciech and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Binder, Alexander},
  journal={Scientific Reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{wu2021medical,
  title={How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals},
  author={Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E and Zou, James},
  journal={Nature Medicine},
  volume={27},
  number={4},
  pages={582--584},
  year={2021},
  publisher={Nature Publishing Group}
}

@book{haggstrom2016here,
  title={Here be dragons: Science, technology and the future of humanity},
  author={H{\"a}ggstr{\"o}m, Olle},
  year={2016},
  publisher={Oxford University Press}
}

@article{10.1145/2843948,
author = {Gomez-Uribe, Carlos A. and Hunt, Neil},
title = {The Netflix Recommender System: Algorithms, Business Value, and Innovation},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2843948},
doi = {10.1145/2843948},
abstract = {This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {13},
numpages = {19},
keywords = {Recommender systems}
}

@misc{GoogleAI54:online,
author = {},
title = {Google AI Blog: Smart Compose: Using Neural Networks to Help Write Emails},
howpublished = {\url{https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html}},
month = {},
year = {},
note = {(Accessed on 06/10/2021)}
}

@misc{Applesne35:online,
author = {Stephen Nellis},
title = {Apple's new iPhones shift smartphone camera battleground to AI | Reuters},
howpublished = {\url{https://www.reuters.com/article/us-apple-iphone-cameras/apples-new-iphones-shift-smartphone-camera-battleground-to-ai-idUKKCN1VX18M}},
month = {},
year = {},
note = {(Accessed on 06/10/2021)}
}

@article{mackay1992practical,
  title={A practical Bayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={448--472},
  year={1992},
  publisher={MIT Press}
}

@article{bronstein2017geometric,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}

@article{friedman2002stochastic,
  title={Stochastic gradient boosting},
  author={Friedman, Jerome H},
  journal={Computational statistics \& data analysis},
  volume={38},
  number={4},
  pages={367--378},
  year={2002},
  publisher={Elsevier}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{weiss1995rule,
  title={Rule-based machine learning methods for functional prediction},
  author={Weiss, Sholom M and Indurkhya, Nitin},
  journal={Journal of Artificial Intelligence Research},
  volume={3},
  pages={383--403},
  year={1995}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@book{hardtrecht,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: A story about machine learning},
  year = {2021},
  publisher = {\url{https://mlstory.org}},
  archivePrefix = {arXiv},
  eprint = {2102.05242},
  primaryClass = {cs.LG},
}

@article{guyon-elisseeff-03,
  title   = "An Introduction to Variable and Feature Selection",
  author  = "Isabelle Guyon and Andr\'{e} Elisseeff",
  journal = "JMLR",
  volume  = "3",
  month   = MAR,
  pages   = "1157-1182",
  year    = 2003
}
@article {Dalal2002,
	Title = {Artifacts that may be present on a blood film},
	Author = {Dalal, Bakul I. and Brigden, Malcolm L.},
	DOI = {10.1016/s0272-2712(03)00068-4},
	Number = {1},
	Volume = {22},
	Month = {March},
	Year = {2002},
	Journal = {Clinics in laboratory medicine},
	ISSN = {0272-2712},
	Pages = {81—100, vi},
}


@article{Corrons2004,
author = {{Vives Corrons}, Juan Lluis and Albar{\`{e}}de, Stephanie and Flandrin, George and Heller, Silke and Horvath, Katalin and Houwen, Berend and Nordin, Gunnar and Sarkani, Erika and Skitek, Milan and {Van Blerk}, Marjan and Libeer, Jean Claude},
doi = {10.1515/CCLM.2004.149},
issn = {14346621},
journal = {Clinical Chemistry and Laboratory Medicine},
keywords = {Blood smears,Control samples,External quality assessment,Guidelines},
mendeley-groups = {PhD/FGAI4HPaper},
number = {8},
pages = {922--926},
pmid = {15387443}
}

title = {{Guidelines for blood smear preparation and staining procedure for setting up an external quality assessment scheme for blood smear interpretation. Part 1: Control material}},
volume = {42},
year = {2004}
}



@inproceedings{Guo2017,
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
title = {On Calibration of Modern Neural Networks},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1321–1330},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@techreport{guyon2007causalreport,
  author      = {Isabelle Guyon and Constantin Aliferis and Andr\'{e} Elisseeff},
  title       = {Causal Feature Selection},
  institution = {Clopinet},
  year        = 2007,
  type        = {Technical Report },
  source      = {\url{http://clopinet.com/isabelle/Papers/causalFS.pdf}}
}

@article{Saleiro2018,
archivePrefix = {arXiv},
arxivId = {1811.05577},
author = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T. and Ghani, Rayid},
eprint = {1811.05577},
number = {2018},
title = {{Aequitas: A Bias and Fairness Audit Toolkit}},
url = {http://arxiv.org/abs/1811.05577},
year = {2018}
}

@article{Ozawa2015,
author = {Ozawa, Glen Y. and Bearse, Marcus A. and Adams, Anthony J.},
doi = {10.3109/02713683.2014.958500},
issn = {14602202},
journal = {Current Eye Research},
number = {2},
pages = {234--246},
pmid = {25545999},
title = {{Male-female differences in diabetic retinopathy?}},
volume = {40},
year = {2015}
}

@article{Salathe2018,
archivePrefix = {arXiv},
arxivId = {1809.04797},
author = {Salath{\'{e}}, Marcel and Wiegand, Thomas and Wenzel, Markus},
eprint = {1809.04797},
journal = {arXiv},
mendeley-groups = {PhD/Fairness-Statistics in AI},
title = {{Focus Group on Artificial Intelligence for Health}},
url = {http://arxiv.org/abs/1809.04797},
year = {2018}
}

@article{Matek2019,
  title={Human-level recognition of blast cells in acute myeloid leukaemia with convolutional neural networks},
  author={Matek, Christian and Schwarz, Simone and Spiekermann, Karsten and Marr, Carsten},
  journal={Nature Machine Intelligence},
  volume={1},
  number={11},
  pages={538--544},
  year={2019},
  publisher={Nature Publishing Group}
}


@article{Wiegand2019,
author = {Wiegand, Thomas and Krishnamurthy, Ramesh and Kuglitsch, Monique and Lee, Naomi and Pujari, Sameer and Salath{\'{e}}, Marcel and Wenzel, Markus and Xu, Shan},
doi = {https://doi.org/10.1016/S0140-6736(19)30762-7},
journal = {The Lancet},
mendeley-groups = {PhD/Fairness-Statistics in AI},
number = {10192},
pages = {9--11},
title = {{WHO and ITU establish benchmarking process for artificial intelligence in healthitle}},
volume = {394},
year = {2019}
}

@article{Gebru2018,
archivePrefix = {arXiv},
arxivId = {1803.09010},
author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'{e}}, Hal and Crawford, Kate},
eprint = {1803.09010},
pages = {1--28},
title = {{Datasheets for Datasets}},
url = {http://arxiv.org/abs/1803.09010},
year = {2018}
}

@article{Mitchell2019,
archivePrefix = {arXiv},
arxivId = {1810.03993},
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
doi = {10.1145/3287560.3287596},
eprint = {1810.03993},
isbn = {9781450361255},
journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
keywords = {Datasheets,Disaggregated evaluation,Documentation,Ethical considerations,Fairness evaluation,ML model evaluation,Model cards},
mendeley-groups = {PhD/Fairness-Statistics in AI},
number = {Figure 2},
pages = {220--229},
title = {{Model cards for model reporting}},
year = {2019}
}

@article{Sendak2020,
author = {Sendak, Mark P. and Gao, Michael and Brajer, Nathan and Balu, Suresh},
doi = {10.1038/s41746-020-0253-3},
file = {:C$\backslash$:/Users/jana.fehr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sendak et al. - 2020 - Presenting machine learning model information to clinical end users with model facts labels.pdf:pdf},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
publisher = {Springer US},
title = {{Presenting machine learning model information to clinical end users with model facts labels}},
url = {http://dx.doi.org/10.1038/s41746-020-0253-3},
volume = {3},
year = {2020}
}

@article{DBLP:journals/corr/abs-1803-09010,
  author    = {Timnit Gebru and
               Jamie Morgenstern and
               Briana Vecchione and
               Jennifer Wortman Vaughan and
               Hanna M. Wallach and
               Hal Daum{\'{e}} III and
               Kate Crawford},
  title     = {Datasheets for Datasets},
  journal   = {CoRR},
  volume    = {abs/1803.09010},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09010},
  archivePrefix = {arXiv},
  eprint    = {1803.09010},
  timestamp = {Mon, 20 Aug 2018 15:16:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-09010.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Topol2019,
author = {Topol, Eric J.},
doi = {10.1038/s41591-018-0300-7},
issn = {1546-170X},
journal = {Nature Medicine},
number = {January},
publisher = {Springer US},
title = {human and artificial intelligence},
url = {http://dx.doi.org/10.1038/s41591-018-0300-7},
volume = {25},
year = {2019}
}
@article{Esteva2019,
author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
doi = {10.1038/s41591-018-0316-z},
issn = {1546170X},
journal = {Nature Medicine},
number = {1},
pages = {24--29},
pmid = {30617335},
publisher = {Springer US},
title = {{A guide to deep learning in healthcare}},
url = {http://dx.doi.org/10.1038/s41591-018-0316-z},
volume = {25},
year = {2019}
}

@article{Gulshan2016,
    author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
    title = "{Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs}",
    journal = {JAMA},
    volume = {316},
    number = {22},
    pages = {2402-2410},
    year = {2016},
    month = {12},
    issn = {0098-7484},
    doi = {10.1001/jama.2016.17216},
    url = {https://doi.org/10.1001/jama.2016.17216},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2588763/joi160132.pdf},
}

@article{Ehteshami2017,
    author = {Ehteshami Bejnordi, Babak and Veta, Mitko and Johannes van Diest, Paul and van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and van der Laak, Jeroen A. W. M. and and the CAMELYON16 Consortium},
    title = "{Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer}",
    journal = {JAMA},
    volume = {318},
    number = {22},
    pages = {2199-2210},
    year = {2017},
    month = {12},
    issn = {0098-7484},
    doi = {10.1001/jama.2017.14585},
    url = {https://doi.org/10.1001/jama.2017.14585},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2665774/jama\_ehteshami\_bejnordi\_2017\_oi\_170113.pdf},
}

@article{Bellamy2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.01943v1},
author = {Bellamy, Rachel K.E. and Kuntal, Dey and Hind, Michael and Hoffmann, Samuel C and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Karthikeyan, Natesan R and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Mohinder and Varshney, Kush R and Yunfeng, Zhang},
doi = {10.1147/JRD.2019.2942287},
eprint = {arXiv:1810.01943v1},
issn = {21518556},
journal = {arXiv},
title = {{AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias}},
year = {2018}
}


@article{Sounderajah2020,
author = {Sounderajah, Viknesh and Ashrafian, Hutan and Aggarwal, Ravi and {De Fauw}, Jeffrey and Denniston, Alastair K. and Greaves, Felix and Karthikesalingam, Alan and King, Dominic and Liu, Xiaoxuan and Markar, Sheraz R. and McInnes, Matthew D.F. and Panch, Trishan and Pearson-Stuttard, Jonathan and Ting, Daniel S.W. and Golub, Robert M. and Moher, David and Bossuyt, Patrick M. and Darzi, Ara},
doi = {10.1038/s41591-020-0941-1},
issn = {1546170X},
journal = {Nature Medicine},
number = {6},
pages = {807--808},
pmid = {32514173},
publisher = {Springer US},
title = {{Developing specific reporting guidelines for diagnostic accuracy studies assessing AI interventions: The STARD-AI Steering Group}},
url = {http://dx.doi.org/10.1038/s41591-020-0941-1},
volume = {26},
year = {2020}
}

@article{Mongan2020,
author = {Mongan, John and Moy, Linda and Kahn, Charles E},
doi = {10.1148/ryai.2020200029},
file = {:C$\backslash$:/Users/jana.fehr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mongan, Moy, Kahn - 2020 - Checklist for Artificial Intelligence and Medical Imaging (Claim).pdf:pdf},
journal = {Radiology: Artificial Intelligence},
title = {{Checklist for Artificial Intelligence and Medical Imaging (Claim)}},
year = {2020}
}

@article{Liu2020,
author = {Liu, Xiaoxuan and {Cruz Rivera}, Samantha and Moher, David and Calvert, Melanie and Denniston, Alastair K and Spirit-ai, The and Group, Consort-ai Working},
doi = {10.1038/s41591-020-1034-x},
file = {:C$\backslash$:/Users/jana.fehr/Documents/Literature/Fairness{\_}Explainability in AI/CONSORT-AI.pdf:pdf},
journal = {Nature Medicine},
mendeley-groups = {PhD/Fairness-Statistics in AI},
number = {September},
pages = {1364--1374},
title = {{CONSORT-AI extension}},
volume = {26},
year = {2020}
}

@article{Rivera2020,
author = {Rivera, Samantha Cruz and Liu, Xiaoxuan and Chan, An-Wen and Denniston, Alastair K and Calvert, Melanie J},
doi = {10.1136/bmj.m3210},
issn = {1756-1833},
journal = {Bmj},
pages = {m3210},
title = {{Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI Extension}},
url = {http://www.bmj.com/lookup/doi/10.1136/bmj.m3210},
volume = {370},
year = {2020}
}

@article{Wiegand2019a,
author = {Wiegand, Thomas and Krishnamurthy, Ramesh and Kuglitsch, Monique and Lee, N. and Pujari, Sameer and Salath{\'{e}}, Marcel and Wenzel, Markus and Xu, Shan},
doi = {10.1016/S0140-6736(19)30762-7},
issn = {1474547X},
journal = {The Lancet},
number = {10192},
pages = {9--11},
pmid = {30935732},
title = {{WHO and ITU establish benchmarking process for artificial intelligence in health}},
volume = {394},
year = {2019}
}

@article{Moons2015,
annote = {Guidelines for reviewing prediction model studies},
author = {Moons, Karel G.M. and Altman, Douglas G. and Reitsma, Johannes B. and Ioannidis, John P.A. and Macaskill, Petra and Steyerberg, Ewout W. and Vickers, Andrew J. and Ransohoff, David F. and Collins, Gary S.},
doi = {10.7326/M14-0698},
issn = {15393704},
journal = {Annals of Internal Medicine},
number = {1},
pages = {W1--W73},
pmid = {25560730},
title = {{Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): Explanation and elaboration}},
volume = {162},
year = {2015}
}

@article{Yau2012,
author = {Yau, Joanne W.Y. and Rogers, Sophie L. and Kawasaki, Rho and Lamoureux, Ecosse L. and Kowalski, Jonathan W. and Bek, Toke and Chen, Shih Jen and Dekker, Jacqueline M. and Fletcher, Astrid and Grauslund, Jakob and Haffner, Steven and Hamman, Richard F. and Ikram, M. Kamran and Kayama, Takamasa and Klein, Barbara E.K. and Klein, Ronald and Krishnaiah, Sannapaneni and Mayurasakorn, Korapat and O'Hare, Joseph P. and Orchard, Trevor J. and Porta, Massimo and Rema, Mohan and Roy, Monique S. and Sharma, Tarun and Shaw, Jonathan and Taylor, Hugh and Tielsch, James M. and Varma, Rohit and Wang, Jie Jin and Wang, Ningli and West, Sheila and Zu, Liang and Yasuda, Miho and Zhang, Xinzhi and Mitchell, Paul and Wong, Tien Y.},
doi = {10.2337/dc11-1909},
file = {:C$\backslash$:/Users/jana.fehr/Documents/Literature/other diseases/Retinopathy.pdf:pdf},
issn = {01495992},
journal = {Diabetes Care},
mendeley-groups = {PhD/other diseases},
number = {3},
pages = {556--564},
pmid = {22301125},
title = {{Global prevalence and major risk factors of diabetic retinopathy}},
volume = {35},
year = {2012}
}

@article{Namperumalsamy2003,
author = {Namperumalsamy, Perumalsamy and Nirmalan, Praveen K. and Ramasamy, Kim},
doi = {10.2337/diacare.26.6.1831},
issn = {01495992},
journal = {Diabetes Care},
number = {6},
pages = {1831--1835},
pmid = {12766118},
title = {{Developing a screening program to detect sight-threatening diabetic retinopathy in South India}},
volume = {26},
year = {2003}
}

@article{Dubois2009,
author = {Dubois, Bruno and Feldman, Howard H. and Jacova, Claudia and Cummings, Jeffrey L. and Dekosky, Steven T. and Barberger-Gateau, Pascale and Delacourte, Andr{\'{e}} and Frisoni, Giovanni and Fox, Nick C and Galasko, Douglas and Gauthier, Serge and Hampel, Harald and Jicha, Gregory A. and Meguro, Kenichi and O'Brien, John and Pasquier, Florence and Robert, Philippe and Rossor, Martin and Salloway, Steven and Sarazin, Marie and de Souza, Leonardo C and Stern, Yaakov and Visser, Pieter J and Scheltens, Philip},
journal = {The Lancet Neurology},
mendeley-groups = {PhD/Alzheimers},
pages = {118--1127},
title = {{Revising the definition of Alzheimer's disease: a new lexicon}},
volume = {9},
year = {2009}
}

@article{Dubois2019a,
author = {Dubois, Bruno and Hampel, Harald and Feldman, Howard H. and Scheltens, Philip and Andrieu, Sandrine and Bakardjian, Hovagim and Benali, Habib and Bertram, Lars and Broich, Karl and Cavedo, Enrica and Crutch, Sebastian and Duyckaerts, Charles and Frisoni, Giovanni B. and Gauthier, Serge and Gouw, Alida A. and Habert, Marie-odile and Holtzman, David M. and Kivipelto, Miia and Lista, Simone and Rabinovici, Gil D. and Rowe, Christopher and Salloway, Stephen and Schneider, Lon S. and Sperling, Reisa and Carrillo, Maria C. and Cummings, Jeffrey and Jr, Cliff R Jack},
doi = {10.1016/j.jalz.2016.02.002.Preclinical},
isbn = {1331421675},
journal = {The Lancet Neurology},
number = {3},
pages = {292--323},
title = {{Preclinical Alzheimer's disease: Definition, natural history, and diagnostic criteria}},
volume = {12},
year = {2009}
}


@article{adni,
annote = {adni original paper},
author = {Mueller, Susanne G and Weiner, Michael W and Thal, Leon J and Petersen, Ronald C and Jack, Clifford and Jagust, William and Trojanowski, John Q and Toga, Arthur W and Beckett, Laurel},
doi = {doi: 10.1016/j.nic.2005.09.008.},
journal = {Neuroimaging Clin N Am.},
mendeley-groups = {PhD/Alzheimers},
number = {4},
pages = {869--77},
title = {{The Alzheimer's disease neuroimaging initiative}},
volume = {15},
year = {2005}
}


@article{Wolff2019,
author = {Wolff, Robert F. and Moons, Karel G.M. and Riley, Richard D. and Whiting, Penny F. and Westwood, Marie and Collins, Gary S. and Reitsma, Johannes B. and Kleijnen, Jos and Mallett, Sue},
doi = {10.7326/M18-1376},
issn = {15393704},
journal = {Annals of Internal Medicine},
number = {1},
pages = {51--58},
title = {{PROBAST: A tool to assess the risk of bias and applicability of prediction model studies}},
volume = {170},
year = {2019}
}


@article{Wiens2019,
author = {Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and Liu, Vincent X. and Doshi-Velez, Finale and Jung, Kenneth and Heller, Katherine and Kale, David and Saeed, Mohammed and Ossorio, Pilar N. and Thadaney-Israni, Sonoo and Goldenberg, Anna},
doi = {10.1038/s41591-019-0548-6},
issn = {1546170X},
journal = {Nature Medicine},
mendeley-groups = {PhD/Fairness-Statistics in AI},
pages = {15--18},
pmid = {31427808},
publisher = {Springer US},
title = {{Do no harm: a roadmap for responsible machine learning for health care}},
url = {http://dx.doi.org/10.1038/s41591-019-0548-6},
year = {2019}
}

@incollection{NIPS2016_6374,
title = {Equality of Opportunity in Supervised Learning},
author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3315--3323},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf}
}


@article{Mendez2015,
author = {Mendez, Mario F},
doi = {10.1016/j.physbeh.2017.03.040},
file = {:C$\backslash$:/Users/jana.fehr/Documents/Literature/Alzheimers/Mendez{\_}EarlyOnsetAD.pdf:pdf},
isbn = {3902264330},
journal = {Neurol. Clin.},
keywords = {endothelium,estrogen,estrogen receptors,vascular smooth muscle},
mendeley-groups = {PhD/Alzheimers},
number = {2},
pages = {263--281},
title = {{Early-Onset Alzheimer's Disease}},
volume = {35},
year = {2017}
}


@article{Gianfrancesco2019,
author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela},
doi = {10.1001/jamainternmed.2018.3763.Potential},
journal = {JAMA Intern. Med.},
mendeley-groups = {PhD/Digital Health},
number = {11},
pages = {1544--1547},
title = {{Potential Biases in ML Algorithms Using EHR Data}},
volume = {178},
year = {2018}
}

@article{Badgeley2019a,
archivePrefix = {arXiv},
arxivId = {1811.03695},
author = {Badgeley, Marcus A. and Zech, John R. and Oakden-Rayner, Luke and Glicksberg, Benjamin S. and Liu, Manway and Gale, William and McConnell, Michael V. and Percha, Bethany and Snyder, Thomas M. and Dudley, Joel T.},
doi = {10.1038/s41746-019-0105-1},
eprint = {1811.03695},
issn = {2398-6352},
journal = {npj Digital Medicine},
number = {1},
title = {{Deep learning predicts hip fracture using confounding patient and healthcare variables}},
volume = {2},
year = {2019}
}

@article{NarayanBiswal2017,
author = {Taqi, Syed Ahmed and Sami, Syed Abdus and Sami, Lateef Begum and Zaki, Syed Ahmed},
doi = {10.4103/jomfp.JOMFP},
issn = {1998393X},
journal = {Journal of oral and Maxillofacial Pathology},
keywords = {753 007,address for correspondence,and microbiology,b,biranchi,biranchi narayan biswal,biswal,c,cancer cell,com,cuttack,dental college and hospital,department of oral pathology,dr,e,gmail,india,mail,metabolic alterations,odisha,s,therapeutics},
number = {3},
pages = {244--51},
title = {{Alteration of cellular metabolism in cancer cells and its therapeutic}},
volume = {21},
year = {2017}
}

@article{Oakden-Rayner2020,
archivePrefix = {arXiv},
arxivId = {1907.12720},
author = {Oakden-Rayner, Luke},
doi = {10.1016/j.acra.2019.10.006},
eprint = {1907.12720},
issn = {18784046},
journal = {Academic Radiology},
keywords = {Artificial intelligence,dataset,deep learning,exploratory analysis,quality control},
number = {1},
pages = {106--112},
publisher = {Elsevier Inc.},
title = {{Exploring Large-scale Public Medical Image Datasets}},
url = {https://doi.org/10.1016/j.acra.2019.10.006},
volume = {27},
year = {2020}
}

@article{Zech2018,
author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
doi = {10.1371/journal.pmed.1002683},
isbn = {1111111111},
issn = {15491676},
journal = {PLoS Medicine},
mendeley-groups = {PhD/Imageanalysis-methods},
number = {11},
pages = {1--17},
pmid = {30399157},
title = {{Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study}},
volume = {15},
year = {2018}
}

@InProceedings{Oala2020,
  title = 	 {ML4H Auditing: From Paper to Practice},
  author =       {Oala, Luis and Fehr, Jana and Gilli, Luca and Balachandran, Pradeep and Leite, Alixandro Werneck and Calderon-Ramirez, Saul and Li, Danny Xie and Nobis, Gabriel and Alvarado, Erick Alejandro Mu\~noz and Jaramillo-Gutierrez, Giovanna and Matek, Christian and Shroff, Arun and Kherif, Ferath and Sanguinetti, Bruno and Wiegand, Thomas},
  booktitle = 	 {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = 	 {280--317},
  year = 	 {2020},
  volume = 	 {136},
  publisher =    {PMLR},
}



@article{Brueggen2017,
author = {Brueggen, Katharina and Grothe, Michel J. and Dyrba, Martin and Fellgiebel, Andreas and Fischer, Florian and Filippi, Massimo and Agosta, Federica and Nestor, Peter and Meisenzahl, Eva and Blautzik, Janusch and Fr{\"{o}}lich, Lutz and Hausner, Lucrezia and Bokde, Arun L.W. and Frisoni, Giovanni and Pievani, Michela and Kl{\"{o}}ppel, Stefan and Prvulovic, David and Barkhof, Frederik and Pouwels, Petra J.W. and Schr{\"{o}}der, Johannes and Hampel, Harald and Hauenstein, Karlheinz and Teipel, Stefan},
doi = {10.1016/j.neuroimage.2016.03.067},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Data sharing,Diffusion tensor imaging,Mild cognitive impairment,Multicenter study},
mendeley-groups = {PhD/Alzheimers},
pages = {305--308},
pmid = {27046114},
publisher = {Elsevier B.V.},
title = {{The European DTI Study on Dementia — A multicenter DTI and MRI study on Alzheimer's disease and Mild Cognitive Impairment}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2016.03.067},
volume = {144},
year = {2017}
}


@article{Eduardo2019,
archivePrefix = {arXiv},
arxivId = {1907.06671},
author = {Eduardo, Sim{\~{a}}o and Naz{\'{a}}bal, Alfredo and Williams, Christopher K. I. and Sutton, Charles},
eprint = {1907.06671},
journal = {arXiv},
title = {{Robust Variational Autoencoders for Outlier Detection and Repair of Mixed-Type Data}},
url = {http://arxiv.org/abs/1907.06671},
year = {2019}
}

@article{cad1,
author = {Fenton, Joshua J. and Taplin, Stephen H. and Carney, Patricia A. and Abraham, Linn and Sickles, Edward A. and D'Orsi, Carl and Berns, Eric A. and Cutter, Gary and Hendrick, Edward and Barlow, William E. and Elmore, Joann G.},
file = {:C$\backslash$:/Users/jana.fehr/Documents/Literature/FGAI4HPaper/nejmoa066099.pdf:pdf},
journal = {The New England Journal of Medicine},
number = {14},
pages = {1399--409},
title = {{Influence of Computer-Aided Detection on Performance of Screening Mammography}},
volume = {356},
year = {2007}
}

@article{tag49,
author = {Collins, Gary S. and Moons, Karel G.},
journal = {The Lancet},
number = {10181},
pages = {1577--1579},
title = {{Reporting of artificial intelligence prediction models}},
url = {https://doi.org/10.1016/S0140-6736(19)30037-6},
volume = {393},
year = {2019}
}

@article{tag71,
archivePrefix = {arXiv},
arxivId = {1902.10178},
author = {Lapuschkin, Sebastian and W{\"{a}}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1038/s41467-019-08987-4},
eprint = {1902.10178},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--8},
pmid = {30858366},
publisher = {Springer US},
title = {{Unmasking Clever Hans predictors and assessing what machines really learn}},
url = {http://dx.doi.org/10.1038/s41467-019-08987-4},
volume = {10},
year = {2019}
}

@article{tag72,
annote = {Meaningful Perturbation for Interpretable Explanations},
archivePrefix = {arXiv},
arxivId = {1704.03296},
author = {Fong, Ruth C. and Vedaldi, Andrea},
doi = {10.1109/ICCV.2017.371},
eprint = {1704.03296},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3449--3457},
title = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
volume = {2017-Octob},
year = {2017}
}

@article{tag74,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.04938v3},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {arXiv:1602.04938v3},
isbn = {9781450342322},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-August-2016},
year = {2016}
}

@article{tag75,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.02391v4},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1007/s11263-019-01228-7},
eprint = {arXiv:1602.04938v3},
isbn = {1573-1405},
journal = {International Journal of Computer Vision},
mendeley-groups = {PhD/FGAI4HPaper},
pages = {336--359},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
volume = {128},
year = {2019}
}

@article{Rajpurkar2018,
author = {Rajpurkar, Pranav and Irvin, Jeremy and Ball, Robyn L. and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis P. and Patel, Bhavik N. and Yeom, Kristen W. and Shpanskaya, Katie and Blankenberg, Francis G. and Seekins, Jayne and Amrhein, Timothy J. and Mong, David A. and Halabi, Safwan S. and Zucker, Evan J. and Ng, Andrew Y. and Lungren, Matthew P.},
doi = {10.1371/journal.pmed.1002686},
isbn = {1111111111},
issn = {15491676},
journal = {PLoS Medicine},
mendeley-groups = {PhD/Deep Learning},
number = {11},
pages = {1--17},
pmid = {30457988},
title = {{Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists}},
volume = {15},
year = {2018}
}


@article{tag76,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.04610v1},
author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
doi = {10.1038/s42256-019-0138-9},
eprint = {arXiv:1905.04610v1},
journal = {Nature Machine Intelligence},
number = {1},
pages = {56--67},
title = {{From local explanations to global understanding with explainable AI for trees}},
volume = {2},
year = {2020}
}

@article{tag78,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
mendeley-groups = {PhD/FGAI4HPaper},
pages = {1527--1535},
title = {{Anchors: High-precision model-agnostic explanations}},
year = {2018}
}


@article{tag79,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Joerg and Xu, Xiaowei},
doi = {10.1016/B978-044452701-1.00067-3},
file = {:C$\backslash$:/Users/jana.fehr/Documents/Literature/FGAI4HPaper/KDD96-037.pdf:pdf},
isbn = {9780444527011},
journal = {KDD Proceedings},
title = {{A density-based algorithm for discovering clusters in large spatial databases with noise}},
year = {1996}
}

@article{tag80,
archivePrefix = {arXiv},
arxivId = {1910.07604},
author = {Pfau, Jacob and Young, Albert T. and Wei, Maria L. and Keiser, Michael J.},
eprint = {1910.07604},
journal = {Machine Learning for Health (ML4H) at NeurIPS 2019},
mendeley-groups = {PhD/FGAI4HPaper},
pages = {1--9},
title = {{Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact Bias}},
url = {http://arxiv.org/abs/1910.07604},
year = {2019}
}

@misc{Xie2016,
      title={Aggregated Residual Transformations for Deep Neural Networks}, 
      author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
      year={2017},
      eprint={1611.05431},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Doehner2017,
author = {D\"{o}hner and Hartmut and Estey and Elihu and Grimwade and David and Amadori and Sergio and Appelbaum and Frederick R and B\"{u}chner and Thomas and Dombret and Herv\'{e} and Ebert and Benjamin L. and Fenaux and Pierre and Larson and Richard A and Levine and Ross L and Lo-Coco and Francesco and Naoe and Tomoki and  Niederwieser and Dietger and Ossenkoppele and Gert J and Sanz and Miguel and Sierra and Jeorge and Tallman and Martin S and Tien and Hwei-Fang and Wei and Andrew H and L\"{o}wenberg and Bob and Bloomfield and Clara D},
journal = {Blood},
volume = {129},
number = {4},
pages = {424--447},
title = {{Diagnosis and management of AML in adults: 2017 ELN recommendations from an international expert panel.}},
url = {https://doi.org/10.1182/blood-2016-08-733196},
year = {2017}
}

@misc{Matek2019a,
      title={A Single-cell Morphological Dataset of Leukocytes from AML Patients and Non-malignant Controls [Data set]. The Cancer Imaging Archive.}, 
      author={Matek, Christian and Schwarz, Simone and Marr, Carsten and Spiekermann, Karsten},
      howpublished= {\url{https://doi.org/10.7937/tcia.2019.36f5o9ld}}}
      
@article{Font2013,
author = {Font P. and Loscertales J. and Benavente C. et al.},
journal = {Ann Hematol},
volume = {93},
pages = {19--24},
title = {{ Inter-observer variance with the diagnosis of myelodysplastic syndromes (MDS) following the 2008 WHO classification.}},
url = {https://doi.org/10.1007/s00277-012-1565-4},
year = {2013}
}

@phdthesis{li2018principled,
  title={Principled approaches to robust machine learning and beyond},
  author={Li, Jerry Zheng},
  year={2018},
  school={Massachusetts Institute of Technology}
}

@inproceedings{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1765--1773},
  year={2017}
}

@inproceedings{paschali2018generalizability,
  title={Generalizability vs. robustness: investigating medical imaging networks using adversarial examples},
  author={Paschali, Magdalini and Conjeti, Sailesh and Navarro, Fernando and Navab, Nassir},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={493--501},
  year={2018},
  organization={Springer}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}


@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={IEEE}
}

@inproceedings{su2018robustness,
  title={Is Robustness the Cost of Accuracy?--A Comprehensive Study on the Robustness of 18 Deep Image Classification Models},
  author={Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={631--648},
  year={2018}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{weng2018evaluating,
  title={Evaluating the robustness of neural networks: An extreme value theory approach},
  author={Weng, Tsui-Wei and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Su, Dong and Gao, Yupeng and Hsieh, Cho-Jui and Daniel, Luca},
  journal={arXiv preprint arXiv:1801.10578},
  year={2018}
}

@article{moradi2015machine,
  title={Machine learning framework for early MRI-based Alzheimer's conversion prediction in MCI subjects},
  author={Moradi, Elaheh and Pepe, Antonietta and Gaser, Christian and Huttunen, Heikki and Tohka, Jussi and Alzheimer's Disease Neuroimaging Initiative and others},
  journal={Neuroimage},
  volume={104},
  pages={398--412},
  year={2015},
  publisher={Elsevier}
}

@article{wenzel2020toward,
  title={Toward Global Validation Standards for Health AI},
  author={Wenzel, Markus and Wiegand, Thomas},
  journal={IEEE Communications Standards Magazine},
  volume={4},
  number={3},
  pages={64--69},
  year={2020},
  publisher={IEEE}
}

@article{nicolae2018adversarial,
  title={Adversarial Robustness Toolbox v1. 0.0},
  author={Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and others},
  journal={arXiv preprint arXiv:1807.01069},
  year={2018}
}

@article{papernot2018cleverhans,
  title={Technical Report on the CleverHans v2.1.0 Adversarial Examples Library},
  author={Nicolas Papernot and Fartash Faghri and Nicholas Carlini and
  Ian Goodfellow and Reuben Feinman and Alexey Kurakin and Cihang Xie and
  Yash Sharma and Tom Brown and Aurko Roy and Alexander Matyasko and
  Vahid Behzadan and Karen Hambardzumyan and Zhishuai Zhang and
  Yi-Lin Juang and Zhi Li and Ryan Sheatsley and Abhibhav Garg and
  Jonathan Uesato and Willi Gierke and Yinpeng Dong and David Berthelot and
  Paul Hendricks and Jonas Rauber and Rujun Long},
  journal={arXiv preprint arXiv:1610.00768},
  year={2018}
}

@article{questionnaire,
    title={Model Questionnaire},
    author={FG-AI4H},
    journal={Reference document J-038 on FG-AI4H server},
    url={https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx},
    year={2020},
}

@article{dashref,
    title={Data sharing practices},
    author={FG-AI4H},
    journal={Reference document DEL 5.6 on FG-AI4H server},
    url={https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx},
    year={2021},
}

@article{daisamref,
    title={Data and Artificial Intelligence Assessment Methods (DAISAM) Reference},
    author={FG-AI4H},
    journal={Reference document DEL 7.3 on FG-AI4H server},
    url={https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx},
    year={2020},
}

@article{ceref,
    title={Clinical evaluation of AI for health},
    author={FG-AI4H},
    journal={Reference document DEL 7.4 on FG-AI4H server},
    url={https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx},
    year={2021},
}

@article{reportingcards,
    title={DAISAM Audit Reporting Template},
    author={FG-AI4H},
    journal={Reference document J-048 on FG-AI4H server},
    url={https://extranet.itu.int/sites/itu-t/focusgroups/ai4h/SitePages/Home.aspx},
    year={2020},
}

@book{samek2019explainable,
  title={Explainable AI: interpreting, explaining and visualizing deep learning},
  author={Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  volume={11700},
  year={2019},
  publisher={Springer Nature}
}

@misc{us2019proposed,
  title={Proposed regulatory framework for modifications to artificial intelligence/machine learning (AI/ML)-based software as a medical device (SAMD)—discussion paper and request for feedback. 2019},
  author={US-FDA},
  year={2019}
}

@article{he2019practical,
  title={The practical implementation of artificial intelligence technologies in medicine},
  author={He, Jianxing and Baxter, Sally L and Xu, Jie and Xu, Jiming and Zhou, Xingtao and Zhang, Kang},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={30--36},
  year={2019},
  publisher={Nature Publishing Group}
}

@misc{imdrf,
  title={Artificial Intelligence in Healthcare
Opportunities and Challenges},
  author={IMDRF},
  year={2019},
  URL={http://imdrf.org/docs/imdrf/final/meetings/imdrf-meet-190916-russia/yekaterinburg-14.pdf}
}

@inproceedings{raji2020closing,
  title={Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing},
  author={Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={33--44},
  year={2020}
}

@misc{MDR,
  title={Regulation (EU) 2017/746 of the European Parliament and of the Council on medical devices},
  author={EU},
  year={2017},
  URL={https://eur-lex.europa.eu/eli/reg/2017/745/oj}
}


@misc{IVDR,
  title={Regulation (EU) 2017/746 of the European Parliament and of the Council on in vitro diagnostic medical devices},
  author={EU},
  year={2017},
  URL={https://eur-lex.europa.eu/eli/reg/2017/746/oj}
}

@misc{21CFR,
  title={Code of Federal Regulations, Title 21 on Foods and Drugs},
  author={FDA},
  URL={https://www.ecfr.gov/cgi-bin/text-idx?SID=cc74806513924f0197b7809c8efbefc8&mc=true&tpl=/ecfrbrowse/Title21/21tab_02.tpl}
}

@misc{IEC62304,
  title={Medical device software — Software life cycle processes — Amendment 1},
  author={IEC},
  year={2015},
  URL={https://www.iso.org/standard/64686.html}
}

@misc{IEC62366-1,
  title={Medical devices — Part 1: Application of usability engineering to medical devices — Amendment 1},
  author={IEC},
  year={2020},
  URL={https://www.iso.org/standard/73007.html}
}

@misc{ISO14971,
  title={Medical devices — Application of risk management to medical devices},
  author={ISO},
  year={2019},
  URL={https://www.iso.org/standard/72704.html}
}

@misc{FDA-Guidance,
  title={FDA Guidance Documents},
  author={FDA},
  URL={https://www.fda.gov/regulatory-information/search-fda-guidance-documents}
}

@misc{IMDRF-Document,
  title={Documents by International Medical Device Regulators Forum},
  author={IMDRF},
  URL={http://www.imdrf.org/documents/documents.asp}
}

@misc{AAMI-TIR57,
  title={Techical Report (TR) 57 Principals for Medical Device Security - Risk Management},
  author={AAMI},
  URL={https://store.aami.org/s/store#/store/browse/detail/a152E000006j60WQAQ}
}

@article{nagendran20,
  title={Artificial intelligence versus clinicians: systematic review of design, reporting standards, and clains of deep learning studies},
  author={Nagendran, Myura and Chen, Yang and Lovejoy, Christopher A and Gordon, Anthony C and Komorowski, Matthieu and Harvey, Hugh and Topol, Eric J and Ionnidis, John P A and Collins, Gary S and Maruthappu, Mahiben},
  journal={British Medical Journal},
  volume={360},
  number={},
  pages={m689},
  year={2020},
  publisher={BMJ}
}

@article{Shah18,
  title={Big data and predictive analytics: recalibrating expectations},
  author={Shah, Nilay D and Steyerberg, Ewout W and Kent, David M},
  journal={JAMA},
  volume={320},
  number={1},
  pages={27-28},
  year={2018},
  publisher={JAMA}
}

@article{Ho19,
  title={Governance of automated image analysis and artificial intelligence analytics in healthcare},
  author={Ho, CWL and Soon, D and Caals, K and Kapur, J},
  journal={Clinical Radiology},
  volume={74},
  number={},
  pages={329-337},
  year={2019},
  publisher={The Royal College of Radiologists}
}
  
  @article{Kelly19,
  title={Key challenges for delivering clinical impact with artificial intelligence},
  author={Kelly, Christopher J and Karthikesalingam, Alan and Suleyman, Mustafa and Corrado, Greg and King, Dominic},
  journal={BMC Medicine},
  volume={17},
  number={},
  pages={195},
  year={2019},
  publisher={Springer Nature}
}

@article{Selvaraju2020,
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1007/s11263-019-01228-7},
eprint = {1610.02391},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Explanations,Grad-CAM,Interpretability,Transparency,Visual explanations,Visualizations},
number = {2},
pages = {336--359},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
volume = {128},
year = {2020}
}

@article{cabitza2020if,
  title={As if sand were stone. New concepts and metrics to probe the ground on which to build trustable AI},
  author={Cabitza, Federico and Campagner, Andrea and Sconfienza, Luca Maria},
  journal={BMC Medical Informatics and Decision Making},
  volume={20},
  number={1},
  pages={1--21},
  year={2020},
  publisher={BioMed Central}
}

@article{IJMEDIChecklist,
title = {The need to separate the wheat from the chaff in medical informatics},
journal = {International Journal of Medical Informatics},
pages = {104510},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104510},
author = {Federico Cabitza and Andrea Campagner}
}

@article{scott2021clinician,
  title={Clinician checklist for assessing suitability of machine learning applications in healthcare},
  author={Scott, Ian and Carter, Stacy and Coiera, Enrico},
  journal={BMJ Health \& Care Informatics},
  volume={28},
  number={1},
  year={2021},
  publisher={BMJ Publishing Group}
}

@article{hernandez2020minimar,
  title={MINIMAR (MINimum Information for Medical AI Reporting): developing reporting standards for artificial intelligence in health care},
  author={Hernandez-Boussard, Tina and Bozkurt, Selen and Ioannidis, John PA and Shah, Nigam H},
  journal={Journal of the American Medical Informatics Association},
  volume={27},
  number={12},
  pages={2011--2015},
  year={2020},
  publisher={Oxford University Press}
}

@article{10.1001/jama.2020.12067,
    author = {Kaushal, Amit and Altman, Russ and Langlotz, Curt},
    title = "{Geographic Distribution of US Cohorts Used to Train Deep Learning Algorithms}",
    journal = {JAMA},
    volume = {324},
    number = {12},
    pages = {1212-1213},
    year = {2020},
    month = {09},
    abstract = "{Advances in machine learning, specifically the subfield of deep learning, have produced algorithms that perform image-based diagnostic tasks with accuracy approaching or exceeding that of trained physicians. Despite their well-documented successes, these machine learning algorithms are vulnerable to cognitive and technical bias, including bias introduced when an insufficient quantity or diversity of data is used to train an algorithm. We investigated an understudied source of systemic bias in clinical applications of deep learning—the geographic distribution of patient cohorts used to train algorithms.}",
    issn = {0098-7484},
    doi = {10.1001/jama.2020.12067},
    url = {https://doi.org/10.1001/jama.2020.12067},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2770833/jama\_kaushal\_2020\_ld\_200073\_1600712104.82262.pdf},
}

@inproceedings{mendez2019using,
  title={Using Cluster Analysis to Assess the Impact of Dataset Heterogeneity on Deep Convolutional Network Accuracy: A First Glance},
  author={Mendez, Mauro and Calderon-Ramirez, Saul and Tyrrell, Pascal N},
  booktitle={Latin American High Performance Computing Conference},
  pages={307--319},
  year={2019},
  organization={Springer}
}

@article{noseworthy2020assessing,
  title={Assessing and mitigating bias in medical artificial intelligence: the effects of race and ethnicity on a deep learning model for ECG analysis},
  author={Noseworthy, Peter A and Attia, Zachi I and Brewer, LaPrincess C and Hayes, Sharonne N and Yao, Xiaoxi and Kapa, Suraj and Friedman, Paul A and Lopez-Jimenez, Francisco},
  journal={Circulation: Arrhythmia and Electrophysiology},
  volume={13},
  number={3},
  pages={e007988},
  year={2020},
  publisher={Am Heart Assoc}
}


@article{balki2019sample,
  title={Sample-size determination methodologies for machine learning in medical imaging research: a systematic review},
  author={Balki, Indranil and Amirabadi, Afsaneh and Levman, Jacob and Martel, Anne L and Emersic, Ziga and Meden, Blaz and Garcia-Pedrero, Angel and Ramirez, Saul C and Kong, Dehan and Moody, Alan R and others},
  journal={Canadian Association of Radiologists Journal},
  volume={70},
  number={4},
  pages={344--353},
  year={2019},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{maartensson2020reliability,
  title={The reliability of a deep learning model in clinical out-of-distribution MRI data: a multicohort study},
  author={M{\aa}rtensson, Gustav and Ferreira, Daniel and Granberg, Tobias and Cavallin, Lena and Oppedal, Ketil and Padovani, Alessandro and Rektorova, Irena and Bonanni, Laura and Pardini, Matteo and Kramberger, Milica G and others},
  journal={Medical Image Analysis},
  volume={66},
  pages={101714},
  year={2020},
  publisher={Elsevier}
}
@article{parmar2018data,
  title={Data analysis strategies in medical imaging},
  author={Parmar, Chintan and Barry, Joseph D and Hosny, Ahmed and Quackenbush, John and Aerts, Hugo JWL},
  journal={Clinical cancer research},
  volume={24},
  number={15},
  pages={3492--3499},
  year={2018},
  publisher={AACR}
}

@article{SamPIEEE21,
  author = {Wojciech Samek and Gr{\'e}goire Montavon and Sebastian Lapuschkin and Christopher J. Anders and Klaus-Robert M{\"u}ller},
  title = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  journal = {Proceedings of the IEEE},
  year = {2021},
  volume = {109},
  number = {3},
  pages = {247-278},
}


@misc{zhao2021robust,
      title={Robust Semi-Supervised Learning with Out of Distribution Data}, 
      author={Xujiang Zhao and Killamsetty Krishnateja and Rishabh Iyer and Feng Chen},
      year={2021},
      eprint={2010.03658},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{chen2020semi,
  title={Semi-supervised learning under class distribution mismatch},
  author={Chen, Yanbei and Zhu, Xiatian and Li, Wei and Gong, Shaogang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3569--3576},
  year={2020}
}

@inproceedings{nalisnick2018deep,
  title={Do Deep Generative Models Know What They Don't Know?},
  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{garczarczyk_interval_2000,
	address = {Geneva, Switzerland},
	title = {Interval neural networks},
	volume = {3},
	isbn = {978-0-7803-5482-1},
	url = {http://ieeexplore.ieee.org/document/856123/},
	doi = {10.1109/ISCAS.2000.856123},
	urldate = {2019-01-16},
	booktitle = {2000 {IEEE} {International} {Symposium} on {Circuits} and {Systems}. {Emerging} {Technologies} for the 21st {Century}. {Proceedings} ({IEEE} {Cat} {No}.00CH36353)},
	publisher = {Presses Polytech. Univ. Romandes},
	author = {Garczarczyk, Z.A.},
	year = {2000},
	pages = {567--570},
	file = {00856123.pdf:/home/donald/Zotero/storage/4RIZPDRR/00856123.pdf:application/pdf}
}

@inproceedings{ronneberger_u-net:_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241}
}

@article{kowalski_interval_2017,
	title = {Interval probabilistic neural network},
	volume = {28},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-015-2109-3},
	doi = {10.1007/s00521-015-2109-3},
	abstract = {Automated classiﬁcation systems have allowed for the rapid development of exploratory data analysis. Such systems increase the independence of human intervention in obtaining the analysis results, especially when inaccurate information is under consideration. The aim of this paper is to present a novel approach, a neural networking, for use in classifying interval information. As presented, neural methodology is a generalization of probabilistic neural network for interval data processing. The simple structure of this neural classiﬁcation algorithm makes it applicable for research purposes. The procedure is based on the Bayes approach, ensuring minimal potential losses with regard to that which comes about through classiﬁcation errors. In this article, the topological structure of the network and the learning process are described in detail. Of note, the correctness of the procedure proposed here has been veriﬁed by way of numerical tests. These tests include examples of both synthetic data, as well as benchmark instances. The results of numerical veriﬁcation, carried out for different shapes of data sets, as well as a comparative analysis with other methods of similar conditioning, have validated both the concept presented here and its positive features.},
	language = {en},
	number = {4},
	urldate = {2019-11-25},
	journal = {Neural Computing and Applications},
	author = {Kowalski, Piotr A. and Kulczycki, Piotr},
	month = apr,
	year = {2017},
	pages = {817--834},
	file = {Kowalski and Kulczycki - 2017 - Interval probabilistic neural network.pdf:/home/donald/Zotero/storage/RF8F88TU/Kowalski and Kulczycki - 2017 - Interval probabilistic neural network.pdf:application/pdf}
}

@article{khosravi_comprehensive_2011,
	title = {Comprehensive {Review} of {Neural} {Network}-{Based} {Prediction} {Intervals} and {New} {Advances}},
	volume = {22},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/5966350/},
	doi = {10.1109/TNN.2011.2162110},
	abstract = {This paper evaluates the four leading techniques proposed in the literature for construction of prediction intervals (PIs) for neural network point forecasts. The delta, Bayesian, bootstrap, and mean-variance estimation (MVE) methods are reviewed and their performance for generating high-quality PIs is compared. PI-based measures are proposed and applied for the objective and quantitative assessment of each method’s performance. A selection of 12 synthetic and real-world case studies is used to examine each method’s performance for PI construction. The comparison is performed on the basis of the quality of generated PIs, the repeatability of the results, the computational requirements and the PIs variability with regard to the data uncertainty. The obtained results in this paper indicate that: 1) the delta and Bayesian methods are the best in terms of quality and repeatability, and 2) the MVE and bootstrap methods are the best in terms of low computational load and the width variability of PIs. This paper also introduces the concept of combinations of PIs, and proposes a new method for generating combined PIs using the traditional PIs. Genetic algorithm is applied for adjusting the combiner parameters through minimization of a PI-based cost function subject to two sets of restrictions. It is shown that the quality of PIs produced by the combiners is dramatically better than the quality of PIs obtained from each individual method.},
	language = {en},
	number = {9},
	urldate = {2019-11-25},
	journal = {IEEE Transactions on Neural Networks},
	author = {Khosravi, A. and Nahavandi, S. and Creighton, D. and Atiya, A. F.},
	month = sep,
	year = {2011},
	pages = {1341--1356},
	file = {Khosravi et al. - 2011 - Comprehensive Review of Neural Network-Based Predi.pdf:/home/donald/Zotero/storage/S6A36ARY/Khosravi et al. - 2011 - Comprehensive Review of Neural Network-Based Predi.pdf:application/pdf}
}

@article{hwang_prediction_1997,
	title = {Prediction {Intervals} for {Artificial} {Neural} {Networks}},
	volume = {92},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1997.10474027},
	doi = {10.1080/01621459.1997.10474027},
	abstract = {The artificial neural network (ANN) is becoming a very popular model for engineering and scientific applications. Inspired by brain architecture, artificial neural networks represent a class of nonlinear models capable of learning from data. Neural networks have been applied in many areas, including pattern matching, classification, prediction, and process control. This article focuses on the construction of prediction intervals. Previous statistical theory for constructing confidence intervals for the parameters (or the weights in an ANN), is inappropriate, because the parameters are unidentifiable. We show in this article that the problem disappears in prediction. We then construct asymptotically valid prediction intervals and also show how to use the prediction intervals to choose the number of nodes in the network. We then apply the theory to an example for predicting the electrical load.},
	number = {438},
	urldate = {2019-11-25},
	journal = {Journal of the American Statistical Association},
	author = {Hwang, J. T. Gene and Ding, A. Adam},
	month = jun,
	year = {1997},
	keywords = {Asymptotic properties, Electric load forecasting, Identifiability, Jackknife estimate of coverage probability, Model selection, Statistical nonlinear regression},
	pages = {748--757},
	file = {Snapshot:/home/donald/Zotero/storage/P4XWYX85/01621459.1997.html:text/html}
}

@article{de_veaux_prediction_1998,
	title = {Prediction {Intervals} for {Neural} {Networks} via {Nonlinear} {Regression}},
	volume = {40},
	issn = {0040-1706},
	url = {www.jstor.org/stable/1270528},
	doi = {10.2307/1270528},
	abstract = {Standard methods for computing prediction intervals in nonlinear regression can be effectively applied to neural networks when the number of training points is large. Simulations show, however, that these methods can generate unreliable prediction intervals on smaller datasets when the network is trained to convergence. Stopping the training algorithm prior to convergence, to avoid overfitting, reduces the effective number of parameters but can lead to prediction intervals that are too wide. We present an alternative approach to estimating prediction intervals using weight decay to fit the network and show via a simulation study that this method may be effective in overcoming some of the shortcomings of the other approaches.},
	number = {4},
	urldate = {2019-11-25},
	journal = {Technometrics},
	author = {de Veaux, Richard D. and Schumi, Jennifer and Schweinsberg, Jason and Ungar, Lyle H.},
	year = {1998},
	pages = {273--282}
}

@article{mackay_evidence_1992,
	title = {The {Evidence} {Framework} {Applied} to {Classification} {Networks}},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.5.720},
	doi = {10.1162/neco.1992.4.5.720},
	language = {en},
	number = {5},
	urldate = {2019-11-25},
	journal = {Neural Computation},
	author = {MacKay, David J. C.},
	month = sep,
	year = {1992},
	pages = {720--736},
	file = {MacKay - 1992 - The Evidence Framework Applied to Classification N.pdf:/home/donald/Zotero/storage/6REJCMH3/MacKay - 1992 - The Evidence Framework Applied to Classification N.pdf:application/pdf}
}

@article{efron_bootstrap_1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364},
	shorttitle = {Bootstrap {Methods}},
	url = {www.jstor.org/stable/2958830},
	abstract = {We discuss the following problem: given a random sample X = (X1, X2, ⋯, Xn) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = θ(F̂) - θ(F), θ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	number = {1},
	urldate = {2019-11-25},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	year = {1979},
	pages = {1--26}
}

@article{heskes_practical_nodate,
	title = {Practical {Confidence} and {Prediction} {Intervals}},
	abstract = {We propose a new method to compute prediction intervals. Especially for small data sets the width of a prediction interval does not only depend on the variance of the target distribution, but also on the accuracy of our estimator of the mean of the target, i.e., on the width of the confidence interval. The confidence interval follows from the variation in an ensemble of neural networks, each of them trained and stopped on bootstrap replicates of the original data set. A second improvement is the use of the residuals on validation patterns instead of on training patterns for estimation of the variance of the target distribution. As illustrated on a synthetic example, our method is better than existing methods with regard to extrapolation and interpolation in data regimes with a limited amount of data, and yields prediction intervals which actual confidence levels are closer to the desired confidence levels.},
	language = {en},
	author = {Heskes, Tom},
	pages = {7},
	file = {Heskes - Practical Confidence and Prediction Intervals.pdf:/home/donald/Zotero/storage/CJMFELY3/Heskes - Practical Confidence and Prediction Intervals.pdf:application/pdf}
}

@article{shafer_tutorial_2008,
	author = {Shafer, Glenn and Vovk, Vladimir},
title = {A Tutorial on Conformal Prediction},
year = {2008},
issue_date = {June 2008},
publisher = {JMLR.org},
volume = {9},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {371–421},
numpages = {51}
}

@incollection{NIPS2019_9547,
title = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D. and Dillon, Joshua and Ren, Jie and Nado, Zachary},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {13969--13980},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf}
}

@article{DBLP:journals/corr/HendrycksG16c,
  author    = {Dan Hendrycks and
               Kevin Gimpel},
  title     = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples
               in Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1610.02136},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02136},
  archivePrefix = {arXiv},
  eprint    = {1610.02136},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HendrycksG16c.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5050--5060},
  year={2019}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{zagoruyko8wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={NIN},
  volume={8},
  pages={35--67}
}

@article{van2020survey,
  title={A survey on semi-supervised learning},
  author={Van Engelen, Jesper E and Hoos, Holger H},
  journal={Machine Learning},
  volume={109},
  number={2},
  pages={373--440},
  year={2020},
  publisher={Springer}
}

@inproceedings{doersch2015unsupervised,
  title={Unsupervised visual representation learning by context prediction},
  author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1422--1430},
  year={2015}
}

@inproceedings{dong2018tri,
  title={Tri-net for semi-supervised deep learning},
  author={Dong-DongChen, WeiWang and WeiGao, Zhi-HuaZhou},
  year={2018},
  organization={IJCAI}
}

@article{cheplygina2019not,
  title={Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
  author={Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien PW},
  journal={Medical image analysis},
  volume={54},
  pages={280--296},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{oliver2018realistic,
  title={Realistic evaluation of deep semi-supervised learning algorithms},
  author={Oliver, Avital and Odena, Augustus and Raffel, Colin A and Cubuk, Ekin Dogus and Goodfellow, Ian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3235--3246},
  year={2018}
}

@article{nair2019realmix,
  title={RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms},
  author={Nair, Varun and Alonso, Javier Fuentes and Beltramelli, Tony},
  journal={arXiv preprint arXiv:1912.08766},
  year={2019}
}

@article{tatti2007distances,
  title={Distances between data sets based on summary statistics},
  author={Tatti, Nikolaj},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Jan},
  pages={131--154},
  year={2007}
}

@article{liu2017survey,
  title={A survey of deep neural network architectures and their applications},
  author={Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
  journal={Neurocomputing},
  volume={234},
  pages={11--26},
  year={2017},
  publisher={Elsevier}
}

@article{balki2019sample,
  title={Sample-Size Determination Methodologies for Machine Learning in Medical Imaging Research: A Systematic Review},
  author={Balki, Indranil and Amirabadi, Afsaneh and Levman, Jacob and Martel, Anne L and Emersic, Ziga and Meden, Blaz and Garcia-Pedrero, Angel and Ramirez, Saul C and Kong, Dehan and Moody, Alan R and others},
  journal={Canadian Association of Radiologists Journal},
  year={2019},
  publisher={Elsevier}
}

@article{zech2018variable,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study},
  author={Zech, John R and Badgeley, Marcus A and Liu, Manway and Costa, Anthony B and Titano, Joseph J and Oermann, Eric Karl},
  journal={PLoS medicine},
  volume={15},
  number={11},
  year={2018},
  publisher={Public Library of Science}
}

@inproceedings{akcay2018ganomaly,
  title={Ganomaly: Semi-supervised anomaly detection via adversarial training},
  author={Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P},
  booktitle={Asian Conference on Computer Vision},
  pages={622--637},
  year={2018},
  organization={Springer}
}
@inproceedings{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7167--7177},
  year={2018}
}

@inproceedings{tagasovska2018frequentist,
  title={Single-Model Uncertainties for Deep Learning},
  author={Tagasovska, Natasa and Lopez-Paz, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6414--6425},
  year={2019}
}

@article{quintanilha2018detecting,
  title={Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics},
  author={Quintanilha, Igor M and de ME Filho, Roberto and Lezama, Jos{\'e} and Delbracio, Mauricio and Nunes, Leonardo O},
  year={2018}
}

@inproceedings{schlegl2017unsupervised,
  title={Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={Information Processing in Medical Imaging: 25th International Conference, IPMI 2017, Boone, NC, USA, June 25-30, 2017, Proceedings},
  volume={10265},
  pages={146},
  year={2017},
  organization={Springer}
}

@inproceedings{pidhorskyi2018generative,
  title={Generative probabilistic novelty detection with adversarial autoencoders},
  author={Pidhorskyi, Stanislav and Almohsen, Ranya and Doretto, Gianfranco},
  booktitle={Advances in neural information processing systems},
  pages={6822--6833},
  year={2018}
}


@article{mozejko2018inhibited,
  title={Inhibited softmax for uncertainty estimation in neural networks},
  author={Mo{\.z}ejko, Marcin and Susik, Mateusz and Karczewski, Rafa{\l}},
  journal={arXiv preprint arXiv:1810.01861},
  year={2018}
}


@article{cabitza2019wants,
  title={Who wants accurate models? Arguing for a different metrics to take classification models seriously},
  author={Cabitza, Federico and Campagner, Andrea},
  journal={arXiv preprint arXiv:1910.09246},
  year={2019}
}

@article{krzanowski2003non,
  title={Non-parametric estimation of distance between groups},
  author={Krzanowski, WJ},
  journal={Journal of Applied Statistics},
  volume={30},
  number={7},
  pages={743--750},
  year={2003},
  publisher={Taylor \& Francis}
}

@inproceedings{cremers2003pseudo,
  title={A pseudo-distance for shape priors in level set segmentation},
  author={Cremers, Daniel and Soatto, Stefano},
  booktitle={2nd IEEE workshop on variational, geometric and level set methods in computer vision},
  pages={169--176},
  year={2003},
  organization={Citeseer}
}

@article{markou2003novelty,
  title={Novelty detection: a review—part 1: statistical approaches},
  author={Markou, Markos and Singh, Sameer},
  journal={Signal processing},
  volume={83},
  number={12},
  pages={2481--2497},
  year={2003},
  publisher={Elsevier}
}

@article{zhou2018brief,
  title={A brief introduction to weakly supervised learning},
  author={Zhou, Zhi-Hua},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={44--53},
  year={2018},
  publisher={Oxford University Press}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={9},
  year={2016},
  publisher={SpringerOpen}
}

@article{wang2017effectiveness,
  title={The effectiveness of data augmentation in image classification using deep learning},
  author={Perez, Luis and Wang, Jason},
  journal={arXiv preprint arXiv:1712.04621},
  year={2017}
}

@inproceedings{ren2019likelihood,
  title={Likelihood ratios for out-of-distribution detection},
  author={Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14680--14691},
  year={2019}
}

@article{ren2018alignment,
  title={Alignment-free sequence analysis and applications},
  author={Ren, Jie and Bai, Xin and Lu, Yang Young and Tang, Kujin and Wang, Ying and Reinert, Gesine and Sun, Fengzhu},
  journal={Annual Review of Biomedical Data Science},
  volume={1},
  pages={93--114},
  year={2018},
  publisher={Annual Reviews}
}

@inproceedings{perera2019deep,
  title={Deep transfer learning for multiple class novelty detection},
  author={Perera, Pramuditha and Patel, Vishal M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={11544--11552},
  year={2019}
}

@article{singh2012outlier,
  title={Outlier detection: applications and techniques},
  author={Singh, Karanjit and Upadhyaya, Shuchita},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={9},
  number={1},
  pages={307},
  year={2012},
  publisher={Citeseer}
}


@inproceedings{hamaguchi2019rare,
  title={Rare Event Detection using Disentangled Representation Learning},
  author={Hamaguchi, Ryuhei and Sakurada, Ken and Nakamura, Ryosuke},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9327--9335},
  year={2019}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}



@article{kuo2020featmatch,
  title={Featmatch: Feature-based augmentation for semi-supervised learning},
  author={Kuo, Chia-Wen and Ma, Chih-Yao and Huang, Jia-Bin and Kira, Zsolt},
  journal={arXiv preprint arXiv:2007.08505},
  year={2020}
}

@article{augustin2020adversarial,
  title={Adversarial Robustness on In-and Out-Distribution Improves Explainability},
  author={Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
  journal={arXiv preprint arXiv:2003.09461},
  year={2020}
}

@article{zhao2020robust,
  title={Robust Semi-Supervised Learning with Out of Distribution Data},
  author={Zhao, Xujiang and Krishnateja, Killamsetty and Iyer, Rishabh and Chen, Feng},
  journal={arXiv preprint arXiv:2010.03658},
  year={2020}
}

@inproceedings{calderon2020dealing,
  title={Dealing with scarce labelled data: Semi-supervised deep learning with mix match for Covid-19 detection using chest X-ray images},
  author={Calderon-Ramirez, Saul and Giri, Raghvendra and Yang, Shengxiang and Moemeni, Armaghan and Umana, Mario and Elizondo, David and Torrents-Barrena, Jordina and Molina-Cabello, Miguel A},
  year={2020},
  organization={IEEE Press}
}

@article{fagerland2009wilcoxon,
  title={The wilcoxon--mann--whitney test under scrutiny},
  author={Fagerland, Morten W and Sandvik, Leiv},
  journal={Statistics in medicine},
  volume={28},
  number={10},
  pages={1487--1497},
  year={2009},
  publisher={Wiley Online Library}
}

@inproceedings{liang2018enhancing,
  title={Enhancing the reliability of out-of-distribution image detection in neural networks},
  author={Liang, Shiyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={6th International Conference on Learning Representations, ICLR 2018},
  year={2018}
}


@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@article{DBLP:journals/corr/HendrycksG16c,
  author    = {Dan Hendrycks and
               Kevin Gimpel},
  title     = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples
               in Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1610.02136},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02136},
  archivePrefix = {arXiv},
  eprint    = {1610.02136},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HendrycksG16c.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5050--5060},
  year={2019}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{zagoruyko8wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={NIN},
  volume={8},
  pages={35--67}
}

@article{van2020survey,
  title={A survey on semi-supervised learning},
  author={Van Engelen, Jesper E and Hoos, Holger H},
  journal={Machine Learning},
  volume={109},
  number={2},
  pages={373--440},
  year={2020},
  publisher={Springer}
}

@inproceedings{doersch2015unsupervised,
  title={Unsupervised visual representation learning by context prediction},
  author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1422--1430},
  year={2015}
}

@inproceedings{dong2018tri,
  title={Tri-net for semi-supervised deep learning},
  author={Dong-DongChen, WeiWang and WeiGao, Zhi-HuaZhou},
  year={2018},
  organization={IJCAI}
}

@article{cheplygina2019not,
  title={Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
  author={Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien PW},
  journal={Medical image analysis},
  volume={54},
  pages={280--296},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{oliver2018realistic,
  title={Realistic evaluation of deep semi-supervised learning algorithms},
  author={Oliver, Avital and Odena, Augustus and Raffel, Colin A and Cubuk, Ekin Dogus and Goodfellow, Ian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3235--3246},
  year={2018}
}

@article{nair2019realmix,
  title={RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms},
  author={Nair, Varun and Alonso, Javier Fuentes and Beltramelli, Tony},
  journal={arXiv preprint arXiv:1912.08766},
  year={2019}
}

@article{tatti2007distances,
  title={Distances between data sets based on summary statistics},
  author={Tatti, Nikolaj},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Jan},
  pages={131--154},
  year={2007}
}

@article{liu2017survey,
  title={A survey of deep neural network architectures and their applications},
  author={Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
  journal={Neurocomputing},
  volume={234},
  pages={11--26},
  year={2017},
  publisher={Elsevier}
}

@article{balki2019sample,
  title={Sample-Size Determination Methodologies for Machine Learning in Medical Imaging Research: A Systematic Review},
  author={Balki, Indranil and Amirabadi, Afsaneh and Levman, Jacob and Martel, Anne L and Emersic, Ziga and Meden, Blaz and Garcia-Pedrero, Angel and Ramirez, Saul C and Kong, Dehan and Moody, Alan R and others},
  journal={Canadian Association of Radiologists Journal},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{liang2018enhancing,
  title={Enhancing the reliability of out-of-distribution image detection in neural networks},
  author={Liang, Shiyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={6th International Conference on Learning Representations, ICLR 2018},
  year={2018}
}

@article{zech2018variable,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study},
  author={Zech, John R and Badgeley, Marcus A and Liu, Manway and Costa, Anthony B and Titano, Joseph J and Oermann, Eric Karl},
  journal={PLoS medicine},
  volume={15},
  number={11},
  year={2018},
  publisher={Public Library of Science}
}

@article{cheplygina2019not,
  title={Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis},
  author={Cheplygina, Veronika and de Bruijne, Marleen and Pluim, Josien PW},
  journal={Medical image analysis},
  volume={54},
  pages={280--296},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{akcay2018ganomaly,
  title={Ganomaly: Semi-supervised anomaly detection via adversarial training},
  author={Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P},
  booktitle={Asian Conference on Computer Vision},
  pages={622--637},
  year={2018},
  organization={Springer}
}
@inproceedings{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7167--7177},
  year={2018}
}

@inproceedings{tagasovska2018frequentist,
  title={Single-Model Uncertainties for Deep Learning},
  author={Tagasovska, Natasa and Lopez-Paz, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6414--6425},
  year={2019}
}

@article{quintanilha2018detecting,
  title={Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics},
  author={Quintanilha, Igor M and de ME Filho, Roberto and Lezama, Jos{\'e} and Delbracio, Mauricio and Nunes, Leonardo O},
  year={2018}
}

@inproceedings{schlegl2017unsupervised,
  title={Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={Information Processing in Medical Imaging: 25th International Conference, IPMI 2017, Boone, NC, USA, June 25-30, 2017, Proceedings},
  volume={10265},
  pages={146},
  year={2017},
  organization={Springer}
}

@inproceedings{pidhorskyi2018generative,
  title={Generative probabilistic novelty detection with adversarial autoencoders},
  author={Pidhorskyi, Stanislav and Almohsen, Ranya and Doretto, Gianfranco},
  booktitle={Advances in neural information processing systems},
  pages={6822--6833},
  year={2018}
}


@article{mozejko2018inhibited,
  title={Inhibited softmax for uncertainty estimation in neural networks},
  author={Mo{\.z}ejko, Marcin and Susik, Mateusz and Karczewski, Rafa{\l}},
  journal={arXiv preprint arXiv:1810.01861},
  year={2018}
}


@article{cabitza2019wants,
  title={Who wants accurate models? Arguing for a different metrics to take classification models seriously},
  author={Cabitza, Federico and Campagner, Andrea},
  journal={arXiv preprint arXiv:1910.09246},
  year={2019}
}

@article{krzanowski2003non,
  title={Non-parametric estimation of distance between groups},
  author={Krzanowski, WJ},
  journal={Journal of Applied Statistics},
  volume={30},
  number={7},
  pages={743--750},
  year={2003},
  publisher={Taylor \& Francis}
}

@inproceedings{cremers2003pseudo,
  title={A pseudo-distance for shape priors in level set segmentation},
  author={Cremers, Daniel and Soatto, Stefano},
  booktitle={2nd IEEE workshop on variational, geometric and level set methods in computer vision},
  pages={169--176},
  year={2003},
  organization={Citeseer}
}

@article{markou2003novelty,
  title={Novelty detection: a review—part 1: statistical approaches},
  author={Markou, Markos and Singh, Sameer},
  journal={Signal processing},
  volume={83},
  number={12},
  pages={2481--2497},
  year={2003},
  publisher={Elsevier}
}

@article{markou2003novelty,
  title={Novelty detection: a review—part 2:: neural network based approaches},
  author={Markou, Markos and Singh, Sameer},
  journal={Signal processing},
  volume={83},
  number={12},
  pages={2499--2521},
  year={2003},
  publisher={Elsevier}
}

@article{zhou2018brief,
  title={A brief introduction to weakly supervised learning},
  author={Zhou, Zhi-Hua},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={44--53},
  year={2018},
  publisher={Oxford University Press}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={9},
  year={2016},
  publisher={SpringerOpen}
}

@article{wang2017effectiveness,
  title={The effectiveness of data augmentation in image classification using deep learning},
  author={Perez, Luis and Wang, Jason},
  journal={arXiv preprint arXiv:1712.04621},
  year={2017}
}

@inproceedings{ren2019likelihood,
  title={Likelihood ratios for out-of-distribution detection},
  author={Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14680--14691},
  year={2019}
}

@article{ren2018alignment,
  title={Alignment-free sequence analysis and applications},
  author={Ren, Jie and Bai, Xin and Lu, Yang Young and Tang, Kujin and Wang, Ying and Reinert, Gesine and Sun, Fengzhu},
  journal={Annual Review of Biomedical Data Science},
  volume={1},
  pages={93--114},
  year={2018},
  publisher={Annual Reviews}
}

@inproceedings{perera2019deep,
  title={Deep transfer learning for multiple class novelty detection},
  author={Perera, Pramuditha and Patel, Vishal M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={11544--11552},
  year={2019}
}

@article{singh2012outlier,
  title={Outlier detection: applications and techniques},
  author={Singh, Karanjit and Upadhyaya, Shuchita},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={9},
  number={1},
  pages={307},
  year={2012},
  publisher={Citeseer}
}


@inproceedings{hamaguchi2019rare,
  title={Rare Event Detection using Disentangled Representation Learning},
  author={Hamaguchi, Ryuhei and Sakurada, Ken and Nakamura, Ryosuke},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9327--9335},
  year={2019}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}


@article{adler_solving_2017,
	title = {Solving ill-posed inverse problems using iterative deep neural networks},
	volume = {33},
	issn = {0266-5611, 1361-6420},
	url = {http://arxiv.org/abs/1704.04058},
	doi = {10.1088/1361-6420/aa9581},
	abstract = {We propose a partially learned approach for the solution of ill posed inverse problems with not necessarily linear forward operators. The method builds on ideas from classical regularization theory and recent advances in deep learning to perform learning while making use of prior information about the inverse problem encoded in the forward operator, noise model and a regularizing functional. The method results in a gradient-like iterative scheme, where the “gradient” component is learned using a convolutional network that includes the gradients of the data discrepancy and regularizer as input in each iteration.},
	language = {en},
	number = {12},
	urldate = {2019-04-15},
	journal = {Inverse Problems},
	author = {Adler, Jonas and Öktem, Ozan},
	month = dec,
	year = {2017},
	note = {arXiv: 1704.04058},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Functional Analysis, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	pages = {124007},
	file = {Adler and Öktem - 2017 - Solving ill-posed inverse problems using iterative.pdf:/home/oala/Zotero/storage/S43TZYGW/Adler and Öktem - 2017 - Solving ill-posed inverse problems using iterative.pdf:application/pdf}
}

@article{adler_deep_2018,
	title = {Deep {Bayesian} {Inversion}},
	url = {http://arxiv.org/abs/1811.05910},
	abstract = {Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion oﬀers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a Wasserstein GAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a “dark spot” in the liver of a cancer stricken patient is present. Both methods are computationally eﬃcient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.},
	language = {en},
	urldate = {2019-04-15},
	journal = {arXiv:1811.05910 [cs, math, stat]},
	author = {Adler, Jonas and Öktem, Ozan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.05910},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {Adler and Öktem - 2018 - Deep Bayesian Inversion.pdf:/home/oala/Zotero/storage/K55734VH/Adler and Öktem - 2018 - Deep Bayesian Inversion.pdf:application/pdf}
}

@inproceedings{barber_ensemble_nodate,
	author = {Barber, D. and Bishop, Christopher},
title = {Ensemble learning in Bayesian neural networks},
booktitle = {Generalization in Neural Networks and Machine Learning},
year = {1998},
month = {January},
abstract = {Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.},
publisher = {Springer Verlag},
pages = {215-237},
edition = {Generalization in Neural Networks and Machine Learning},
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efﬁcient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classiﬁcation. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2019-04-15},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)},
	file = {Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:/home/oala/Zotero/storage/DU7PAWI6/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf}
}

@article{ching_opportunities_2018,
	title = {Opportunities and obstacles for deep learning in biology and medicine},
	volume = {15},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/lookup/doi/10.1098/rsif.2017.0387},
	doi = {10.1098/rsif.2017.0387},
	language = {en},
	number = {141},
	urldate = {2019-04-15},
	journal = {Journal of The Royal Society Interface},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
	month = apr,
	year = {2018},
	pages = {20170387},
	file = {Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:/home/oala/Zotero/storage/4PUEK86R/Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:application/pdf}
}

@article{cressie_statistics_1992,
	title = {{STATISTICS} {FOR} {SPATIAL} {DATA}},
	volume = {4},
	issn = {0954-4879, 1365-3121},
	url = {http://doi.wiley.com/10.1111/j.1365-3121.1992.tb00605.x},
	doi = {10.1111/j.1365-3121.1992.tb00605.x},
	language = {en},
	number = {5},
	urldate = {2019-04-15},
	journal = {Terra Nova},
	author = {Cressie, Noel},
	month = sep,
	year = {1992},
	pages = {613--617}
}

@article{denker_large_1987,
	title = {Large {Automatic} {Learning}, {Rule} {Extraction}, and {Generalization}},
	volume = {1},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Complex Systems},
	author = {Denker, John S. and Schwartz, Daniel B. and Wittner, Ben S. and Solla, Sara A. and Howard, Richard E. and Jackel, Lawrence D. and Hopfield, John J.},
	year = {1987},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/5P46M4VH/Denker et al. - 1987 - Large Automatic Learning, Rule Extraction, and Gen.pdf:application/pdf}
}

@inproceedings{kiureghian_aleatoric_2008,
	title = {Aleatoric or epistemic ? {Does} it matter ?},
	shorttitle = {Aleatoric or epistemic ?},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	author = {Kiureghian, Armen Der and Ditlevsen, Ove D.},
	year = {2008},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/GX2XIPLQ/Kiureghian and Ditlevsen - 2008 - Aleatoric or epistemic  Does it matter .pdf:application/pdf}
}

@inproceedings{gal_dropout_2016,
	title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Yarin Gal and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@article{gast_lightweight_2018,
	title = {Lightweight {Probabilistic} {Deep} {Networks}},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Gast, Jochen and Roth, Stefan},
	year = {2018},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {3369--3378},
	file = {Full Text PDF:/home/oala/Zotero/storage/ADIBWVT8/Gast and Roth - 2018 - Lightweight Probabilistic Deep Networks.pdf:application/pdf}
}

@inproceedings{huang_investigations_2018,
	author="Huang, Yixing
and W{\"u}rfl, Tobias
and Breininger, Katharina
and Liu, Ling
and Lauritsch, G{\"u}nter
and Maier, Andreas",
editor="Handels, Heinz
and Deserno, Thomas M.
and Maier, Andreas
and Maier-Hein, Klaus Hermann
and Palm, Christoph
and Tolxdorff, Thomas",
title="Abstract: Some Investigations on Robustness of Deep Learning in Limited Angle Tomography",
booktitle="Bildverarbeitung f{\"u}r die Medizin 2019",
year="2019",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="21--21",
abstract="In computed tomography, image reconstruction from an insufficient angular range of projection data is called limited angle tomography. Due to missing data, reconstructed images suffer from artifacts, which cause boundary distortion, edge blurring, and intensity biases. Recently, deep learning methods have been applied very successfully to this problem in simulation studies.",
isbn="978-3-658-25326-4"
}

@article{Bubba_2019,
	doi = {10.1088/1361-6420/ab10ca},
	url = {https://doi.org/10.1088%2F1361-6420%2Fab10ca},
	year = 2019,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {35},
	number = {6},
	pages = {064002},
	author = {Tatiana A Bubba and Gitta Kutyniok and Matti Lassas and Maximilian März and Wojciech Samek and Samuli Siltanen and Vignesh Srinivasan},
	title = {Learning the invisible: a hybrid deep learning-shearlet framework for limited angle computed tomography},
	journal = {Inverse Problems},
	abstract = {The high complexity of various inverse problems poses a significant challenge to model-based reconstruction schemes, which in such situations often reach their limits. At the same time, we witness an exceptional success of data-based methodologies such as deep learning. However, in the context of inverse problems, deep neural networks mostly act as black box routines, used for instance for a somewhat unspecified removal of artifacts in classical image reconstructions. In this paper, we will focus on the severely ill-posed inverse problem of limited angle computed tomography, in which entire boundary sections are not captured in the measurements. We will develop a hybrid reconstruction framework that fuses model-based sparse regularization with data-driven deep learning. Our method is reliable in the sense that we only learn the part that can provably not be handled by model-based methods, while applying the theoretically controllable sparse regularization technique to the remaining parts. Such a decomposition into visible and invisible segments is achieved by means of the shearlet transform that allows to resolve wavefront sets in the phase space. Furthermore, this split enables us to assign the clear task of inferring unknown shearlet coefficients to the neural network and thereby offering an interpretation of its performance in the context of limited angle computed tomography. Our numerical experiments show that our algorithm significantly surpasses both pure model- and more data-based reconstruction methods.}
}

@article{jin_deep_2017,
	title = {Deep {Convolutional} {Neural} {Network} for {Inverse} {Problems} in {Imaging}},
	volume = {26},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {IEEE Transactions on Image Processing},
	author = {Jin, Kyong Hwan and McCann, Michael T. and Froustey, Emmanuel and Unser, Michael},
	year = {2017},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {4509--4522},
	file = {Full Text PDF:/home/oala/Zotero/storage/39JC3TQS/Jin et al. - 2017 - Deep Convolutional Neural Network for Inverse Prob.pdf:application/pdf}
}

@inproceedings{kendall_what_2017,
	author = {Kendall, Alex and Gal, Yarin},
title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5580–5590},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS’17}
}

@inproceedings{kingma_variational_2015,
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
title = {Variational Dropout and the Local Reparameterization Trick},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2575–2583},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS’15}
}

@article{mackay_practical_1992,
	title = {A {Practical} {Bayesian} {Framework} for {Backpropagation} {Networks}},
	volume = {4},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Neural Computation},
	author = {MacKay, David J. C.},
	year = {1992},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {448--472},
	file = {Full Text PDF:/home/oala/Zotero/storage/YBT6YQJP/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation.pdf:application/pdf}
}

@book{hinton_bayesian_1995,
	author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
isbn = {0387947248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@inproceedings{nix_estimating_1994,
	title = {Estimating the mean and variance of the target probability distribution},
	volume = {1},
	doi = {10.1109/ICNN.1994.374138},
	abstract = {Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {Proceedings of 1994 {IEEE} {International} {Conference} on {Neural} {Networks} ({ICNN}'94)},
	author = {Nix, D. A. and Weigend, A. S.},
	month = jun,
	year = {1994},
	keywords = {Cognitive science, Computer errors, Computer science, cost function, Cost function, Equations, Error correction, feedforward neural nets, Feedforward systems, Gaussian distribution, Gaussian target error distribution, mean, Measurement uncertainty, Noise level, probability, Probability distribution, target probability distribution, variance, weight-update equations},
	pages = {55--60 vol.1},
	file = {IEEE Xplore Abstract Record:/home/oala/Zotero/storage/ZVZF3ADU/374138.html:text/html;IEEE Xplore Full Text PDF:/home/oala/Zotero/storage/LC32B2T4/Nix and Weigend - 1994 - Estimating the mean and variance of the target pro.pdf:application/pdf}
}

@inproceedings{osband_risk_2016,
	title = {Risk versus {Uncertainty} in {Deep} {Learning} : {Bayes} , {Bootstrap} and the {Dangers} of {Dropout}},
	shorttitle = {Risk versus {Uncertainty} in {Deep} {Learning}},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	author = {Osband, Ian},
	year = {2016},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/DLLXVL3Q/Osband - 2016 - Risk versus Uncertainty in Deep Learning  Bayes ,.pdf:application/pdf}
}

@article{pate-cornell_uncertainties_1996,
	series = {Treatment of {Aleatory} and {Epistemic} {Uncertainty}},
	title = {Uncertainties in risk analysis: {Six} levels of treatment},
	volume = {54},
	issn = {0951-8320},
	shorttitle = {Uncertainties in risk analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832096000671},
	doi = {10.1016/S0951-8320(96)00067-1},
	abstract = {This paper examines different levels of analytical sophistication in the treatment of uncertainties in risk analysis, and the possibility of transfer of experience across fields of application. First, this paper describes deterministic and probabilistic methods of treatment of risk and uncertainties, and the different viewpoints that shape these analyses. Second, six different levels of treatment of uncertainty are presented and discussed in the light of the evolution of the risk management philosophy in the US. Because an in-depth treatment of uncertainties can be complex and costly, this paper then discusses when and why a full (two-tier) uncertainty analysis is justified. In the treatment of epistemic uncertainty, an unavoidable and difficult problem is the encoding of probability distributions based on scientific evidence and expert judgments. The last sections include a description of different approaches to the aggregation of expert opinions and their use in risk analysis, and a recent example of methodology and application (in seismic hazard analysis) that can be transferred to other domains.},
	number = {2},
	urldate = {2019-04-15},
	journal = {Reliability Engineering \& System Safety},
	author = {Paté-Cornell, M. Elisabeth},
	month = nov,
	year = {1996},
	pages = {95--111},
	file = {ScienceDirect Snapshot:/home/oala/Zotero/storage/D6KWGEAU/S0951832096000671.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	shorttitle = {Dropout},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	year = {2014},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {1929--1958},
	file = {Full Text PDF:/home/oala/Zotero/storage/J7MS25QB/Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:application/pdf}
}

@inproceedings{williams_computing_1996,
	author = {Williams, Christopher K. I.},
title = {Computing with Infinite Networks},
year = {1996},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 9th International Conference on Neural Information Processing Systems},
pages = {295–301},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS’96}
}

@article{mccann_review_2017,
	title = {A {Review} of {Convolutional} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	volume = {abs/1710.04011},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {CoRR},
	author = {McCann, Michael T. and Jin, Kyong Hwan and Unser, Michael},
	year = {2017},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/RFAZ4EH6/McCann et al. - 2017 - A Review of Convolutional Neural Networks for Inve.pdf:application/pdf}
}

@phdthesis{mackay_bayesian_1992,
	type = {phd},
	title = {Bayesian methods for adaptive models},
	url = {http://resolver.caltech.edu/CaltechETD:etd-01042007-131447},
	abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better.

When applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance.

Comparisons of the inferences of the Bayesian Framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models.

The relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness data measurements, in the context of both interpolation and classification problems.

The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.},
	urldate = {2019-04-15},
	school = {California Institute of Technology},
	author = {MacKay, David J. C.},
	year = {1992},
	file = {Full Text PDF:/home/oala/Zotero/storage/LXU96JGN/MacKay - 1992 - Bayesian methods for adaptive models.pdf:application/pdf;Snapshot:/home/oala/Zotero/storage/TJ5IBKVU/25.html:text/html}
}


@article{russell_research_2015,
	title = {Research {Priorities} for {Robust} and {Beneficial} {Artificial} {Intelligence}},
	volume = {36},
	issn = {0738-4602, 0738-4602},
	url = {https://aaai.org/ojs/index.php/aimagazine/article/view/2577},
	doi = {10.1609/aimag.v36i4.2577},
	language = {en},
	number = {4},
	urldate = {2019-04-18},
	journal = {AI Magazine},
	author = {Russell, Stuart and Dewey, Daniel and Tegmark, Max},
	month = dec,
	year = {2015},
	pages = {105},
	file = {Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf:/home/oala/Zotero/storage/J4QAT235/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf:application/pdf}
}

@inproceedings{dwork_fairness_2012,
	address = {Cambridge, Massachusetts},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {http://dl.acm.org/citation.cfm?doid=2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classiﬁcation, where individuals are classiﬁed, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classiﬁer (the university). The main conceptual contribution of this paper is a framework for fair classiﬁcation comprising (1) a (hypothetical) task-speciﬁc metric for determining the degree to which individuals are similar with respect to the classiﬁcation task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of “fair aﬃrmative action,” which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classiﬁcation are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of diﬀerential privacy may be applied to fairness.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference} on - {ITCS} '12},
	publisher = {ACM Press},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	pages = {214--226},
	file = {Dwork et al. - 2012 - Fairness through awareness.pdf:/home/oala/Zotero/storage/G83DIVDW/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf}
}

@article{papernot_towards_2016,
	title = {Towards the {Science} of {Security} and {Privacy} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1611.03814},
	abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.},
	urldate = {2019-04-23},
	journal = {arXiv:1611.03814 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03814},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv\:1611.03814 PDF:/home/oala/Zotero/storage/XJR7S8NQ/Papernot et al. - 2016 - Towards the Science of Security and Privacy in Mac.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/QIHG2SZV/1611.html:text/html}
}

@article{gajane_formalizing_2017,
	title = {On {Formalizing} {Fairness} in {Prediction} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1710.03184},
	abstract = {Machine learning algorithms for prediction are increasingly being used in critical decisions affecting human lives. Various fairness formalizations, with no firm consensus yet, are employed to prevent such algorithms from systematically discriminating against people based on certain attributes protected by law. The aim of this article is to survey how fairness is formalized in the machine learning literature for the task of prediction and present these formalizations with their corresponding notions of distributive justice from the social sciences literature. We provide theoretical as well as empirical critiques of these notions from the social sciences literature and explain how these critiques limit the suitability of the corresponding fairness formalizations to certain domains. We also suggest two notions of distributive justice which address some of these critiques and discuss avenues for prospective fairness formalizations.},
	urldate = {2019-04-23},
	journal = {arXiv:1710.03184 [cs, stat]},
	author = {Gajane, Pratik and Pechenizkiy, Mykola},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03184},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.03184 PDF:/home/oala/Zotero/storage/3XP6GPTL/Gajane and Pechenizkiy - 2017 - On Formalizing Fairness in Prediction with Machine.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/9TCWRMZN/1710.html:text/html}
}

@article{weller_challenges_2017,
	title = {Challenges for {Transparency}},
	url = {http://arxiv.org/abs/1708.01870},
	abstract = {Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.},
	urldate = {2019-04-23},
	journal = {arXiv:1708.01870 [cs]},
	author = {Weller, Adrian},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.01870},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: Presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia},
	file = {arXiv\:1708.01870 PDF:/home/oala/Zotero/storage/P4R6595U/Weller - 2017 - Challenges for Transparency.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/P3T7XSWM/1708.html:text/html}
}

@misc{moss_toward_2019,
	address = {Atlanta, Georgia},
	title = {Toward a {Theory} of {Race} for {Fairness} in {Machine} {Learning}},
	url = {https://fatconference.org/2019/acceptedtuts.html},
	urldate = {2019-04-23},
	author = {Moss, Emanuel},
	month = jan,
	year = {2019},
	file = {FAT2019 - Translation Tutorial - Toward a Theory of Race for Fairness in Machine Learning (Moss).pptx:/home/oala/Zotero/storage/CCMEXLVU/FAT2019 - Translation Tutorial - Toward a Theory of Race for Fairness in Machine Learning (Moss).pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation}
}

@article{shaikh_end--end_2017,
	title = {An {End}-{To}-{End} {Machine} {Learning} {Pipeline} {That} {Ensures} {Fairness} {Policies}},
	url = {http://arxiv.org/abs/1710.06876},
	abstract = {In consequential real-world applications, machine learning (ML) based systems are expected to provide fair and non-discriminatory decisions on candidates from groups defined by protected attributes such as gender and race. These expectations are set via policies or regulations governing data usage and decision criteria (sometimes explicitly calling out decisions by automated systems). Often, the data creator, the feature engineer, the author of the algorithm and the user of the results are different entities, making the task of ensuring fairness in an end-to-end ML pipeline challenging. Manually understanding the policies and ensuring fairness in opaque ML systems is time-consuming and error-prone, thus necessitating an end-to-end system that can: 1) understand policies written in natural language, 2) alert users to policy violations during data usage, and 3) log each activity performed using the data in an immutable storage so that policy compliance or violation can be proven later. We propose such a system to ensure that data owners and users are always in compliance with fairness policies.},
	urldate = {2019-04-23},
	journal = {arXiv:1710.06876 [cs]},
	author = {Shaikh, Samiulla and Vishwakarma, Harit and Mehta, Sameep and Varshney, Kush R. and Ramamurthy, Karthikeyan Natesan and Wei, Dennis},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06876},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: Presented at the Data For Good Exchange 2017},
	file = {arXiv\:1710.06876 PDF:/home/oala/Zotero/storage/YDGXZM6E/Shaikh et al. - 2017 - An End-To-End Machine Learning Pipeline That Ensur.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/K8ZGPACL/1710.html:text/html}
}

@techreport{paul_reflecting_2018,
	title = {Reflecting the {Past}, {Shaping} the {Future}: {Making} {AI} {Work} for {International} {Development}},
	url = {https://www.usaid.gov/sites/default/files/documents/15396/AI-ML-in-Development.pdf},
	language = {en},
	urldate = {2019-04-23},
	institution = {USAID},
	author = {Paul, Amy and Jolley, Craig and Anthony, Aubra},
	year = {2018},
	pages = {98},
	file = {Reflecting the Past, Shaping the Future Making AI.pdf:/home/oala/Zotero/storage/TAKS2PNJ/Reflecting the Past, Shaping the Future Making AI.pdf:application/pdf}
}

@misc{dobbe_values_2019,
	address = {Atlanta, Georgia},
	title = {Values, {Reﬂection} and {Engagement} in {Machine} {Learning}},
	url = {https://fatconference.org/2019/acceptedtuts.html},
	language = {en},
	urldate = {2019-04-23},
	author = {Dobbe, Roel and Ames, Morgan G and Berkeley, U C},
	month = jan,
	year = {2019},
	file = {Dobbe et al. - Values, Reﬂection and Engagement in Machine Learni.pdf:/home/oala/Zotero/storage/WNMKJBDE/Dobbe et al. - Values, Reﬂection and Engagement in Machine Learni.pdf:application/pdf}
}

@inproceedings{albarghouthi_fairness-aware_2019,
	address = {Atlanta, GA, USA},
	title = {Fairness-{Aware} {Programming}},
	isbn = {978-1-4503-6125-5},
	url = {http://dl.acm.org/citation.cfm?doid=3287560.3287588},
	doi = {10.1145/3287560.3287588},
	abstract = {Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}  - {FAT}* '19},
	publisher = {ACM Press},
	author = {Albarghouthi, Aws and Vinitsky, Samuel},
	year = {2019},
	pages = {211--219},
	file = {Albarghouthi and Vinitsky - 2019 - Fairness-Aware Programming.pdf:/home/oala/Zotero/storage/FPGZJBMT/Albarghouthi and Vinitsky - 2019 - Fairness-Aware Programming.pdf:application/pdf}
}

@article{holstein_improving_2018,
	title = {Improving fairness in machine learning systems: {What} do industry practitioners need?},
	shorttitle = {Improving fairness in machine learning systems},
	url = {http://arxiv.org/abs/1812.05239},
	doi = {10.1145/3290605.3300830},
	abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.},
	urldate = {2019-04-23},
	journal = {arXiv:1812.05239 [cs]},
	author = {Holstein, Kenneth and Vaughan, Jennifer Wortman and Daumé III, Hal and Dudík, Miro and Wallach, Hanna},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.05239},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Software Engineering},
	annote = {Comment: To appear in the 2019 ACM CHI Conference on Human Factors in Computing Systems (CHI 2019)},
	file = {arXiv\:1812.05239 PDF:/home/oala/Zotero/storage/Q57ZV5LL/Holstein et al. - 2018 - Improving fairness in machine learning systems Wh.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/Y278FJCE/1812.html:text/html}
}

@article{bellamy_ai_2018,
	title = {{AI} {Fairness} 360: {An} {Extensible} {Toolkit} for {Detecting}, {Understanding}, and {Mitigating} {Unwanted} {Algorithmic} {Bias}},
	shorttitle = {{AI} {Fairness} 360},
	url = {http://arxiv.org/abs/1810.01943},
	abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license \{\vphantom{\}}https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.},
	urldate = {2019-04-23},
	journal = {arXiv:1810.01943 [cs]},
	author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01943},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 20 pages},
	file = {arXiv\:1810.01943 PDF:/home/oala/Zotero/storage/6MMD493D/Bellamy et al. - 2018 - AI Fairness 360 An Extensible Toolkit for Detecti.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/GVAMGJVS/1810.html:text/html}
}

@article{friedler_comparative_2018,
	title = {A comparative study of fairness-enhancing interventions in machine learning},
	url = {http://arxiv.org/abs/1802.04422},
	abstract = {Computers are increasingly used to make decisions that have significant impact in people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers and predictors have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions. Concretely, we present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures, and a large number of existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits), indicating that fairness interventions might be more brittle than previously thought.},
	urldate = {2019-04-23},
	journal = {arXiv:1802.04422 [cs, stat]},
	author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04422},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1802.04422 PDF:/home/oala/Zotero/storage/P9UFB5QL/Friedler et al. - 2018 - A comparative study of fairness-enhancing interven.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/JUI4LTKZ/1802.html:text/html}
}

@inproceedings{epstein_turingbox:_2018,
	address = {Stockholm, Sweden},
	title = {{TuringBox}: {An} {Experimental} {Platform} for the {Evaluation} of {AI} {Systems}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {{TuringBox}},
	url = {https://www.ijcai.org/proceedings/2018/851},
	doi = {10.24963/ijcai.2018/851},
	abstract = {We introduce TuringBox, a platform to democratize the study of AI. On one side of the platform, AI contributors upload existing and novel algorithms to be studied scientifically by others. On the other side, AI examiners develop and post machine intelligence tasks to evaluate and characterize the outputs of algorithms. We outline the architecture of such a platform, and describe two interactive case studies of algorithmic auditing on the platform.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Epstein, Ziv and Payne, Blakeley H. and Shen, Judy Hanwen and Hong, Casey Jisoo and Felbo, Bjarke and Dubey, Abhimanyu and Groh, Matthew and Obradovich, Nick and Cebrian, Manuel and Rahwan, Iyad},
	month = jul,
	year = {2018},
	pages = {5826--5828},
	file = {Full Text:/home/oala/Zotero/storage/7T6N3HWF/Epstein et al. - 2018 - TuringBox An Experimental Platform for the Evalua.pdf:application/pdf}
}

@article{voosen_ai_2017,
	title = {The {AI} detectives},
	volume = {357},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/lookup/doi/10.1126/science.357.6346.22},
	doi = {10.1126/science.357.6346.22},
	language = {en},
	number = {6346},
	urldate = {2019-04-23},
	journal = {Science},
	author = {Voosen, Paul},
	month = jul,
	year = {2017},
	pages = {22--27},
	file = {Full Text:/home/oala/Zotero/storage/PFYBQZMF/Voosen - 2017 - The AI detectives.pdf:application/pdf}
}

@misc{gunning_explainable_2017,
	title = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.darpa.mil/attachments/XAIProgramUpdate.pdf},
	language = {en},
	urldate = {2019-04-23},
	author = {Gunning, David},
	month = nov,
	year = {2017},
	file = {Gunning - Explainable Artificial Intelligence (XAI).pdf:/home/oala/Zotero/storage/AAR2IMES/Gunning - Explainable Artificial Intelligence (XAI).pdf:application/pdf}
}

@article{rothblum_probably_2018,
	title = {Probably {Approximately} {Metric}-{Fair} {Learning}},
	url = {http://arxiv.org/abs/1803.03242},
	abstract = {The seminal work of Dwork \{{\textbackslash}em et al.\} [ITCS 2012] introduced a metric-based notion of individual fairness. Given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of \{{\textbackslash}em approximate metric-fairness\}: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness \{{\textbackslash}em does\} generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.},
	urldate = {2019-04-23},
	journal = {arXiv:1803.03242 [cs]},
	author = {Rothblum, Guy N. and Yona, Gal},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03242},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
	annote = {Comment: Published in International Conference on Machine Learning (ICML) 2018},
	file = {arXiv\:1803.03242 PDF:/home/oala/Zotero/storage/9PRZCUEQ/Rothblum and Yona - 2018 - Probably Approximately Metric-Fair Learning.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/YBLJ5NPF/1803.html:text/html}
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	note = {OCLC: 552376743},
	annote = {Includes bibliographical references and indexes},
	file = {Table of Contents PDF:/home/oala/Zotero/storage/GV6GLABQ/Rasmussen and Williams - 2008 - Gaussian processes for machine learning.pdf:application/pdf}
}

@book{bishop_pattern_2009,
	address = {New York, NY},
	edition = {Corrected at 8th printing 2009},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2 978-1-4939-3843-8},
	language = {eng},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2009},
	note = {OCLC: 845772798},
	annote = {Literaturverzeichnis: Seite 711-728 Berichtigter Nachdruck der Originalausgabe von 2006 Hier auch später erschienene, unveränderte Nachdrucke},
	file = {Table of Contents PDF:/home/oala/Zotero/storage/IG8LAL8A/Bishop - 2009 - Pattern recognition and machine learning.pdf:application/pdf}
}



@inproceedings{liang2018enhancing,
  title={Enhancing the reliability of out-of-distribution image detection in neural networks},
  author={Liang, Shiyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={6th International Conference on Learning Representations, ICLR 2018},
  year={2018}
}


@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@article{DBLP:journals/corr/HendrycksG16c,
  author    = {Dan Hendrycks and
               Kevin Gimpel},
  title     = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples
               in Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1610.02136},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02136},
  archivePrefix = {arXiv},
  eprint    = {1610.02136},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HendrycksG16c.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{memoli2011gromov,
  title={Gromov--Wasserstein distances and the metric approach to object matching},
  author={M{\'e}moli, Facundo},
  journal={Foundations of computational mathematics},
  volume={11},
  number={4},
  pages={417--487},
  year={2011},
  publisher={Springer}
}

@inproceedings{10.1145/3128572.3140444,
author = {Carlini, Nicholas and Wagner, David},
title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
year = {2017},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {3–14},
series = {AISec ’17}
}

@article{doi:10.1080/01621459.1984.10477105,
author = { Peter J.   Rousseeuw },
title = {Least Median of Squares Regression},
journal = {Journal of the American Statistical Association},
volume = {79},
number = {388},
pages = {871-880},
year  = {1984},
publisher = {Taylor & Francis},
}


@article{10.1023/B:MACH.0000008084.60811.49,
author = {Tax, David M. J. and Duin, Robert P. W.},
title = {Support Vector Data Description},
year = {2004},
issue_date = {January 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1},
journal = {Mach. Learn.},
pages = {45–66},
}

@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}


@misc{fashionproduct,
author = {Param Aggarwal},
title = {Fashion Product Images (Small)},
howpublished= {\url{https://www.kaggle.com/paramaggarwal/fashion-product-images-small}},
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={618--626},
  year={2017}
}
@article{lin2016intensity,
  title={Intensity and edge based adaptive unsharp masking filter for color image enhancement},
  author={Lin, SCF and Wong, CY and Jiang, G and Rahman, MA and Ren, TR and Kwok, Ngaiming and Shi, Haiyan and Yu, Ying-Hao and Wu, Tonghai},
  journal={Optik},
  volume={127},
  number={1},
  pages={407--414},
  year={2016},
  publisher={Elsevier}
}

@article{sun2018research,
  title={Research progress of visual inspection technology of steel products—A review},
  author={Sun, Xiaohong and Gu, Jinan and Tang, Shixi and Li, Jing},
  journal={Applied Sciences},
  volume={8},
  number={11},
  pages={2195},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@misc{DhaliwalASinghNKapilaR2012,
abstract = {The  discovery  of  x-rays  can  be  perceived  as  the culmination of work of many  years,  beginning as  early as  1821. Roentgen started his work on  x-rays during the  summer of 1894, when he repeated all of Lenard's  published work (on cathode rays and inverse square law), familiarizing himself with the equipment. Following  several  experiments,  Roentgen  discovered  x-rays  on November 8,  1895. Roentgen's  discovery  opened  up  an  exciting field  for  doctors.  It  was  now  possible  to  use  this  new  form  of radiation  in  the  study  of  the  human  body.  Broken  bones,  for example, could now be looked at by using the rays to see straight through  flesh.  The  news  of this  amazing  breakthrough  caused  a major stir in the medical and scientific communities.},
author = {{Dhaliwal A, Singh N, Kapila R}, Rajput R},
mendeley-groups = {report 1 - professional practice},
title = {{(PDF) History of X-Rays in Dentistry}},
url = {https://www.researchgate.net/publication/289626013{\_}History{\_}of{\_}X-Rays{\_}in{\_}Dentistry},
urldate = {2018-10-17},
year = {2012}
}


@misc{photonicsa42800,
abstract = {The dental industry is continually changing, with more and more dentists adopting the latest digital x-ray technology into their practices. The driving advantages of digital x-ray systems versus film-based systems include the following: increased work flow, high-quality x-ray images for improved diagnosis and reduced radiation exposure for patients. Digital systems are based primarily on two sensor technologies: CMOS (complementary metal oxide semiconductor) and CCD (charge-coupled device). This article focuses on the key features and benefits of CMOS technology used in the dental field.},
author = {{Gilmore J, Weldon J}, Lares M},
title = {{CMOS technology for digital dental imaging | Features | Apr 2010 | BioPhotonics}},
url = {https://www.photonics.com/Articles/CMOS{\_}technology{\_}for{\_}digital{\_}dental{\_}imaging/a42008},
urldate = {2018-10-17},
year = {2010}
}
@article{PMC3068829,
abstract = {BACKGROUND This in vitro study was conducted to compare the accuracy of two digital image receptors in identifying the location of tip of a fine endodontic file and radiographic apex in mandibular posterior teeth. METHODS Fourteen human cadaver mandibles with retained molars were selected. These molars were prepared for access to the canals and an endodontic file {\#}10 was introduced into the canal at one of the three random distances from the apex of the tooth. At each distance from the apex and at the apex of the tooth, images were made with two different image receptors; DenOptix storage phosphor plates and Gendex CCD sensor. Six raters viewed all the images for identification of the radiographic apex of the tooth and the tip of the endodontic file. Images were displayed randomly under standardized conditions. To assess intra-rater reliability, all the examiners viewed a subset of randomly selected images again after a time period of one week, inter rater reliability was also assessed. At the end of the study, teeth were extracted and the length of the canals measured to obtain a gold standard. RESULTS T-test revealed a significant main effect for the type of image, indicating that raters' error in identifying structures of interest was significantly higher for Denoptix storage phosphor plates. CONCLUSION The results of the study clearly reveal that Gnedex CCD produce most reliable images for Root Canal working length estimation when compared with Denoptix SPP.},
author = {Anas, A and Asaad, Jm and Tarboush, Ka},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anas, Asaad, Tarboush - 2010 - A Comparison of intra-oral digital imaging modalities Charged Couple Device versus Storage Phosphor Plate.pdf:pdf},
journal = {International journal of health sciences},
month = {nov},
number = {2},
pages = {156--67},
pmid = {21475554},
publisher = {Qassim University},
title = {{A Comparison of intra-oral digital imaging modalities: Charged Couple Device versus Storage Phosphor Plate.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21475554 http://www.ncbi.nlm.nih.gov/pubmed/21475554},
volume = {4},
year = {2010}
}
@techreport{Zhang,
abstract = {In the presence of metal implants, metal artifacts are introduced to x-ray CT images. Although a large number of metal artifact reduction (MAR) methods have been proposed in the past decades, MAR is still one of the major problems in clinical x-ray CT. In this work, we develop a convolutional neural network (CNN) based open MAR framework, which fuses the information from the original and corrected images to suppress artifacts. The proposed approach consists two phases. In the CNN training phase, we build a database consisting of metal-free, metal-inserted and pre-corrected CT images, and image patches are extracted and used for CNN training. In the MAR phase, the uncorrected and pre-corrected images are used as the input of the trained CNN to generate a CNN image with reduced artifacts. To further reduce the remaining artifacts, water equivalent tissues in a CNN image are set to a uniform value to yield a CNN prior, whose forward projections are used to replace the metal-affected projections, followed by the FBP reconstruction. The effectiveness of the proposed method is validated on both simulated and real data. Experimental results demonstrate the superior MAR capability of the proposed method to its competitors in terms of artifact suppression and preservation of anatomical structures in the vicinity of metal implants.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01581v2},
author = {Zhang, Yanbo and Yu, Hengyong},
eprint = {arXiv:1709.01581v2},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Yu - Unknown - Convolutional Neural Network based Metal Artifact Reduction in X-ray Computed Tomography.pdf:pdf},
keywords = {Index Terms-X-ray computed tomography (CT),convolutional neural networks,deep learning,metal arti-facts},
title = {{Convolutional Neural Network based Metal Artifact Reduction in X-ray Computed Tomography}},
url = {https://github.com/yanbozhang007/CNN-MAR.git}
}
@inproceedings{Prajapati2017,
author = {Prajapati, Shreyansh A. and Nagaraj, R. and Mitra, Suman},
booktitle = {2017 5th International Symposium on Computational and Business Intelligence (ISCBI)},
doi = {10.1109/ISCBI.2017.8053547},
isbn = {978-1-5386-1771-7},
month = {aug},
pages = {70--74},
publisher = {IEEE},
title = {{Classification of dental diseases using CNN and transfer learning}},
url = {http://ieeexplore.ieee.org/document/8053547/},
year = {2017}
}
@article{Poedjiastoeti2018,
abstract = {Objectives Ameloblastomas and keratocystic odontogenic tumors (KCOTs) are important odontogenic tumors of the jaw. While their radiological findings are similar, the behaviors of these two types of tumors are different. Precise preoperative diagnosis of these tumors can help oral and maxillofacial surgeons plan appropriate treatment. In this study, we created a convolutional neural network (CNN) for the detection of ameloblastomas and KCOTs. Methods Five hundred digital panoramic images of ameloblastomas and KCOTs were retrospectively collected from a hospital information system, whose patient information could not be identified, and preprocessed by inverse logarithm and histogram equalization. To overcome the imbalance of data entry, we focused our study on 2 tumors with equal distributions of input data. We implemented a transfer learning strategy to overcome the problem of limited patient data. Transfer learning used a 16-layer CNN (VGG-16) of the large sample dataset and was refined with our secondary training dataset comprising 400 images. A separate test dataset comprising 100 images was evaluated to compare the performance of CNN with diagnosis results produced by oral and maxillofacial specialists. Results The sensitivity, specificity, accuracy, and diagnostic time were 81.8{\%}, 83.3{\%}, 83.0{\%}, and 38 seconds, respectively, for the CNN. These values for the oral and maxillofacial specialist were 81.1{\%}, 83.2{\%}, 82.9{\%}, and 23.1 minutes, respectively. Conclusions Ameloblastomas and KCOTs could be detected based on digital panoramic radiographic images using CNN with accuracy comparable to that of manual diagnosis by oral maxillofacial specialists. These results demonstrate that CNN may aid in screening for ameloblastomas and KCOTs in a substantially shorter time.},
author = {Poedjiastoeti, Wiwiek and Suebnukarn, Siriwan},
doi = {10.4258/hir.2018.24.3.236},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poedjiastoeti, Suebnukarn - 2018 - Application of Convolutional Neural Network in the Diagnosis of Jaw Tumors.pdf:pdf},
issn = {2093-3681},
journal = {Healthcare informatics research},
keywords = {Ameloblastoma,Artificial Intelligence,Odontogenic Tumors,Oral and Maxillofacial Surgeons,Panoramic Radiography},
month = {jul},
number = {3},
pages = {236--241},
pmid = {30109156},
publisher = {Korean Society of Medical Informatics},
title = {{Application of Convolutional Neural Network in the Diagnosis of Jaw Tumors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30109156 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6085208},
volume = {24},
year = {2018}
}
@misc{Rouse2018,
author = {Rouse, Margaret and Haughn, Matthew},
title = {{What is convolutional neural network? - Definition}},
url = {https://searchenterpriseai.techtarget.com/definition/convolutional-neural-network},
urldate = {2018-09-21},
year = {2018}
}
@misc{unknown,
author = {Yousefi, Jamileh},
title = {{Image Binarization using Otsu Thresholding Algorithm}},
year = {2015}
}
@misc{Barnard,
author = {Barnard, Kobus},
title = {{Kittler algorithm}},
url = {http://vision.cs.arizona.edu/nvs/research/image{\_}analysis/kittler.html},
urldate = {2018-09-21}
}
@techreport{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by factor of more than 100 in terms of efficiency.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jiirg and Xu, Xiaowei},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.pdf:pdf},
keywords = {Arbitrary Shape of Clus-ters,Clustering Algorithms,Efficiency on Large Spatial Databases,Handling Nlj4-275oise},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {www.aaai.org},
year = {1996}
}
@misc{MELODA,
author = {MELODA},
title = {{Dataset definition}},
url = {http://www.meloda.org/dataset-definition/},
urldate = {2018-09-21}
}
@misc{Christensson2017,
author = {Christensson, P.},
title = {{CMOS Definition.}},
url = {https://techterms.com/definition/cmos},
urldate = {2018-09-21},
year = {2017}
}
@techreport{Ronneberger,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper , we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localiza-tion. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu-ronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04597v1},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {arXiv:1505.04597v1},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - Unknown - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://lmb.informatik.uni-freiburg.de/}
}
@techreport{ada2012,
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2012 - DENTAL RADIOGRAPHIC EXAMINATIONS RECOMMENDATIONS FOR PATIENT SELECTION AND LIMITING RADIATION EXPOSURE REVISED 2012 AME.pdf:pdf},
institution = {ADA},
title = {{Dental radiographic examinations: Recomentations for patient selection and limiting radiation exposure. Revised: 2012 AMERICAN DENTAL ASSOCIATION Council on Scientific Affairs}},
url = {https://www.ada.org/{~}/media/ADA/Member Center/FIles/Dental{\_}Radiographic{\_}Examinations{\_}2012.pdf},
year = {2012}
}
@article{Gulsahi2016,
abstract = {OBJECTIVE The aim of the present study was to evaluate the presence, frequency, and causes of artifacts in intraoral images obtained using photostimulable phosphor (PSP) plates. MATERIALS AND METHODS A total of 11,443 intraoral images, including 4291 periapical and 7152 bitewing images, acquired over a 6-month period as well as over a month 1-year after the initial imaging were evaluated by a single observer and image artifacts only related to the PSP system were recorded. Before the study, an experienced dentomaxillofacial radiologist and a research assistant assessed a set of image artifacts and agreed on the causes of these artifacts. All unidentified artifacts were reassessed by both researchers before the final decision. The data were analyzed using the statistical software SPSS 11.5. RESULTS The total number of images with one or more artifacts was 2344 (20.4{\%}). Of these, 2008 were of adult patients and 336 were of pediatric patients. While movement of the phosphor plate in the disposable pocket was the most common cause of the observed image artifacts in the children, non-uniform image brightness was the most frequently observed artifact in the case of the adults. CONCLUSION The percentage of images with artifacts in the 6th month was lower than that during the 1st month. More significantly, the lowest percentage was obtained 1-year after the initial imaging, owing to the increase in familiarity with the system. Understanding the reasons for the image artifacts and studying ways of preventing are of high clinical importance.},
author = {Gulsahi, A and Secgin, C K},
doi = {10.4103/1119-3077.164338},
issn = {1119-3077},
journal = {Nigerian journal of clinical practice},
number = {2},
pages = {248--53},
pmid = {26856290},
publisher = {Medknow Publications and Media Pvt. Ltd.},
title = {{Assessment of intraoral image artifacts related to photostimulable phosphor plates in a dentomaxillofacial radiology department.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26856290},
volume = {19},
year = {2016}
}
@article{Kalathingal2010,
abstract = {OBJECTIVE The aims of this study were (1) to subjectively quantify the degree of scratching and smudging that had taken place in the junior clinic in the 9 months following the implementation of digital radiology; (2) to compare the findings with a previously published report; and (3) to identify areas in the protocol and training that can be refined to minimize future scratching and smudging. METHODS Seven sets of blank clinical photostimulable storage phosphor (PSP) plates were scanned after exposing them at 65 kV and 7 mA for 0.80 s. Scanned plates were lightly wiped with a soft cloth and alcohol, repackaged in plastic sleeves, re-exposed and rescanned. The two sets of resulting images were subjectively rated independently by two investigators for artefacts and placed in five categories. RESULTS Of all the images, approximately 75{\%} were rated in the top 3 categories (most readable), leaving 17{\%} and 8{\%} in the poor and unsatisfactory categories, respectively. Mean rated values of the two image sets (before and after wiping) were not statistically different, but ratings slightly improved after cleaning the plates. CONCLUSIONS Wiping all plates to remove surface contamination may not always be necessary or desirable. Systems that are designed to minimize handling of the plates may help minimize scratching of them.},
annote = {lo que me puede interesar esq hay como 5 categorias que ellos generan pero son como de intensidad, no de area},
author = {Kalathingal, S M and Shrout, M K and Comer, C and Brady, C},
doi = {10.1259/dmfr/28972644},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalathingal et al. - 2010 - Rating the extent of surface scratches on photostimulable storage phosphor plates in a dental school environ.pdf:pdf},
issn = {0250-832X},
journal = {Dento maxillo facial radiology},
month = {mar},
number = {3},
pages = {179--83},
pmid = {20203281},
publisher = {British Institute of Radiology},
title = {{Rating the extent of surface scratches on photostimulable storage phosphor plates in a dental school environment.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20203281 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3520222},
volume = {39},
year = {2010}
}
@article{AmericanDentalAssociation2011,
abstract = {Dental radiographs (often called x-rays) are
an important part of your dental care.
Along with an oral examination, they provide
your dentist with a more complete
view of what's happening in your mouth},
author = {{American Dental Association}},
file = {:home/miuser/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/American Dental Association - 2011 - Dental radiographs Benefits and safety.pdf:pdf},
journal = {JADA},
number = {9},
pages = {1101},
title = {{Dental radiographs: Benefits and safety}},
url = {https://www.ada.org/{~}/media/ADA/Publications/Files/for{\_}the{\_}dental{\_}patient{\_}sept{\_}2011.pdf?la=en},
volume = {142},
year = {2011}
}


@article{kavitha_diagnosis_2012,
	title = {Diagnosis of osteoporosis from dental panoramic radiographs using the support vector machine method in a computer-aided system},
	volume = {12},
	issn = {1471-2342},
	url = {https://doi.org/10.1186/1471-2342-12-1},
	doi = {10.1186/1471-2342-12-1},
	abstract = {Early diagnosis of osteoporosis can potentially decrease the risk of fractures and improve the quality of life. Detection of thin inferior cortices of the mandible on dental panoramic radiographs could be useful for identifying postmenopausal women with low bone mineral density ({BMD}) or osteoporosis. The aim of our study was to assess the diagnostic efficacy of using kernel-based support vector machine ({SVM}) learning regarding the cortical width of the mandible on dental panoramic radiographs to identify postmenopausal women with low {BMD}.},
	pages = {1},
	number = {1},
	journaltitle = {{BMC} Medical Imaging},
	shortjournal = {{BMC} Medical Imaging},
	author = {Kavitha, M. S. and Asano, Akira and Taguchi, Akira and Kurita, Takio and Sanada, Mitsuhiro},
	urldate = {2018-11-26},
	date = {2012-01-16},
	file = {Full Text PDF:C\:\\Users\\bermu\\Zotero\\storage\\YG98XUIU\\Kavitha et al. - 2012 - Diagnosis of osteoporosis from dental panoramic ra.pdf:application/pdf;Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\9AD3MA6Q\\1471-2342-12-1.html:text/html}
}


@inproceedings{prajapati_classification_2017,
	title = {Classification of dental diseases using {CNN} and transfer learning},
	doi = {10.1109/ISCBI.2017.8053547},
	abstract = {Automated medical assistance system is in high demand with the advances in research in the machine learning area. In many such applications, availability of labeled medical dataset is a primary challenge and dataset of dental diseases is not an exception. An attempt towards accurate classification of dental diseases is addressed in this paper. Labeled dataset consisting of 251 Radio Visiography ({RVG}) x-ray images of 3 different classes is used for classification. Convolutional neural network ({CNN}) has become a most effective tool in machine learning which enables solving the problems like image recognition, segmentation, classification, etc., with high order of accuracy. It is found from literature that {CNN} performs well in natural image classification problems where large dataset is available. In this paper we experimented on the performance of {CNN} for diagnosis of small labeled dental dataset. In addition, transfer learning is used to improve the accuracy. Experimental results are presented for three different architectures of {CNN}. Overall accuracy achieved is very encouraging.},
	eventtitle = {2017 5th International Symposium on Computational and Business Intelligence ({ISCBI})},
	pages = {70--74},
	booktitle = {2017 5th International Symposium on Computational and Business Intelligence ({ISCBI})},
	author = {Prajapati, S. A. and Nagaraj, R. and Mitra, S.},
	date = {2017-08},
	keywords = {automated medical assistance system, {CNN}, Computer architecture, Computer Vision, convolutional neural network, Convolutional Neural Network({CNN}), dental diseases, dentistry, Dentistry, diseases, Diseases, Feature extraction, image classification, image recognition, labeled dental dataset, labeled medical dataset, learning (artificial intelligence), machine learning, Machine Learning ({ML}), medical image processing, natural image classification problems, neural nets, radio visiography x-ray images, Teeth, Training, transfer learning, Transfer Learning ({TL}), X-ray imaging},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bermu\\Zotero\\storage\\I9LDFFSX\\8053547.html:text/html}
}


@inproceedings{karimian_deep_2018,
	title = {Deep learning classifier with optical coherence tomography images for early dental caries detection},
	volume = {10473},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10473/1047304/Deep-learning-classifier-with-optical-coherence-tomography-images-for-early/10.1117/12.2291088.short},
	doi = {10.1117/12.2291088},
	abstract = {Dental caries is a microbial disease that results in localized dissolution of the mineral content of dental tissue. Despite considerable decline in the incidence of dental caries, it remains a major health problem in many societies. Early detection of incipient lesions at initial stages of demineralization can result in the implementation of non-surgical preventive approaches to reverse the demineralization process. In this paper, we present a novel approach combining deep convolutional neural networks ({CNN}) and optical coherence tomography ({OCT}) imaging modality for classification of human oral tissues to detect early dental caries. {OCT} images of oral tissues with various densities were input to a {CNN} classifier to determine variations in tissue densities resembling the demineralization process. The {CNN} automatically learns a hierarchy of increasingly complex features and a related classifier directly from training data sets. The initial {CNN} layer parameters were randomly selected. The training set is split into minibatches, with 10 {OCT} images per batch. Given a batch of training patches, the {CNN} employs two convolutional and pooling layers to extract features and then classify each patch based on the probabilities from the {SoftMax} classification layer (output-layer). Afterward, the {CNN} calculates the error between the classification result and the reference label, and then utilizes the backpropagation process to fine-tune all the layer parameters to minimize this error using batch gradient descent algorithm. We validated our proposed technique on ex-vivo {OCT} images of human oral tissues (enamel, cortical-bone, trabecular-bone, muscular-tissue, and fatty-tissue), which attested to effectiveness of our proposed method.},
	eventtitle = {Lasers in Dentistry {XXIV}},
	pages = {1047304},
	booktitle = {Lasers in Dentistry {XXIV}},
	publisher = {International Society for Optics and Photonics},
	author = {Karimian, Nima and Salehi, Hassan S. and Mahdian, Mina and Alnajjar, Hisham and Tadinada, Aditya},
	urldate = {2018-11-26},
	date = {2018-02-08},
	file = {Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\HTC2UHAW\\12.2291088.html:text/html}
}





@article{lecun_gradient-based_1998,
	title = {Gradient-Based Learning Applied to Document Recognition},
	pages = {46},
	author = {{LeCun}, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	date = {1998},
	langid = {english},
	file = {LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:C\:\\Users\\bermu\\Zotero\\storage\\XUUA32FH\\LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:application/pdf}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2018-11-26},
	date = {2012},
	file = {NIPS Full Text PDF:C\:\\Users\\bermu\\Zotero\\storage\\RNSJAQAJ\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\28ATTCBC\\4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}



@article{polesel_image_2000,
	title = {Image enhancement via adaptive unsharp masking},
	volume = {9},
	issn = {10577149},
	url = {http://ieeexplore.ieee.org/document/826787/},
	doi = {10.1109/83.826787},
	abstract = {This paper presents a new method for unsharp masking for contrast enhancement of images. Our approach employs an adaptive filter that controls the contribution of the sharpening path in such a way that contrast enhancement occurs in high detail areas and little or no image sharpening occurs in smooth areas.},
	pages = {505--510},
	number = {3},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Polesel, A. and Ramponi, G. and Mathews, V.J.},
	urldate = {2018-11-26},
	date = {2000-03},
	langid = {english},
	file = {Polesel et al. - 2000 - Image enhancement via adaptive unsharp masking.pdf:C\:\\Users\\bermu\\Downloads\\Polesel et al. - 2000 - Image enhancement via adaptive unsharp masking.pdf:application/pdf}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{selvaraju_grad-cam:_2016,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	url = {http://arxiv.org/abs/1610.02391},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network ({CNN})-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-{CAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers (e.g. {VGG}), (2) {CNNs} used for structured outputs (e.g. captioning), (3) {CNNs} used in tasks with multi-modal inputs (e.g. {VQA}) or reinforcement learning, without architectural changes or re-training. We combine Grad-{CAM} with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classiﬁcation, image captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on the {ILSVRC}-15 weakly-supervised localization task, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and {VQA}, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-{CAM} explanations help users establish appropriate trust in predictions from deep networks and show that {GradCAM} helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one. Our code is available at https://github.com/ramprs/grad-cam/ and a demo is available on {CloudCV} [2]1. Video of the demo can be found at youtu.be/{COjUB}9Izk6E.},
	journaltitle = {{arXiv}:1610.02391 [cs]},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2018-11-03},
	date = {2016-10-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1610.02391},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Selvaraju et al. - 2016 - Grad-CAM Visual Explanations from Deep Networks v.pdf:C\:\\Users\\bermu\\Downloads\\Selvaraju et al. - 2016 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf}
}

@online{noauthor_opencv:_nodate,
	title = {{OpenCV}: Histograms - 2: Histogram Equalization},
	url = {https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html},
	urldate = {2018-11-06},
	file = {OpenCV\: Histograms - 2\: Histogram Equalization:C\:\\Users\\bermu\\Zotero\\storage\\UM4NXNYN\\tutorial_py_histogram_equalization.html:text/html}
}



@article{mukherjee_estimating_2003,
	title = {Estimating Dataset Size Requirements for Classifying {DNA} Microarray Data},
	volume = {10},
	issn = {1066-5277, 1557-8666},
	url = {http://www.liebertpub.com/doi/10.1089/106652703321825928},
	doi = {10.1089/106652703321825928},
	abstract = {A statistical methodology for estimating dataset size requirements for classifying microarray data using learning curves is introduced. The goal is to use existing classi cation results to estimate dataset size requirements for future classi cation experiments and to evaluate the gain in accuracy and signi cance of classi ers built with additional data. The method is based on tting inverse power-law models to construct empirical learning curves. It also includes a permutation test procedure to assess the statistical signi cance of classi cation performance for a given dataset size. This procedure is applied to several molecular classication problems representing a broad spectrum of levels of complexity.},
	pages = {119--142},
	number = {2},
	journaltitle = {Journal of Computational Biology},
	author = {Mukherjee, Sayan and Tamayo, Pablo and Rogers, Simon and Rifkin, Ryan and Engle, Anna and Campbell, Colin and Golub, Todd R. and Mesirov, Jill P.},
	urldate = {2018-11-26},
	date = {2003-04},
	langid = {english},
	file = {Mukherjee et al. - 2003 - Estimating Dataset Size Requirements for Classifyi.pdf:C\:\\Users\\bermu\\Downloads\\Mukherjee et al. - 2003 - Estimating Dataset Size Requirements for Classifyi.pdf:application/pdf}
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	journaltitle = {{arXiv}:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2018-11-26},
	date = {2014-09-16},
	eprinttype = {arxiv},
	eprint = {1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.4842 PDF:C\:\\Users\\bermu\\Zotero\\storage\\I89RZ5BG\\Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\YMQUDH8D\\1409.html:text/html}
}



@article{simonyan_very_2014,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journaltitle = {{arXiv}:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2018-11-26},
	date = {2014-09-04},
	eprinttype = {arxiv},
	eprint = {1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.1556 PDF:C\:\\Users\\bermu\\Zotero\\storage\\YPDJJGFL\\Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\D5PC34EZ\\1409.html:text/html}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the {ILSVRC} 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
	journaltitle = {{arXiv}:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2018-11-26},
	date = {2015-12-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{figueroa_predicting_2012,
	title = {Predicting sample size required for classification performance},
	volume = {12},
	issn = {1472-6947},
	url = {http://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-12-8},
	doi = {10.1186/1472-6947-12-8},
	abstract = {Background: Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.
Methods: We designed and implemented a method that fits an inverse power law model to points of a given learning curve created using a small annotated training set. Fitting is carried out using nonlinear weighted least squares optimization. The fitted model is then used to predict the classifier’s performance and confidence interval for larger sample sizes. For evaluation, the nonlinear weighted curve fitting method was applied to a set of learning curves generated using clinical text and waveform classification tasks with active and passive sampling methods, and predictions were validated using standard goodness of fit measures. As control we used an unweighted fitting method.
Results: A total of 568 models were fitted and the model predictions were compared with the observed performances. Depending on the data set and sampling method, it took between 80 to 560 annotated samples to achieve mean average and root mean squared error below 0.01. Results also show that our weighted fitting method outperformed the baseline un-weighted method (p {\textless} 0.05).
Conclusions: This paper describes a simple and effective sample size prediction algorithm that conducts weighted fitting of learning curves. The algorithm outperformed an un-weighted algorithm described in previous literature. It can help researchers determine annotation sample size for supervised machine learning.},
	number = {1},
	journaltitle = {{BMC} Medical Informatics and Decision Making},
	author = {Figueroa, Rosa L and Zeng-Treitler, Qing and Kandula, Sasikiran and Ngo, Long H},
	urldate = {2018-11-26},
	date = {2012-12},
	langid = {english},
	file = {Figueroa et al. - 2012 - Predicting sample size required for classification.pdf:C\:\\Users\\bermu\\Zotero\\storage\\7VLDYYVP\\Figueroa et al. - 2012 - Predicting sample size required for classification.pdf:application/pdf}
}


@online{sharma_epoch_2017,
	title = {Epoch vs Batch Size vs Iterations},
	url = {https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9},
	abstract = {Know your code…},
	titleaddon = {Towards Data Science},
	author = {{SHARMA}, {SAGAR}},
	urldate = {2018-11-06},
	date = {2017-09-23},
	file = {Snapshot:C\:\\Users\\bermu\\Zotero\\storage\\DKD9VC38\\epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9.html:text/html}
}

@article{solanki_review_2017,
	title = {A Review on Dental Radiographic Images},
	volume = {07},
	issn = {22489622, 22489622},
	url = {http://www.ijera.com/papers/Vol7_issue7/Part-7/I0707074953.pdf},
	doi = {10.9790/9622-0707074953},
	abstract = {Medical image processing is essential in many fields of medical research and clinical practices because it greatly facilitates early detection and diagnosis of diseases. This paper surveys an add-on approach in the area of medical image analysis for diagnosis of diseases in oral radiology using dental Xrays in dentistry. In case of medical images human involvement and perception is of prime importance. It is indeed a difficult task to interpret fine features in various contrast situations. The raw data obtained directly from X-ray acquisition device may yield a comparatively poor image quality representation. Because of the role of a human (dentist) interpretation based on his knowledge, experience and perception which may differ from doctor to doctor; there are chances of error in deciding the right medical treatment. Software developers along with domain experts have designed various standardized and scientific tools to minimize the human error in the case of deciding the right treatment on the basis of visual perception. One of the aims of this paper is to focus on the extracted part of the tooth from digital dental X-ray, finding the required information in the form of features and helping the dentist in the form of pre-diagnosis suggestions at an early stage.},
	pages = {49--53},
	number = {7},
	journaltitle = {International Journal of Engineering Research and Applications},
	author = {Solanki, A.J. and Mahant, P.M.},
	urldate = {2019-03-25},
	date = {2017-07},
	langid = {english},
	file = {Solanki and Mahant - 2017 - A Review on Dental Radiographic Images.pdf:C\:\\Users\\bermu\\Downloads\\Solanki and Mahant - 2017 - A Review on Dental Radiographic Images.pdf:application/pdf}
}

@inbook{inbook,
author = {Siegmund, Dirk and Prajapati, Ashok and Kirchbuchner, Florian and Kuijper, Arjan},
year = {2018},
month = {09},
pages = {77-84},
title = {An Integrated Deep Neural Network for Defect Detection in Dynamic Textile Textures: 6th International Workshop, IWAIPR 2018, Havana, Cuba, September 24–26, 2018, Proceedings},
isbn = {978-3-030-01131-4},
doi = {10.1007/978-3-030-01132-1_9}
}

@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}

@inproceedings{dai2016r,
  title={R-fcn: Object detection via region-based fully convolutional networks},
  author={Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={379--387},
  year={2016}
}

@inproceedings{calderon2018assessing,
  title={Assessing the impact of the deceived non local means filter as a preprocessing stage in a convolutional neural network based approach for age estimation using digital hand x-ray images},
  author={Calderon, S and Fallas, F and Zumbado, M and Tyrrell, PN and Stark, H and Emersic, Ziga and Meden, Blaz and Solis, M},
  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
  pages={1752--1756},
  year={2018},
  organization={IEEE}
}

@book{girden1992anova,
  title={ANOVA: Repeated measures},
  author={Girden, Ellen R},
  number={84},
  year={1992},
  publisher={Sage}
}


@inproceedings{zisselman2020deep,
  title={Deep Residual Flow for Out of Distribution Detection},
  author={Zisselman, Ev and Tamar, Aviv},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13994--14003},
  year={2020}
}

@inproceedings{chensemi,
  title={Semi-Supervised Learning under Class Distribution Mismatch.},
  author={Chen, Yanbei and Zhu, Xiatian and Li, Wei and Gong, Shaogang},
  booktitle={InAAAI 2020},
  pages={3569-3576},
  year={2020}
   
}

@inproceedings{zhai2019s4l,
  title={S4l: Self-supervised semi-supervised learning},
  author={Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1476--1485},
  year={2019}
}

@inproceedings{zhang2020label,
  title={Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation},
  author={Zhang, Yabin and Deng, Bin and Jia, Kui and Zhang, Lei},
  booktitle={European Conference on Computer Vision},
  pages={781--797},
  year={2020},
  organization={Springer}
}

@inproceedings{chen2020semi,
  title={Semi-Supervised Learning under Class Distribution Mismatch.},
  author={Chen, Yanbei and Zhu, Xiatian and Li, Wei and Gong, Shaogang},
  booktitle={AAAI},
  pages={3569-3576},
  year={2020}
}

@article{webb2016characterizing,
  title={Characterizing concept drift},
  author={Webb, Geoffrey I and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
  journal={Data Mining and Knowledge Discovery},
  volume={30},
  number={4},
  pages={964--994},
  year={2016},
  publisher={Springer}
}

@misc{hendrycks2020augmix,
      title={AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty}, 
      author={Dan Hendrycks and Norman Mu and Ekin D. Cubuk and Barret Zoph and Justin Gilmer and Balaji Lakshminarayanan},
      year={2020},
      eprint={1912.02781},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v136-oala20a,
  title = 	 {ML4H Auditing: From Paper to Practice},
  author =       {Oala, Luis and Fehr, Jana and Gilli, Luca and Balachandran, Pradeep and Leite, Alixandro Werneck and Calderon-Ramirez, Saul and Li, Danny Xie and Nobis, Gabriel and Alvarado, Erick Alejandro Mu\~noz and Jaramillo-Gutierrez, Giovanna and Matek, Christian and Shroff, Arun and Kherif, Ferath and Sanguinetti, Bruno and Wiegand, Thomas},
  booktitle = 	 {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = 	 {280--317},
  year = 	 {2020},
  editor = 	 {Emily Alsentzer and Matthew B. A. McDermott and Fabian Falck and Suproteem K. Sarkar and Subhrajit Roy and Stephanie L. Hyland},
  volume = 	 {136},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v136/oala20a/oala20a.pdf},
  url = 	 {http://proceedings.mlr.press/v136/oala20a.html}
}


@article{adler_solving_2017,
	title = {Solving ill-posed inverse problems using iterative deep neural networks},
	volume = {33},
	issn = {0266-5611, 1361-6420},
	url = {http://arxiv.org/abs/1704.04058},
	doi = {10.1088/1361-6420/aa9581},
	abstract = {We propose a partially learned approach for the solution of ill posed inverse problems with not necessarily linear forward operators. The method builds on ideas from classical regularization theory and recent advances in deep learning to perform learning while making use of prior information about the inverse problem encoded in the forward operator, noise model and a regularizing functional. The method results in a gradient-like iterative scheme, where the “gradient” component is learned using a convolutional network that includes the gradients of the data discrepancy and regularizer as input in each iteration.},
	language = {en},
	number = {12},
	urldate = {2019-04-15},
	journal = {Inverse Problems},
	author = {Adler, Jonas and Öktem, Ozan},
	month = dec,
	year = {2017},
	note = {arXiv: 1704.04058},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Functional Analysis, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	pages = {124007},
	file = {Adler and Öktem - 2017 - Solving ill-posed inverse problems using iterative.pdf:/home/oala/Zotero/storage/S43TZYGW/Adler and Öktem - 2017 - Solving ill-posed inverse problems using iterative.pdf:application/pdf}
}

@article{adler_deep_2018,
	title = {Deep {Bayesian} {Inversion}},
	url = {http://arxiv.org/abs/1811.05910},
	abstract = {Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion oﬀers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a Wasserstein GAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a “dark spot” in the liver of a cancer stricken patient is present. Both methods are computationally eﬃcient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.},
	language = {en},
	urldate = {2019-04-15},
	journal = {arXiv:1811.05910 [cs, math, stat]},
	author = {Adler, Jonas and Öktem, Ozan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.05910},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {Adler and Öktem - 2018 - Deep Bayesian Inversion.pdf:/home/oala/Zotero/storage/K55734VH/Adler and Öktem - 2018 - Deep Bayesian Inversion.pdf:application/pdf}
}

@inproceedings{barber_ensemble_nodate,
	author = {Barber, D. and Bishop, Christopher},
title = {Ensemble learning in Bayesian neural networks},
booktitle = {Generalization in Neural Networks and Machine Learning},
year = {1998},
month = {January},
abstract = {Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.},
publisher = {Springer Verlag},
pages = {215-237},
edition = {Generalization in Neural Networks and Machine Learning},
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efﬁcient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classiﬁcation. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2019-04-15},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)},
	file = {Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:/home/oala/Zotero/storage/DU7PAWI6/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf}
}

@article{ching_opportunities_2018,
	title = {Opportunities and obstacles for deep learning in biology and medicine},
	volume = {15},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/lookup/doi/10.1098/rsif.2017.0387},
	doi = {10.1098/rsif.2017.0387},
	language = {en},
	number = {141},
	urldate = {2019-04-15},
	journal = {Journal of The Royal Society Interface},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
	month = apr,
	year = {2018},
	pages = {20170387},
	file = {Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:/home/oala/Zotero/storage/4PUEK86R/Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:application/pdf}
}

@article{cressie_statistics_1992,
	title = {{STATISTICS} {FOR} {SPATIAL} {DATA}},
	volume = {4},
	issn = {0954-4879, 1365-3121},
	url = {http://doi.wiley.com/10.1111/j.1365-3121.1992.tb00605.x},
	doi = {10.1111/j.1365-3121.1992.tb00605.x},
	language = {en},
	number = {5},
	urldate = {2019-04-15},
	journal = {Terra Nova},
	author = {Cressie, Noel},
	month = sep,
	year = {1992},
	pages = {613--617}
}

@article{denker_large_1987,
	title = {Large {Automatic} {Learning}, {Rule} {Extraction}, and {Generalization}},
	volume = {1},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Complex Systems},
	author = {Denker, John S. and Schwartz, Daniel B. and Wittner, Ben S. and Solla, Sara A. and Howard, Richard E. and Jackel, Lawrence D. and Hopfield, John J.},
	year = {1987},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/5P46M4VH/Denker et al. - 1987 - Large Automatic Learning, Rule Extraction, and Gen.pdf:application/pdf}
}

@inproceedings{kiureghian_aleatoric_2008,
	title = {Aleatoric or epistemic ? {Does} it matter ?},
	shorttitle = {Aleatoric or epistemic ?},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	author = {Kiureghian, Armen Der and Ditlevsen, Ove D.},
	year = {2008},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/GX2XIPLQ/Kiureghian and Ditlevsen - 2008 - Aleatoric or epistemic  Does it matter .pdf:application/pdf}
}

@inproceedings{gal_dropout_2016,
	title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Yarin Gal and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@article{gast_lightweight_2018,
	title = {Lightweight {Probabilistic} {Deep} {Networks}},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Gast, Jochen and Roth, Stefan},
	year = {2018},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {3369--3378},
	file = {Full Text PDF:/home/oala/Zotero/storage/ADIBWVT8/Gast and Roth - 2018 - Lightweight Probabilistic Deep Networks.pdf:application/pdf}
}

@inproceedings{huang_investigations_2018,
	author="Huang, Yixing
and W{\"u}rfl, Tobias
and Breininger, Katharina
and Liu, Ling
and Lauritsch, G{\"u}nter
and Maier, Andreas",
editor="Handels, Heinz
and Deserno, Thomas M.
and Maier, Andreas
and Maier-Hein, Klaus Hermann
and Palm, Christoph
and Tolxdorff, Thomas",
title="Abstract: Some Investigations on Robustness of Deep Learning in Limited Angle Tomography",
booktitle="Bildverarbeitung f{\"u}r die Medizin 2019",
year="2019",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="21--21",
abstract="In computed tomography, image reconstruction from an insufficient angular range of projection data is called limited angle tomography. Due to missing data, reconstructed images suffer from artifacts, which cause boundary distortion, edge blurring, and intensity biases. Recently, deep learning methods have been applied very successfully to this problem in simulation studies.",
isbn="978-3-658-25326-4"
}

@article{Bubba_2019,
	doi = {10.1088/1361-6420/ab10ca},
	url = {https://doi.org/10.1088%2F1361-6420%2Fab10ca},
	year = 2019,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {35},
	number = {6},
	pages = {064002},
	author = {Tatiana A Bubba and Gitta Kutyniok and Matti Lassas and Maximilian März and Wojciech Samek and Samuli Siltanen and Vignesh Srinivasan},
	title = {Learning the invisible: a hybrid deep learning-shearlet framework for limited angle computed tomography},
	journal = {Inverse Problems},
	abstract = {The high complexity of various inverse problems poses a significant challenge to model-based reconstruction schemes, which in such situations often reach their limits. At the same time, we witness an exceptional success of data-based methodologies such as deep learning. However, in the context of inverse problems, deep neural networks mostly act as black box routines, used for instance for a somewhat unspecified removal of artifacts in classical image reconstructions. In this paper, we will focus on the severely ill-posed inverse problem of limited angle computed tomography, in which entire boundary sections are not captured in the measurements. We will develop a hybrid reconstruction framework that fuses model-based sparse regularization with data-driven deep learning. Our method is reliable in the sense that we only learn the part that can provably not be handled by model-based methods, while applying the theoretically controllable sparse regularization technique to the remaining parts. Such a decomposition into visible and invisible segments is achieved by means of the shearlet transform that allows to resolve wavefront sets in the phase space. Furthermore, this split enables us to assign the clear task of inferring unknown shearlet coefficients to the neural network and thereby offering an interpretation of its performance in the context of limited angle computed tomography. Our numerical experiments show that our algorithm significantly surpasses both pure model- and more data-based reconstruction methods.}
}

@article{jin_deep_2017,
	title = {Deep {Convolutional} {Neural} {Network} for {Inverse} {Problems} in {Imaging}},
	volume = {26},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {IEEE Transactions on Image Processing},
	author = {Jin, Kyong Hwan and McCann, Michael T. and Froustey, Emmanuel and Unser, Michael},
	year = {2017},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {4509--4522},
	file = {Full Text PDF:/home/oala/Zotero/storage/39JC3TQS/Jin et al. - 2017 - Deep Convolutional Neural Network for Inverse Prob.pdf:application/pdf}
}

@inproceedings{kendall_what_2017,
	author = {Kendall, Alex and Gal, Yarin},
title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5580–5590},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS’17}
}

@inproceedings{kingma_variational_2015,
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
title = {Variational Dropout and the Local Reparameterization Trick},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2575–2583},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS’15}
}

@article{mackay_practical_1992,
	title = {A {Practical} {Bayesian} {Framework} for {Backpropagation} {Networks}},
	volume = {4},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Neural Computation},
	author = {MacKay, David J. C.},
	year = {1992},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {448--472},
	file = {Full Text PDF:/home/oala/Zotero/storage/YBT6YQJP/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation.pdf:application/pdf}
}

@book{hinton_bayesian_1995,
	author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
isbn = {0387947248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@inproceedings{nix_estimating_1994,
	title = {Estimating the mean and variance of the target probability distribution},
	volume = {1},
	doi = {10.1109/ICNN.1994.374138},
	abstract = {Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {Proceedings of 1994 {IEEE} {International} {Conference} on {Neural} {Networks} ({ICNN}'94)},
	author = {Nix, D. A. and Weigend, A. S.},
	month = jun,
	year = {1994},
	keywords = {Cognitive science, Computer errors, Computer science, cost function, Cost function, Equations, Error correction, feedforward neural nets, Feedforward systems, Gaussian distribution, Gaussian target error distribution, mean, Measurement uncertainty, Noise level, probability, Probability distribution, target probability distribution, variance, weight-update equations},
	pages = {55--60 vol.1},
	file = {IEEE Xplore Abstract Record:/home/oala/Zotero/storage/ZVZF3ADU/374138.html:text/html;IEEE Xplore Full Text PDF:/home/oala/Zotero/storage/LC32B2T4/Nix and Weigend - 1994 - Estimating the mean and variance of the target pro.pdf:application/pdf}
}

@inproceedings{osband_risk_2016,
	title = {Risk versus {Uncertainty} in {Deep} {Learning} : {Bayes} , {Bootstrap} and the {Dangers} of {Dropout}},
	shorttitle = {Risk versus {Uncertainty} in {Deep} {Learning}},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	author = {Osband, Ian},
	year = {2016},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/DLLXVL3Q/Osband - 2016 - Risk versus Uncertainty in Deep Learning  Bayes ,.pdf:application/pdf}
}

@article{pate-cornell_uncertainties_1996,
	series = {Treatment of {Aleatory} and {Epistemic} {Uncertainty}},
	title = {Uncertainties in risk analysis: {Six} levels of treatment},
	volume = {54},
	issn = {0951-8320},
	shorttitle = {Uncertainties in risk analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832096000671},
	doi = {10.1016/S0951-8320(96)00067-1},
	abstract = {This paper examines different levels of analytical sophistication in the treatment of uncertainties in risk analysis, and the possibility of transfer of experience across fields of application. First, this paper describes deterministic and probabilistic methods of treatment of risk and uncertainties, and the different viewpoints that shape these analyses. Second, six different levels of treatment of uncertainty are presented and discussed in the light of the evolution of the risk management philosophy in the US. Because an in-depth treatment of uncertainties can be complex and costly, this paper then discusses when and why a full (two-tier) uncertainty analysis is justified. In the treatment of epistemic uncertainty, an unavoidable and difficult problem is the encoding of probability distributions based on scientific evidence and expert judgments. The last sections include a description of different approaches to the aggregation of expert opinions and their use in risk analysis, and a recent example of methodology and application (in seismic hazard analysis) that can be transferred to other domains.},
	number = {2},
	urldate = {2019-04-15},
	journal = {Reliability Engineering \& System Safety},
	author = {Paté-Cornell, M. Elisabeth},
	month = nov,
	year = {1996},
	pages = {95--111},
	file = {ScienceDirect Snapshot:/home/oala/Zotero/storage/D6KWGEAU/S0951832096000671.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	shorttitle = {Dropout},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	year = {2014},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	pages = {1929--1958},
	file = {Full Text PDF:/home/oala/Zotero/storage/J7MS25QB/Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:application/pdf}
}

@inproceedings{williams_computing_1996,
	author = {Williams, Christopher K. I.},
title = {Computing with Infinite Networks},
year = {1996},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 9th International Conference on Neural Information Processing Systems},
pages = {295–301},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS’96}
}

@article{mccann_review_2017,
	title = {A {Review} of {Convolutional} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	volume = {abs/1710.04011},
	abstract = {Since an tiquity, man has dreamed of building a de vice that would "learn from examples" 1 "form generalizations", and "discover t he rules" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be "curve fit" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n " regularized", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he "human " solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of "program ming" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem ("c l umps") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that "automat ic learn ing will always succeed, given t he right preprocessor," but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the "programming" or "architecture" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be "programmed" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a "7" or a "Q" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could "discover t he rules" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of "test cases" where t here was an alternativeth at is, where the "correct" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, "moment um term s" , "weight decay te rms" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing},
	journal = {CoRR},
	author = {McCann, Michael T. and Jin, Kyong Hwan and Unser, Michael},
	year = {2017},
	keywords = {Computer program, EXA, Ext JS JavaScript Framework, Fortuna (PRNG), Genera, Hopfield network, Illustra, Kludge, Linear algebra, Maxima and minima, Norm (social), Numerical analysis, Pixel, Polynomial, Preprocessor, Programmable logic array, Programmable logic device, Raw image format, Rule induction, Self-information, Signal-to-noise ratio, SONAR (Symantec), Test case, Traction TeamPage, Well-posed problem},
	file = {Full Text PDF:/home/oala/Zotero/storage/RFAZ4EH6/McCann et al. - 2017 - A Review of Convolutional Neural Networks for Inve.pdf:application/pdf}
}

@phdthesis{mackay_bayesian_1992,
	type = {phd},
	title = {Bayesian methods for adaptive models},
	url = {http://resolver.caltech.edu/CaltechETD:etd-01042007-131447},
	abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better.

When applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance.

Comparisons of the inferences of the Bayesian Framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models.

The relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness data measurements, in the context of both interpolation and classification problems.

The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.},
	urldate = {2019-04-15},
	school = {California Institute of Technology},
	author = {MacKay, David J. C.},
	year = {1992},
	file = {Full Text PDF:/home/oala/Zotero/storage/LXU96JGN/MacKay - 1992 - Bayesian methods for adaptive models.pdf:application/pdf;Snapshot:/home/oala/Zotero/storage/TJ5IBKVU/25.html:text/html}
}


@article{russell_research_2015,
	title = {Research {Priorities} for {Robust} and {Beneficial} {Artificial} {Intelligence}},
	volume = {36},
	issn = {0738-4602, 0738-4602},
	url = {https://aaai.org/ojs/index.php/aimagazine/article/view/2577},
	doi = {10.1609/aimag.v36i4.2577},
	language = {en},
	number = {4},
	urldate = {2019-04-18},
	journal = {AI Magazine},
	author = {Russell, Stuart and Dewey, Daniel and Tegmark, Max},
	month = dec,
	year = {2015},
	pages = {105},
	file = {Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf:/home/oala/Zotero/storage/J4QAT235/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf:application/pdf}
}

@inproceedings{dwork_fairness_2012,
	address = {Cambridge, Massachusetts},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {http://dl.acm.org/citation.cfm?doid=2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classiﬁcation, where individuals are classiﬁed, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classiﬁer (the university). The main conceptual contribution of this paper is a framework for fair classiﬁcation comprising (1) a (hypothetical) task-speciﬁc metric for determining the degree to which individuals are similar with respect to the classiﬁcation task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of “fair aﬃrmative action,” which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classiﬁcation are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of diﬀerential privacy may be applied to fairness.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference} on - {ITCS} '12},
	publisher = {ACM Press},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	pages = {214--226},
	file = {Dwork et al. - 2012 - Fairness through awareness.pdf:/home/oala/Zotero/storage/G83DIVDW/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf}
}

@article{papernot_towards_2016,
	title = {Towards the {Science} of {Security} and {Privacy} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1611.03814},
	abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.},
	urldate = {2019-04-23},
	journal = {arXiv:1611.03814 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03814},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv\:1611.03814 PDF:/home/oala/Zotero/storage/XJR7S8NQ/Papernot et al. - 2016 - Towards the Science of Security and Privacy in Mac.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/QIHG2SZV/1611.html:text/html}
}

@article{gajane_formalizing_2017,
	title = {On {Formalizing} {Fairness} in {Prediction} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1710.03184},
	abstract = {Machine learning algorithms for prediction are increasingly being used in critical decisions affecting human lives. Various fairness formalizations, with no firm consensus yet, are employed to prevent such algorithms from systematically discriminating against people based on certain attributes protected by law. The aim of this article is to survey how fairness is formalized in the machine learning literature for the task of prediction and present these formalizations with their corresponding notions of distributive justice from the social sciences literature. We provide theoretical as well as empirical critiques of these notions from the social sciences literature and explain how these critiques limit the suitability of the corresponding fairness formalizations to certain domains. We also suggest two notions of distributive justice which address some of these critiques and discuss avenues for prospective fairness formalizations.},
	urldate = {2019-04-23},
	journal = {arXiv:1710.03184 [cs, stat]},
	author = {Gajane, Pratik and Pechenizkiy, Mykola},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03184},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.03184 PDF:/home/oala/Zotero/storage/3XP6GPTL/Gajane and Pechenizkiy - 2017 - On Formalizing Fairness in Prediction with Machine.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/9TCWRMZN/1710.html:text/html}
}

@article{weller_challenges_2017,
	title = {Challenges for {Transparency}},
	url = {http://arxiv.org/abs/1708.01870},
	abstract = {Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.},
	urldate = {2019-04-23},
	journal = {arXiv:1708.01870 [cs]},
	author = {Weller, Adrian},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.01870},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: Presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia},
	file = {arXiv\:1708.01870 PDF:/home/oala/Zotero/storage/P4R6595U/Weller - 2017 - Challenges for Transparency.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/P3T7XSWM/1708.html:text/html}
}

@misc{moss_toward_2019,
	address = {Atlanta, Georgia},
	title = {Toward a {Theory} of {Race} for {Fairness} in {Machine} {Learning}},
	url = {https://fatconference.org/2019/acceptedtuts.html},
	urldate = {2019-04-23},
	author = {Moss, Emanuel},
	month = jan,
	year = {2019},
	file = {FAT2019 - Translation Tutorial - Toward a Theory of Race for Fairness in Machine Learning (Moss).pptx:/home/oala/Zotero/storage/CCMEXLVU/FAT2019 - Translation Tutorial - Toward a Theory of Race for Fairness in Machine Learning (Moss).pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation}
}

@article{shaikh_end--end_2017,
	title = {An {End}-{To}-{End} {Machine} {Learning} {Pipeline} {That} {Ensures} {Fairness} {Policies}},
	url = {http://arxiv.org/abs/1710.06876},
	abstract = {In consequential real-world applications, machine learning (ML) based systems are expected to provide fair and non-discriminatory decisions on candidates from groups defined by protected attributes such as gender and race. These expectations are set via policies or regulations governing data usage and decision criteria (sometimes explicitly calling out decisions by automated systems). Often, the data creator, the feature engineer, the author of the algorithm and the user of the results are different entities, making the task of ensuring fairness in an end-to-end ML pipeline challenging. Manually understanding the policies and ensuring fairness in opaque ML systems is time-consuming and error-prone, thus necessitating an end-to-end system that can: 1) understand policies written in natural language, 2) alert users to policy violations during data usage, and 3) log each activity performed using the data in an immutable storage so that policy compliance or violation can be proven later. We propose such a system to ensure that data owners and users are always in compliance with fairness policies.},
	urldate = {2019-04-23},
	journal = {arXiv:1710.06876 [cs]},
	author = {Shaikh, Samiulla and Vishwakarma, Harit and Mehta, Sameep and Varshney, Kush R. and Ramamurthy, Karthikeyan Natesan and Wei, Dennis},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06876},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: Presented at the Data For Good Exchange 2017},
	file = {arXiv\:1710.06876 PDF:/home/oala/Zotero/storage/YDGXZM6E/Shaikh et al. - 2017 - An End-To-End Machine Learning Pipeline That Ensur.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/K8ZGPACL/1710.html:text/html}
}

@techreport{paul_reflecting_2018,
	title = {Reflecting the {Past}, {Shaping} the {Future}: {Making} {AI} {Work} for {International} {Development}},
	url = {https://www.usaid.gov/sites/default/files/documents/15396/AI-ML-in-Development.pdf},
	language = {en},
	urldate = {2019-04-23},
	institution = {USAID},
	author = {Paul, Amy and Jolley, Craig and Anthony, Aubra},
	year = {2018},
	pages = {98},
	file = {Reflecting the Past, Shaping the Future Making AI.pdf:/home/oala/Zotero/storage/TAKS2PNJ/Reflecting the Past, Shaping the Future Making AI.pdf:application/pdf}
}

@misc{dobbe_values_2019,
	address = {Atlanta, Georgia},
	title = {Values, {Reﬂection} and {Engagement} in {Machine} {Learning}},
	url = {https://fatconference.org/2019/acceptedtuts.html},
	language = {en},
	urldate = {2019-04-23},
	author = {Dobbe, Roel and Ames, Morgan G and Berkeley, U C},
	month = jan,
	year = {2019},
	file = {Dobbe et al. - Values, Reﬂection and Engagement in Machine Learni.pdf:/home/oala/Zotero/storage/WNMKJBDE/Dobbe et al. - Values, Reﬂection and Engagement in Machine Learni.pdf:application/pdf}
}

@inproceedings{albarghouthi_fairness-aware_2019,
	address = {Atlanta, GA, USA},
	title = {Fairness-{Aware} {Programming}},
	isbn = {978-1-4503-6125-5},
	url = {http://dl.acm.org/citation.cfm?doid=3287560.3287588},
	doi = {10.1145/3287560.3287588},
	abstract = {Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}  - {FAT}* '19},
	publisher = {ACM Press},
	author = {Albarghouthi, Aws and Vinitsky, Samuel},
	year = {2019},
	pages = {211--219},
	file = {Albarghouthi and Vinitsky - 2019 - Fairness-Aware Programming.pdf:/home/oala/Zotero/storage/FPGZJBMT/Albarghouthi and Vinitsky - 2019 - Fairness-Aware Programming.pdf:application/pdf}
}

@article{holstein_improving_2018,
	title = {Improving fairness in machine learning systems: {What} do industry practitioners need?},
	shorttitle = {Improving fairness in machine learning systems},
	url = {http://arxiv.org/abs/1812.05239},
	doi = {10.1145/3290605.3300830},
	abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by industry practitioners and solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address industry practitioners' needs.},
	urldate = {2019-04-23},
	journal = {arXiv:1812.05239 [cs]},
	author = {Holstein, Kenneth and Vaughan, Jennifer Wortman and Daumé III, Hal and Dudík, Miro and Wallach, Hanna},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.05239},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Software Engineering},
	annote = {Comment: To appear in the 2019 ACM CHI Conference on Human Factors in Computing Systems (CHI 2019)},
	file = {arXiv\:1812.05239 PDF:/home/oala/Zotero/storage/Q57ZV5LL/Holstein et al. - 2018 - Improving fairness in machine learning systems Wh.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/Y278FJCE/1812.html:text/html}
}

@article{bellamy_ai_2018,
	title = {{AI} {Fairness} 360: {An} {Extensible} {Toolkit} for {Detecting}, {Understanding}, and {Mitigating} {Unwanted} {Algorithmic} {Bias}},
	shorttitle = {{AI} {Fairness} 360},
	url = {http://arxiv.org/abs/1810.01943},
	abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license \{\vphantom{\}}https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.},
	urldate = {2019-04-23},
	journal = {arXiv:1810.01943 [cs]},
	author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01943},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 20 pages},
	file = {arXiv\:1810.01943 PDF:/home/oala/Zotero/storage/6MMD493D/Bellamy et al. - 2018 - AI Fairness 360 An Extensible Toolkit for Detecti.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/GVAMGJVS/1810.html:text/html}
}

@article{friedler_comparative_2018,
	title = {A comparative study of fairness-enhancing interventions in machine learning},
	url = {http://arxiv.org/abs/1802.04422},
	abstract = {Computers are increasingly used to make decisions that have significant impact in people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers and predictors have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions. Concretely, we present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures, and a large number of existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits), indicating that fairness interventions might be more brittle than previously thought.},
	urldate = {2019-04-23},
	journal = {arXiv:1802.04422 [cs, stat]},
	author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04422},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1802.04422 PDF:/home/oala/Zotero/storage/P9UFB5QL/Friedler et al. - 2018 - A comparative study of fairness-enhancing interven.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/JUI4LTKZ/1802.html:text/html}
}

@inproceedings{epstein_turingbox:_2018,
	address = {Stockholm, Sweden},
	title = {{TuringBox}: {An} {Experimental} {Platform} for the {Evaluation} of {AI} {Systems}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {{TuringBox}},
	url = {https://www.ijcai.org/proceedings/2018/851},
	doi = {10.24963/ijcai.2018/851},
	abstract = {We introduce TuringBox, a platform to democratize the study of AI. On one side of the platform, AI contributors upload existing and novel algorithms to be studied scientifically by others. On the other side, AI examiners develop and post machine intelligence tasks to evaluate and characterize the outputs of algorithms. We outline the architecture of such a platform, and describe two interactive case studies of algorithmic auditing on the platform.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Epstein, Ziv and Payne, Blakeley H. and Shen, Judy Hanwen and Hong, Casey Jisoo and Felbo, Bjarke and Dubey, Abhimanyu and Groh, Matthew and Obradovich, Nick and Cebrian, Manuel and Rahwan, Iyad},
	month = jul,
	year = {2018},
	pages = {5826--5828},
	file = {Full Text:/home/oala/Zotero/storage/7T6N3HWF/Epstein et al. - 2018 - TuringBox An Experimental Platform for the Evalua.pdf:application/pdf}
}

@article{voosen_ai_2017,
	title = {The {AI} detectives},
	volume = {357},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/lookup/doi/10.1126/science.357.6346.22},
	doi = {10.1126/science.357.6346.22},
	language = {en},
	number = {6346},
	urldate = {2019-04-23},
	journal = {Science},
	author = {Voosen, Paul},
	month = jul,
	year = {2017},
	pages = {22--27},
	file = {Full Text:/home/oala/Zotero/storage/PFYBQZMF/Voosen - 2017 - The AI detectives.pdf:application/pdf}
}

@misc{gunning_explainable_2017,
	title = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.darpa.mil/attachments/XAIProgramUpdate.pdf},
	language = {en},
	urldate = {2019-04-23},
	author = {Gunning, David},
	month = nov,
	year = {2017},
	file = {Gunning - Explainable Artificial Intelligence (XAI).pdf:/home/oala/Zotero/storage/AAR2IMES/Gunning - Explainable Artificial Intelligence (XAI).pdf:application/pdf}
}

@article{rothblum_probably_2018,
	title = {Probably {Approximately} {Metric}-{Fair} {Learning}},
	url = {http://arxiv.org/abs/1803.03242},
	abstract = {The seminal work of Dwork \{{\textbackslash}em et al.\} [ITCS 2012] introduced a metric-based notion of individual fairness. Given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of \{{\textbackslash}em approximate metric-fairness\}: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness \{{\textbackslash}em does\} generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.},
	urldate = {2019-04-23},
	journal = {arXiv:1803.03242 [cs]},
	author = {Rothblum, Guy N. and Yona, Gal},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03242},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
	annote = {Comment: Published in International Conference on Machine Learning (ICML) 2018},
	file = {arXiv\:1803.03242 PDF:/home/oala/Zotero/storage/9PRZCUEQ/Rothblum and Yona - 2018 - Probably Approximately Metric-Fair Learning.pdf:application/pdf;arXiv.org Snapshot:/home/oala/Zotero/storage/YBLJ5NPF/1803.html:text/html}
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	note = {OCLC: 552376743},
	annote = {Includes bibliographical references and indexes},
	file = {Table of Contents PDF:/home/oala/Zotero/storage/GV6GLABQ/Rasmussen and Williams - 2008 - Gaussian processes for machine learning.pdf:application/pdf}
}

@book{bishop_pattern_2009,
	address = {New York, NY},
	edition = {Corrected at 8th printing 2009},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2 978-1-4939-3843-8},
	language = {eng},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2009},
	note = {OCLC: 845772798},
	annote = {Literaturverzeichnis: Seite 711-728 Berichtigter Nachdruck der Originalausgabe von 2006 Hier auch später erschienene, unveränderte Nachdrucke},
	file = {Table of Contents PDF:/home/oala/Zotero/storage/IG8LAL8A/Bishop - 2009 - Pattern recognition and machine learning.pdf:application/pdf}
}



@inproceedings{liang2018enhancing,
  title={Enhancing the reliability of out-of-distribution image detection in neural networks},
  author={Liang, Shiyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={6th International Conference on Learning Representations, ICLR 2018},
  year={2018}
}


@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@article{memoli2011gromov,
  title={Gromov--Wasserstein distances and the metric approach to object matching},
  author={M{\'e}moli, Facundo},
  journal={Foundations of computational mathematics},
  volume={11},
  number={4},
  pages={417--487},
  year={2011},
  publisher={Springer}
}

@inproceedings{10.1145/3128572.3140444,
author = {Carlini, Nicholas and Wagner, David},
title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
year = {2017},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {3–14},
series = {AISec ’17}
}

@article{doi:10.1080/01621459.1984.10477105,
author = { Peter J.   Rousseeuw },
title = {Least Median of Squares Regression},
journal = {Journal of the American Statistical Association},
volume = {79},
number = {388},
pages = {871-880},
year  = {1984},
publisher = {Taylor & Francis},
}


@article{10.1023/B:MACH.0000008084.60811.49,
author = {Tax, David M. J. and Duin, Robert P. W.},
title = {Support Vector Data Description},
year = {2004},
issue_date = {January 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1},
journal = {Mach. Learn.},
pages = {45–66},
}

@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}


@misc{fashionproduct,
author = {Param Aggarwal},
title = {Fashion Product Images (Small)},
year = {2019},
howpublished= {\url{https://www.kaggle.com/paramaggarwal/fashion-product-images-small}},
}

@article{xiao2017,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{chensemi,
  title={Semi-Supervised Learning under Class Distribution Mismatch},
  author={Chen, Yanbei and Zhu, Xiatian and Li, Wei and Gong, Shaogang},
  year = {2020}
  
}

@InProceedings{eonss,
author="Wang, Zhongling and Athar, Shahrukh and Wang, Zhou",
title="Blind Quality Assessment of Multiply Distorted Images Using Deep Neural Networks",
booktitle="International Conference on Image Analysis and Recognition",
year="2019",
pages="89--101"
}

@INPROCEEDINGS{resnet,
author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Deep Residual Learning for Image Recognition},
year={2016},
volume={},
number={},
pages={770-778},
keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
doi={10.1109/CVPR.2016.90},
ISSN={1063-6919},
month={June},}

@INPROCEEDINGS{densenet,
author={G. {Huang} and Z. {Liu} and L. v. d. {Maaten} and K. Q. {Weinberger}},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Densely Connected Convolutional Networks},
year={2017},
volume={},
number={},
pages={2261-2269},
keywords={convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation},
doi={10.1109/CVPR.2017.243},
ISSN={1063-6919},
month={July},}

@ARTICLE{lpc,
author={R. {Hassen} and Z. {Wang} and M. M. A. {Salama}},
journal={IEEE Transactions on Image Processing},
title={Image Sharpness Assessment Based on Local Phase Coherence},
year={2013},
volume={22},
number={7},
pages={2798-2810},
month={July},}

@article{tomczak2015cancer,
  title={The Cancer Genome Atlas (TCGA): an immeasurable source of knowledge},
  author={Tomczak, Katarzyna and Czerwi{\'n}ska, Patrycja and Wiznerowicz, Maciej},
  journal={Contemporary oncology},
  volume={19},
  number={1A},
  pages={A68},
  year={2015},
  publisher={Termedia Publishing}
}

@misc{Sedeen,
title = {{Sedeen Viewer from Pathcore}},
howpublished = {\url{https://pathcore.com/sedeen/}}
}

@article{hosseini2019encoding,
  title={Encoding visual sensitivity by maxpol convolution filters for image sharpness assessment},
  author={Hosseini, Mahdi S and Zhang, Yueyang and Plataniotis, Konstantinos N},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={9},
  pages={4510--4525},
  year={2019},
  publisher={IEEE}
}

@misc{dixon2014pathology,
  title={Pathology slide scanner},
  author={Dixon, Arthur Edward},
  year={2014},
  month=nov # "~25",
  publisher={Google Patents},
  note={US Patent 8,896,918}
}

@article{gpc,
  title={No-reference image quality assessment and blind deblurring with sharpness metrics exploiting fourier phase information},
  author={Leclaire, Arthur and Moisan, Lionel},
  journal={Journal of Mathematical Imaging and Vision},
  volume={52},
  number={1},
  pages={145--172},
  year={2015},
  publisher={Springer}
}

@article{mlv,
  title={A fast approach for no-reference image sharpness assessment based on maximum local variation},
  author={Bahrami, Khosro and Kot, Alex C},
  journal={IEEE Signal Processing Letters},
  volume={21},
  number={6},
  pages={751--755},
  year={2014},
  publisher={IEEE}
}

@article{sparish,
  title={Image sharpness assessment by sparse representation},
  author={Li, Leida and Wu, Dong and Wu, Jinjian and Li, Haoliang and Lin, Weisi and Kot, Alex C},
  journal={IEEE Transactions on Multimedia},
  volume={18},
  number={6},
  pages={1085--1097},
  year={2016},
  publisher={IEEE}
}

@article{fqpath,
  title={Focus Quality Assessment of High-Throughput Whole Slide Imaging in Digital Pathology},
  author={Hosseini, Mahdi S and Brawley-Hayes, Jasper AZ and Zhang, Yueyang and Chan, Lyndon and Plataniotis, Konstantinos N and Damaskinos, Savvas},
  journal={IEEE transactions on medical imaging},
  volume={39},
  number={1},
  pages={62--74},
  year={2019},
  publisher={IEEE}
}

@article{topol2019high,
  title={High-performance medicine: the convergence of human and artificial intelligence},
  author={Topol, Eric J},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={44--56},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{gupta2019deep,
  title={Deep learning in image cytometry: a review},
  author={Gupta, Anindya and Harrison, Philip J and Wieslander, H{\aa}kan and Pielawski, Nicolas and Kartasalo, Kimmo and Partel, Gabriele and Solorzano, Leslie and Suveer, Amit and Klemm, Anna H and Spjuth, Ola and others},
  journal={Cytometry Part A},
  volume={95},
  number={4},
  pages={366--380},
  year={2019},
  publisher={Wiley Online Library}
}

@article{yang2018assessing,
  title={Assessing microscope image focus quality with deep learning},
  author={Yang, Samuel J and Berndl, Marc and Ando, D Michael and Barch, Mariya and Narayanaswamy, Arunachalam and Christiansen, Eric and Hoyer, Stephan and Roat, Chris and Hung, Jane and Rueden, Curtis T and others},
  journal={BMC bioinformatics},
  volume={19},
  number={1},
  pages={77},
  year={2018},
  publisher={BioMed Central}
}

@article{janowczyk2019histoqc,
  title={HistoQC: an open-source quality control tool for digital pathology slides},
  author={Janowczyk, Andrew and Zuo, Ren and Gilmore, Hannah and Feldman, Michael and Madabhushi, Anant},
  journal={JCO clinical cancer informatics},
  volume={3},
  pages={1--7},
  year={2019},
  publisher={American Society of Clinical Oncology}
}

@article{pinkard2019deep,
  title={Deep learning for single-shot autofocus microscopy},
  author={Pinkard, Henry and Phillips, Zachary and Babakhani, Arman and Fletcher, Daniel A and Waller, Laura},
  journal={Optica},
  volume={6},
  number={6},
  pages={794--797},
  year={2019},
  publisher={Optical Society of America}
}

@article{mcquin2018cellprofiler,
  title={CellProfiler 3.0: Next-generation image processing for biology},
  author={McQuin, Claire and Goodman, Allen and Chernyshev, Vasiliy and Kamentsky, Lee and Cimini, Beth A and Karhohs, Kyle W and Doan, Minh and Ding, Liya and Rafelski, Susanne M and Thirstrup, Derek and others},
  journal={PLoS biology},
  volume={16},
  number={7},
  year={2018},
  publisher={Public Library of Science}
}

@article{senaras2018deepfocus,
  title={DeepFocus: detection of out-of-focus regions in whole slide digital images using deep learning},
  author={Senaras, Caglar and Niazi, M Khalid Khan and Lozanski, Gerard and Gurcan, Metin N},
  journal={PloS one},
  volume={13},
  number={10},
  year={2018},
  publisher={Public Library of Science}
}

@article{kohlberger2019whole,
  title={Whole-slide image focus quality: Automatic assessment and impact on AI cancer detection},
  author={Kohlberger, Timo and Liu, Yun and Moran, Melissa and Chen, Po-Hsuan Cameron and Brown, Trissia and Hipp, Jason D and Mermel, Craig H and Stumpe, Martin C},
  journal={Journal of Pathology Informatics},
  volume={10},
  year={2019},
  publisher={Wolters Kluwer--Medknow Publications}
}

@article{campanella2018towards,
  title={Towards machine learned quality control: A benchmark for sharpness quantification in digital pathology},
  author={Campanella, Gabriele and Rajanna, Arjun R and Corsale, Lorraine and Sch{\"u}ffler, Peter J and Yagi, Yukako and Fuchs, Thomas J},
  journal={Computerized Medical Imaging and Graphics},
  volume={65},
  pages={142--151},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@misc{stumpe2019focus,
  title={Focus-Weighted, Machine Learning Disease Classifier Error Prediction for Microscope Slide Images},
  author={Stumpe, Martin and Kohlberger, Timo},
  year={2019},
  month=nov # "~7",
  publisher={Google Patents},
  note={US Patent App. 15/972,929}
}

@INPROCEEDINGS{syntheticmaxpol,
author={M. S. {Hosseini} and K. N. {Plataniotis}},
booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
title={Image Sharpness Metric Based on Maxpol Convolution Kernels},
year={2018},
volume={},
number={},
pages={296-300},
keywords={approximation theory;convolution;feature extraction;image representation;image resolution;information loss;noise sensitivity;sharpness scores;no-reference image sharpness metric;visual sensitivity model;image sharpness assessment;MaxPol convolution kernels;approximation;feature extraction;pipeline;image differentials;cutoff frequencies;frequency information;Sensitivity;Visualization;Kernel;Cutoff frequency;Image edge detection;Databases;Correlation;Visual sensitivity;MaxPol convolution kernels;No-reference image sharpness assessment},
doi={10.1109/ICIP.2018.8451488},
ISSN={2381-8549},
month={Oct},}

@article{lopez2013automated,
  title={An automated blur detection method for histological whole slide imaging},
  author={Lopez, Xavier Moles and D'Andrea, Etienne and Barbot, Paul and Bridoux, Anne-Sophie and Rorive, Sandrine and Salmon, Isabelle and Debeir, Olivier and Decaestecker, Christine},
  journal={PloS one},
  volume={8},
  number={12},
  year={2013},
  publisher={Public Library of Science}
}

@article{stacke2019closer,
  title={A Closer Look at Domain Shift for Deep Learning in Histopathology},
  author={Stacke, Karin and Eilertsen, Gabriel and Unger, Jonas and Lundstr{\"o}m, Claes},
  journal={arXiv preprint arXiv:1909.11575},
  year={2019}
}

@article{Tellez2019,
title = {Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology},
journal = {Medical Image Analysis},
volume = {58},
pages = {101544},
year = {2019},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101544},
url = {https://www.sciencedirect.com/science/article/pii/S1361841519300799},
author = {David Tellez and Geert Litjens and Péter Bándi and Wouter Bulten and John-Melle Bokhorst and Francesco Ciompi and Jeroen {van der Laak}},
keywords = {Deep learning, Convolutional neural network, Computational pathology},
}

@article{Bain2005,
  title={Diagnosis from the blood smear.},
  author={B. Bain},
  journal={The New England journal of medicine},
  year={2005},
  volume={353 5},
  pages={
          498-507
        }
}

@article{Ayyappan2020,
author = {Ayyappan, Vinay and Chang, Alex and Zhang, Chi and Paidi, Santosh Kumar and Bordett, Rosalie and Liang, Tiffany and Barman, Ishan and Pandey, Rishikesh},
title = {Identification and Staging of B-Cell Acute Lymphoblastic Leukemia Using Quantitative Phase Imaging and Machine Learning},
journal = {ACS Sensors},
volume = {5},
number = {10},
pages = {3281-3289},
year = {2020},
doi = {10.1021/acssensors.0c01811},
    note ={PMID: 33092347},

URL = { 
        https://doi.org/10.1021/acssensors.0c01811
    
},
eprint = { 
        https://doi.org/10.1021/acssensors.0c01811
    
}
}

@article{Matek2019,
author = {Christian Matek and Simone Schwarz and Karsten Spiekermann and Carsten Marr},
title = {Human-level recognition of blast cells in acute myeloid leukaemia with convolutional neural networks},
journal = {Nat. Mach. Intell.},
volume = {1},
pages = {538-544},
year = {2019}
}

@INPROCEEDINGS{Scotti2011,  author={Labati, Ruggero Donida and Piuri, Vincenzo and Scotti, Fabio},  booktitle={2011 18th IEEE International Conference on Image Processing},   title={All-IDB: The acute lymphoblastic leukemia image database for image processing},   year={2011},  volume={},  number={},  pages={2045-2048},  doi={10.1109/ICIP.2011.6115881}}

@inproceedings{Matek2020,
  title={Robustness evaluation of a Convolutional Neural Network for the classification of single cells in Acute Myeloid Leukemia},
  author={Christian Matek and Carsten Marr},
  booktitle={ICLR 2021, RobustML workshop},
  year={2020}
}


@article{janowczyk_histoqc_2019,
	title = {{HistoQC}: {An} {Open}-{Source} {Quality} {Control} {Tool} for {Digital} {Pathology} {Slides}},
	issn = {2473-4276},
	shorttitle = {{HistoQC}},
	url = {https://ascopubs.org/doi/10.1200/CCI.18.00157},
	doi = {10.1200/CCI.18.00157},
	abstract = {PURPOSE Digital pathology (DP), referring to the digitization of tissue slides, is beginning to change the landscape of clinical diagnostic workﬂows and has engendered active research within the area of computational pathology. One of the challenges in DP is the presence of artefacts and batch effects, unintentionally introduced during both routine slide preparation (eg, staining, tissue folding) and digitization (eg, blurriness, variations in contrast and hue). Manual review of glass and digital slides is laborious, qualitative, and subject to intra- and inter-reader variability. Therefore, there is a critical need for a reproducible automated approach of precisely localizing artefacts to identify slides that need to be reproduced or regions that should be avoided during computational analysis.
METHODS Here we present HistoQC, a tool for rapidly performing quality control to not only identify and delineate artefacts but also discover cohort-level outliers (eg, slides stained darker or lighter than others in the cohort). This open-source tool employs a combination of image metrics (eg, color histograms, brightness, contrast), features (eg, edge detectors), and supervised classiﬁers (eg, pen detection) to identify artefact-free regions on digitized slides. These regions and metrics are presented to the user via an interactive graphical user interface, facilitating artefact detection through real-time visualization and ﬁltering. These same metrics afford users the opportunity to explicitly deﬁne acceptable tolerances for their workﬂows.
RESULTS The output of HistoQC on 450 slides from The Cancer Genome Atlas was reviewed by two pathologists and found to be suitable for computational analysis more than 95\% of the time.
CONCLUSION These results suggest that HistoQC could provide an automated, quantiﬁable, quality control process for identifying artefacts and measuring slide quality, in turn helping to improve both the repeatability and robustness of DP workﬂows.},
	language = {en},
	number = {3},
	urldate = {2020-11-04},
	journal = {JCO Clinical Cancer Informatics},
	author = {Janowczyk, Andrew and Zuo, Ren and Gilmore, Hannah and Feldman, Michael and Madabhushi, Anant},
	month = dec,
	year = {2019},
	pages = {1--7},
	file = {cci.18.00157.pdf:/Users/enricopomarico/switchdrive/Project_ICARO/Bibliography/Articles/Pre-RAW processing perturbations/Hystopath/cci.18.00157.pdf:application/pdf},
}

@article{chatterjee_shailja_artefacts_2014,
	title = {Artefacts in histopathology},
	volume = {18},
	journal = {J. Oral Maxillofac. Pathol.},
	author = {Chatterjee, Shailja},
	year = {2014},
	pages = {111--116},
}

@article{taqi_syed_ahmed_review_2018,
	title = {A review of artifacts in histopathology},
	journal = {J. Oral Maxillofac. Pathol.},
	author = {Taqi, Syed Ahmed and Sami, Syed Abdus and Sami, Lateef Begum and Zaki, Syed Ahmed},
	year = {2018},
	pages = {22--279},
}

@article{campanella_towards_2018,
	title = {Towards machine learned quality control: {A} benchmark for sharpness quantification in digital pathology},
	volume = {65},
	issn = {08956111},
	shorttitle = {Towards machine learned quality control},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611117300800},
	doi = {10.1016/j.compmedimag.2017.09.001},
	abstract = {Pathology is on the verge of a profound change from an analog and qualitative to a digital and quantitative discipline. This change is mostly driven by the high-throughput scanning of microscope slides in modern pathology departments, reaching tens of thousands of digital slides per month. The resulting vast digital archives form the basis of clinical use in digital pathology and allow large scale machine learning in computational pathology.},
	language = {en},
	urldate = {2020-11-05},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Campanella, Gabriele and Rajanna, Arjun R. and Corsale, Lorraine and Schüffler, Peter J. and Yagi, Yukako and Fuchs, Thomas J.},
	month = apr,
	year = {2018},
	pages = {142--151},
	file = {1-s2.0-S0895611117300800-main.pdf:/Users/enricopomarico/switchdrive/Project_ICARO/Bibliography/Articles/Pre-RAW processing perturbations/Focusing issues/1-s2.0-S0895611117300800-main.pdf:application/pdf},
}

@article{senaras_deepfocus_2018,
	title = {{DeepFocus}: {Detection} of out-of-focus regions in whole slide digital images using deep learning},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {{DeepFocus}},
	url = {https://dx.plos.org/10.1371/journal.pone.0205387},
	doi = {10.1371/journal.pone.0205387},
	language = {en},
	number = {10},
	urldate = {2020-11-05},
	journal = {PLOS ONE},
	author = {Senaras, Caglar and Niazi, M. Khalid Khan and Lozanski, Gerard and Gurcan, Metin N.},
	editor = {Lo, Chung-Ming},
	month = oct,
	year = {2018},
	pages = {e0205387},
	file = {DeepFocus_Detection_of_out-of-focus_regions_in_who.pdf:/Users/enricopomarico/switchdrive/Project_ICARO/Bibliography/Articles/Pre-RAW processing perturbations/Focusing issues/DeepFocus_Detection_of_out-of-focus_regions_in_who.pdf:application/pdf},
}

@article{yang_assessing_2018,
	title = {Assessing microscope image focus quality with deep learning},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2087-4},
	doi = {10.1186/s12859-018-2087-4},
	abstract = {Background: Large image datasets acquired on automated microscopes typically have some fraction of low quality, out-of-focus images, despite the use of hardware autofocus systems. Identification of these images using automated image analysis with high accuracy is important for obtaining a clean, unbiased image dataset. Complicating this task is the fact that image focus quality is only well-defined in foreground regions of images, and as a result, most previous approaches only enable a computation of the relative difference in quality between two or more images, rather than an absolute measure of quality.
Results: We present a deep neural network model capable of predicting an absolute measure of image focus on a single image in isolation, without any user-specified parameters. The model operates at the image-patch level, and also outputs a measure of prediction certainty, enabling interpretable predictions. The model was trained on only 384 in-focus Hoechst (nuclei) stain images of U2OS cells, which were synthetically defocused to one of 11 absolute defocus levels during training. The trained model can generalize on previously unseen real Hoechst stain images, identifying the absolute image focus to within one defocus level (approximately 3 pixel blur diameter difference) with 95\% accuracy. On a simpler binary in/out-of-focus classification task, the trained model outperforms previous approaches on both Hoechst and Phalloidin (actin) stain images (F-scores of 0.89 and 0.86, respectively over 0.84 and 0.83), despite only having been presented Hoechst stain images during training. Lastly, we observe qualitatively that the model generalizes to two additional stains, Hoechst and Tubulin, of an unseen cell type (Human MCF-7) acquired on a different instrument.
Conclusions: Our deep neural network enables classification of out-of-focus microscope images with both higher accuracy and greater precision than previous approaches via interpretable patch-level focus and certainty predictions. The use of synthetically defocused images precludes the need for a manually annotated training dataset. The model also generalizes to different image and cell types. The framework for model training and image prediction is available as a free software library and the pre-trained model is available for immediate use in Fiji (ImageJ) and CellProfiler.},
	language = {en},
	number = {1},
	urldate = {2020-11-05},
	journal = {BMC Bioinformatics},
	author = {Yang, Samuel J. and Berndl, Marc and Michael Ando, D. and Barch, Mariya and Narayanaswamy, Arunachalam and Christiansen, Eric and Hoyer, Stephan and Roat, Chris and Hung, Jane and Rueden, Curtis T. and Shankar, Asim and Finkbeiner, Steven and Nelson, Philip},
	month = dec,
	year = {2018},
	pages = {77},
	file = {s12859-018-2087-4.pdf:/Users/enricopomarico/switchdrive/Project_ICARO/Bibliography/Articles/Pre-RAW processing perturbations/Focusing issues/s12859-018-2087-4.pdf:application/pdf},
}

@article{guzman-altamirano_removing_2015,
	title = {Removing lateral chromatic aberration in bright field optical microscopy},
	volume = {23},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/abstract.cfm?URI=oe-23-11-14380},
	doi = {10.1364/OE.23.014380},
	abstract = {We present an efficient alternative to remove lateral chromatic aberration (LCA) in bright field light microscopy images. Our procedure is based on error calibration using time-sequential acquisition at different wavelengths, and error correction through digital image warping. Measurement of the displacements of fiducial marks in the red and green images relative to blue provide calibration factors that are subsequently used in test images to realign color channels digitally. We demonstrate quantitative improvement in the position and boundaries of objects in target slides and in the color content and morphology of specimens in stained biological samples. Our results show a reduction of LCA content below the 0.1\% level.},
	language = {en},
	number = {11},
	urldate = {2020-11-27},
	journal = {Optics Express},
	author = {Guzmán-Altamirano, Miguel and Gutiérrez-Medina, Braulio},
	month = jun,
	year = {2015},
	pages = {14380},
	file = {Guzmán-Altamirano et Gutiérrez-Medina - 2015 - Removing lateral chromatic aberration in bright fi.pdf:/Users/enricopomarico/Zotero/storage/3U4Y49UM/Guzmán-Altamirano et Gutiérrez-Medina - 2015 - Removing lateral chromatic aberration in bright fi.pdf:application/pdf},
}

@article{shrestha_color_2014,
	title = {Color accuracy and reproducibility in whole slide imaging scanners},
	volume = {1},
	issn = {2329-4302},
	url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.1.2.027501},
	doi = {10.1117/1.JMI.1.2.027501},
	language = {en},
	number = {2},
	urldate = {2020-12-04},
	journal = {Journal of Medical Imaging},
	author = {Shrestha, Prarthana and Hulsken, Bas},
	month = jul,
	year = {2014},
	pages = {027501},
	file = {Shrestha et Hulsken - 2014 - Color accuracy and reproducibility in whole slide .pdf:/Users/enricopomarico/Zotero/storage/U2ZGTXVU/Shrestha et Hulsken - 2014 - Color accuracy and reproducibility in whole slide .pdf:application/pdf},
}

@article{revie_current_2016,
	title = {Current problems and perspectives on colour in medical imaging},
	volume = {2016},
	issn = {2470-1173},
	url = {http://www.ingentaconnect.com/content/10.2352/ISSN.2470-1173.2016.20.COLOR-339},
	doi = {10.2352/ISSN.2470-1173.2016.20.COLOR-339},
	abstract = {Medical practitioners are today increasingly likely to make clinical judgements based on looking at an image on a display rather than direct viewing of the subject. Medical imaging has also been moving away from exclusively grayscale modalities towards increasing use of colour. The importance of accurate calibration and reproducible image capture and display has become more apparent, but standards and best practices in this field are still in development. Since 2013, the International Color Consortium has engaged with the medical imaging community to help understand the particular problems encountered and to help develop solutions. Currently the ICC Medical Imaging Working Group is working on a wide range of topics including digital microscopy, medical displays, ophthalmology, medical photography, multispectral imaging, petri dish imaging, dermatology, skin colour measurement, and 3-D imaging for surgery. In this overview, the problems in each of these areas are summarised and the current activity is described.},
	language = {en},
	number = {20},
	urldate = {2020-12-04},
	journal = {Electronic Imaging},
	author = {Revie, W. Craig and Green, Phil},
	month = feb,
	year = {2016},
	pages = {1--5},
	file = {Revie et Green - 2016 - Current problems and perspectives on colour in med.pdf:/Users/enricopomarico/Zotero/storage/ILE96IJV/Revie et Green - 2016 - Current problems and perspectives on colour in med.pdf:application/pdf},
}

@article{cheng_assessing_nodate,
	title = {Assessing color performance of whole‐slide imaging scanners for digital pathology},
	abstract = {The color performance of two commercial whole-slide imaging (WSI) scanners was compared against the ground truth and a hypothetical monochrome scanner. Three biological tissue slides were used to test the WSI scanners. A multispectral imaging system was developed to obtain the color truth of the biological tissue slides at the pixel level. The hypothetical monochrome scanner was derived from the color truth as a lower bound for comparison. The CIEDE2000 formula was used to measure color errors. Results show that color errors generated by the modern commercial WSI scanner, the legacy commercial WSI scanner, and the monochrome WSI scanner are in the range of [8.4, 13.0], [18.0, 26.33], and [17.4, 17.6] ΔE00, respectively. The legacy commercial WSI scanner was outperformed by not only the modern commercial WSI scanner but also by the hypothetical monochrome scanner.},
	language = {en},
	author = {Cheng, Wei-Chung and Saleheen, Firdous and Badano, Aldo},
	pages = {13},
	file = {Cheng et al. - Assessing color performance of whole‐slide imaging.pdf:/Users/enricopomarico/Zotero/storage/T7M8KW3N/Cheng et al. - Assessing color performance of whole‐slide imaging.pdf:application/pdf},
}

@article{clarke_colour_2017,
	title = {Colour in digital pathology: a review},
	volume = {70},
	issn = {03090167},
	shorttitle = {Colour in digital pathology},
	url = {http://doi.wiley.com/10.1111/his.13079},
	doi = {10.1111/his.13079},
	abstract = {Colour is central to the practice of pathology because of the use of coloured histochemical and immunohistochemical stains to visualize tissue features. Our reliance upon histochemical stains and light microscopy has evolved alongside a wide variation in slide colour, with little investigation into the implications of colour variation. However, the introduction of the digital microscope and whole-slide imaging has highlighted the need for further understanding and control of colour. This is because the digitization process itself introduces further colour variation which may affect diagnosis, and image analysis algorithms often use colour or intensity measures to detect or measure tissue features. The US Food and Drug Administration have released recent guidance stating the need to develop a method of controlling colour reproduction throughout the digitization process in whole-slide imaging for primary diagnostic use. This comprehensive review introduces applied basic colour physics and colour interpretation by the human visual system, before discussing the importance of colour in pathology. The process of colour calibration and its application to pathology are also included, as well as a summary of the current guidelines and recommendations regarding colour in digital pathology.},
	language = {en},
	number = {2},
	urldate = {2020-12-04},
	journal = {Histopathology},
	author = {Clarke, Emily L and Treanor, Darren},
	month = jan,
	year = {2017},
	pages = {153--163},
	file = {Clarke et Treanor - 2017 - Colour in digital pathology a review.pdf:/Users/enricopomarico/Zotero/storage/DR229MEX/Clarke et Treanor - 2017 - Colour in digital pathology a review.pdf:application/pdf},
}

@article{yagi_color_2011,
	title = {Color standardization and optimization in {Whole} {Slide} {Imaging}},
	volume = {6},
	issn = {1746-1596},
	url = {https://diagnosticpathology.biomedcentral.com/articles/10.1186/1746-1596-6-S1-S15},
	doi = {10.1186/1746-1596-6-S1-S15},
	abstract = {Introduction: Standardization and validation of the color displayed by digital slides is an important aspect of digital pathology implementation. While the most common reason for color variation is the variance in the protocols and practices in the histology lab, the color displayed can also be affected by variation in capture parameters (for example, illumination and filters), image processing and display factors in the digital systems themselves. Method: We have been developing techniques for color validation and optimization along two paths. The first was based on two standard slides that are scanned and displayed by the imaging system in question. In this approach, one slide is embedded with nine filters with colors selected especially for H\&E stained slides (looking like tiny Macbeth color chart); the specific color of the nine filters were determined in our previous study and modified for whole slide imaging (WSI). The other slide is an H\&E stained mouse embryo. Both of these slides were scanned and the displayed images were compared to a standard. The second approach was based on our previous multispectral imaging research. Discussion: As a first step, the two slide method (above) was used to identify inaccurate display of color and its cause, and to understand the importance of accurate color in digital pathology. We have also improved the multispectral-based algorithm for more consistent results in stain standardization. In near future, the results of the two slide and multispectral techniques can be combined and will be widely available. We have been conducting a series of researches and developing projects to improve image quality to establish Image Quality Standardization. This paper discusses one of most important aspects of image quality – color.},
	language = {en},
	number = {S1},
	urldate = {2020-12-04},
	journal = {Diagnostic Pathology},
	author = {Yagi, Yukako},
	month = dec,
	year = {2011},
	pages = {S15},
	file = {Yagi - 2011 - Color standardization and optimization in Whole Sl.pdf:/Users/enricopomarico/Zotero/storage/XT36NXF8/Yagi - 2011 - Color standardization and optimization in Whole Sl.pdf:application/pdf},
}

@article{bautista_color_2014,
	title = {Color standardization in whole slide imaging using a color calibration slide},
	volume = {5},
	issn = {2153-3539},
	url = {http://www.jpathinformatics.org/text.asp?2014/5/1/4/126153},
	doi = {10.4103/2153-3539.126153},
	abstract = {Background: Color consistency in histology images is still an issue in digital pathology. Different imaging systems reproduced the colors of a histological slide differently. Materials and Methods: Color correction was implemented using the color information of the nine color patches of a color calibration slide.The inherent spectral colors of these patches along with their scanned colors were used to derive a color correction matrix whose coefficients were used to convert the pixels’ colors to their target colors. Results: There was a significant reduction in the CIELAB color difference, between images of the same H \& E histological slide produced by two different whole slide scanners by 3.42 units, P {\textless} 0.001 at 95\% confidence level. Conclusion: Color variations in histological images brought about by whole slide scanning can be effectively normalized with the use of the color calibration slide.},
	language = {en},
	number = {1},
	urldate = {2020-12-04},
	journal = {Journal of Pathology Informatics},
	author = {Bautista, PinkyA and Hashimoto, Noriaki and Yagi, Yukako},
	year = {2014},
	pages = {4},
	file = {Bautista et al. - 2014 - Color standardization in whole slide imaging using.pdf:/Users/enricopomarico/Zotero/storage/TQ88CTA5/Bautista et al. - 2014 - Color standardization in whole slide imaging using.pdf:application/pdf},
}

@article{revie_color_2014,
	title = {Color {Management} in {Digital} {Pathology}},
	volume = {2014},
	issn = {2210-7177, 2210-7185},
	url = {http://www.hindawi.com/journals/acp/2014/652757/},
	doi = {10.1155/2014/652757},
	abstract = {In digital microscopes and whole slide imaging systems, images of slides are captured, transmitted, and reproduced on a computer display. In order to allow pathologists to interpret these images accurately and efficiently, it is important that colors from the slides are displayed in a consistent and reliable fashion.},
	language = {en},
	urldate = {2020-12-04},
	journal = {Analytical Cellular Pathology},
	author = {Revie, W. Craig and Shires, Mike and Jackson, Pete and Brettle, David and Cochrane, Ravinder and Treanor, Darren},
	year = {2014},
	pages = {1--2},
	file = {Revie et al. - 2014 - Color Management in Digital Pathology.pdf:/Users/enricopomarico/Zotero/storage/3XCI5V6M/Revie et al. - 2014 - Color Management in Digital Pathology.pdf:application/pdf},
}

@article{noauthor_notitle_nodate,
}

@inproceedings{bongiorno_spectral_2013,
	address = {Burlingame, California, USA},
	title = {Spectral characterization of {COTS} {RGB} cameras using a linear variable edge filter},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2001460},
	doi = {10.1117/12.2001460},
	urldate = {2020-12-04},
	author = {Bongiorno, Daniel L. and Bryson, Mitch and Dansereau, Donald G. and Williams, Stefan B.},
	editor = {Sampat, Nitin and Battiato, Sebastiano},
	month = jan,
	year = {2013},
	pages = {86600N},
}

@article{noauthor_notitle_nodate-1,
}

@article{tani_color_2012,
	title = {Color {Standardization} {Method} and {System} for {Whole} {Slide} {Imaging} {Based} on {Spectral} {Sensing}},
	volume = {35},
	issn = {2210-7177, 2210-7185},
	url = {http://www.hindawi.com/journals/acp/2012/154735/},
	doi = {10.1155/2012/154735},
	abstract = {In the ﬁeld of whole slide imaging, the imaging device or staining process cause color variations for each slide that affect the result of image analysis made by pathologist. In order to stabilize the analysis, we developed a color standardization method and system as described below.},
	language = {en},
	number = {2},
	urldate = {2020-12-04},
	journal = {Analytical Cellular Pathology},
	author = {Tani, Shinsuke and Fukunaga, Yasuhiro and Shimizu, Saori and Fukunishi, Munenori and Ishii, Kensuke and Tamiya, Kosei},
	year = {2012},
	pages = {107--115},
}

@article{tani_color_2012-1,
	title = {Color {Standardization} {Method} and {System} for {Whole} {Slide} {Imaging} {Based} on {Spectral} {Sensing}},
	volume = {35},
	issn = {2210-7177, 2210-7185},
	url = {http://www.hindawi.com/journals/acp/2012/154735/},
	doi = {10.1155/2012/154735},
	abstract = {In the ﬁeld of whole slide imaging, the imaging device or staining process cause color variations for each slide that affect the result of image analysis made by pathologist. In order to stabilize the analysis, we developed a color standardization method and system as described below.},
	language = {en},
	number = {2},
	urldate = {2020-12-04},
	journal = {Analytical Cellular Pathology},
	author = {Tani, Shinsuke and Fukunaga, Yasuhiro and Shimizu, Saori and Fukunishi, Munenori and Ishii, Kensuke and Tamiya, Kosei},
	year = {2012},
	pages = {107--115},
	file = {Tani et al. - 2012 - Color Standardization Method and System for Whole .pdf:/Users/enricopomarico/Zotero/storage/CBMVK32L/Tani et al. - 2012 - Color Standardization Method and System for Whole .pdf:application/pdf},
}

@inproceedings{cheng_assessing_2013,
	address = {Lake Buena Vista (Orlando Area), Florida, USA},
	title = {Assessing color reproducibility of whole-slide imaging scanners},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2007215},
	doi = {10.1117/12.2007215},
	urldate = {2020-12-04},
	author = {Cheng, Wei-Chung and Keay, Tyler and O'Flaherty, Neil and Wang, Joel and Ivansky, Adam and Gavrielides, Marios A. and Gallas, Brandon D. and Badano, Aldo},
	editor = {Gurcan, Metin N. and Madabhushi, Anant},
	month = mar,
	year = {2013},
	pages = {86760O},
}

@misc{russakovsky2015imagenet,
      title={ImageNet Large Scale Visual Recognition Challenge}, 
      author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
      year={2015},
      eprint={1409.0575},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{jaroensri2019generating,
  title={Generating training data for denoising real rgb images via camera pipeline simulation},
  author={Jaroensri, Ronnachai and Biscarrat, Camille and Aittala, Miika and Durand, Fr{\'e}do},
  journal={arXiv preprint arXiv:1904.08825},
  year={2019}
}

@article{10.1137/0916069,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
title = {A Limited Memory Algorithm for Bound Constrained Optimization},
year = {1995},
issue_date = {Sept. 1995},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {16},
number = {5},
issn = {1064-8275},
url = {https://doi.org/10.1137/0916069},
doi = {10.1137/0916069},
journal = {SIAM J. Sci. Comput.},
month = sep,
pages = {1190–1208},
numpages = {19},
keywords = {nonlinear optimization, quasi-Newton method, limited memory method, large-scale optimization, bound constrained optimization}
}

@article{longanbach2016rodak,
  title={Rodak’s hematology: Clinical principles and applications},
  author={Longanbach, S and Miers, MK and Keohane, EM and Smith, LJ and Walenga, JM},
  year={2016},
  publisher={Elsevier Saunders, St. Louis}
}

@article{ronchetti2020torchradon,
  title={Torchradon: Fast differentiable routines for computed tomography},
  author={Ronchetti, Matteo},
  journal={arXiv preprint arXiv:2009.14788},
  year={2020}
}

@inproceedings{subbaswamy2021evaluating,
  title={Evaluating Model Robustness and Stability to Dataset Shift},
  author={Subbaswamy, Adarsh and Adams, Roy and Saria, Suchi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2611--2619},
  year={2021},
  organization={PMLR}
}

@inproceedings{wilds2021,
  title = {{WILDS}: A Benchmark of in-the-Wild Distribution Shifts},
  author = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2021}
}

@inproceedings{beery2018recognition,
  title={Recognition in terra incognita},
  author={Beery, Sara and Van Horn, Grant and Perona, Pietro},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={456--473},
  year={2018}
}

@inproceedings{goel2020model,
  title={Model Patching: Closing the Subgroup Performance Gap with Data Augmentation},
  author={Goel, Karan and Gu, Albert and Li, Yixuan and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{49953,
title	= {"Everyone wants to do the model work, not the data work": Data Cascades in High-Stakes AI},
author	= {Nithya Sambasivan and Shivani Kapania and Hannah Highfill and Diana Akrong and Praveen Kumar Paritosh and Lora Mois Aroyo},
year	= {2021}
}


@article{azulay2019deep,
  title={Why do deep convolutional networks generalize so poorly to small image transformations?},
  author={Azulay, Aharon and Weiss, Yair},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--25},
  year={2019}
}

@INPROCEEDINGS{8038465,  author={Dodge, Samuel and Karam, Lina},  booktitle={2017 26th International Conference on Computer Communication and Networks (ICCCN)},   title={A Study and Comparison of Human and Deep Learning Recognition Performance under Visual Distortions},   year={2017},  volume={},  number={},  pages={1-7},  doi={10.1109/ICCCN.2017.8038465}}

@article{10.1145/3446791,
author = {Tseng, Ethan and Mosleh, Ali and Mannan, Fahim and St-Arnaud, Karl and Sharma, Avinash and Peng, Yifan and Braun, Alexander and Nowrouzezahrai, Derek and Lalonde, Jean-Fran\c{c}ois and Heide, Felix},
title = {Differentiable Compound Optics and Processing Pipeline Optimization for End-to-End Camera Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3446791},
doi = {10.1145/3446791},
journal = {ACM Trans. Graph.},
month = jun,
articleno = {18},
numpages = {19},
keywords = {Compound optics, computational imaging, optics design, end-to-end image processing, deep learning}
}

@misc{maier2021known,
      title={Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future}, 
      author={Andreas Maier and Harald Köstler and Marco Heisig and Patrick Krauss and Seung Hee Yang},
      year={2021},
      eprint={2108.04543},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{taori2020measuring,
    title={Measuring Robustness to Natural Distribution Shifts in Image Classification},
    author={Rohan Taori and Achal Dave and Vaishaal Shankar and Nicholas Carlini and Benjamin Recht and Ludwig Schmidt},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2020},
    url={https://arxiv.org/abs/2007.00644},
}

@inbook{10.1145/3313831.3376718,
author = {Beede, Emma and Baylor, Elizabeth and Hersch, Fred and Iurchenko, Anna and Wilcox, Lauren and Ruamviboonsuk, Paisan and Vardoulakis, Laura M.},
title = {A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376718},
abstract = {Deep learning algorithms promise to improve clinician workflows and patient outcomes.
However, these gains have yet to be fully demonstrated in real world clinical settings.
In this paper, we describe a human-centered study of a deep learning system used in
clinics for the detection of diabetic eye disease. From interviews and observation
across eleven clinics in Thailand, we characterize current eye-screening workflows,
user expectations for an AI-assisted screening process, and post-deployment experiences.
Our findings indicate that several socio-environmental factors impact model performance,
nursing workflows, and the patient experience. We draw on these findings to reflect
on the value of conducting human-centered evaluative research alongside prospective
evaluations of model accuracy.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}

@article{MAIMAITIJIANG2020111599,
title = {Soybean yield prediction from UAV using multimodal data fusion and deep learning},
journal = {Remote Sensing of Environment},
volume = {237},
pages = {111599},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.111599},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719306194},
author = {Maitiniyazi Maimaitijiang and Vasit Sagan and Paheding Sidike and Sean Hartling and Flavio Esposito and Felix B. Fritschi},
keywords = {Remote sensing, Yield prediction, Multimodality, Data fusion, Deep learning, Phenotyping, Spatial autocorrelation},
abstract = {Preharvest crop yield prediction is critical for grain policy making and food security. Early estimation of yield at field or plot scale also contributes to high-throughput plant phenotyping and precision agriculture. New developments in Unmanned Aerial Vehicle (UAV) platforms and sensor technology facilitate cost-effective data collection through simultaneous multi-sensor/multimodal data collection at very high spatial and spectral resolutions. The objective of this study is to evaluate the power of UAV-based multimodal data fusion using RGB, multispectral and thermal sensors to estimate soybean (Glycine max) grain yield within the framework of Deep Neural Network (DNN). RGB, multispectral, and thermal images were collected using a low-cost multi-sensory UAV from a test site in Columbia, Missouri, USA. Multimodal information, such as canopy spectral, structure, thermal and texture features, was extracted and combined to predict crop grain yield using Partial Least Squares Regression (PLSR), Random Forest Regression (RFR), Support Vector Regression (SVR), input-level feature fusion based DNN (DNN-F1) and intermediate-level feature fusion based DNN (DNN-F2). The results can be summarized in three messages: (1) multimodal data fusion improves the yield prediction accuracy and is more adaptable to spatial variations; (2) DNN-based models improve yield prediction model accuracy: the highest accuracy was obtained by DNN-F2 with an R2 of 0.720 and a relative root mean square error (RMSE%) of 15.9%; (3) DNN-based models were less prone to saturation effects, and exhibited more adaptive performance in predicting grain yields across the Dwight, Pana and AG3432 soybean genotypes in our study. Furthermore, DNN-based models demonstrated consistent performance over space with less spatial dependency and variations. This study indicates that multimodal data fusion using low-cost UAV within a DNN framework can provide a relatively accurate and robust estimation of crop yield, and deliver valuable insight for high-throughput phenotyping and crop field management with high spatial precision.}
}

@inproceedings{48768,
title	= {A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy},
author	= {Emma Beede and Elizabeth Baylor and Fred Hersch and Anna Iurchenko and Lauren Wilcox and Dr. Paisan Raumviboonsuk and Laura Vardoulakis},
year	= {2020},
URL	= {https://dl.acm.org/doi/abs/10.1145/3313831.3376718}
}



@misc{pascal-voc-2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"}	

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@InProceedings{10.1007/978-3-030-59722-1_39,
author="Wang, Zhongling
and Hosseini, Mahdi S.
and Miles, Adyn
and Plataniotis, Konstantinos N.
and Wang, Zhou",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="FocusLiteNN: High Efficiency Focus Quality Assessment for Digital Pathology",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="403--413",
abstract="Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.",
isbn="978-3-030-59722-1"
}

@InProceedings{10.1007/978-3-030-59722-1_36,
author="Zhang, Songtao
and Ni, Qingwen
and Li, Bing
and Jiang, Shan
and Cai, Wenyu
and Chen, Hang
and Luo, Lin",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Corruption-Robust Enhancement of Deep Neural Networks for Classification of Peripheral Blood Smear Images",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="372--381",
abstract="Deep learning emerges as a promising technology for automated peripheral blood smear analysis and hematologic diagnosis. A big challenge to deep learning model is the accuracy drop when facing image corruptions caused by different smear preparation and digitalization operations. In order to serve the real applications of peripheral blood analysis, a practical deep learning classifier needs to be robust against a wide range of corruptions. In this paper, we first investigate the comprehensive corruption types for peripheral blood smear image and establish the benchmark dataset Smear-C simulating the real physical cause factors. Then we propose a novel method SmearRobust which can easily fit into the existing neural networks and improve the robustness. Experimental results show that SmearRobust can significantly enhance the robustness upon Smear-C dataset. Furthermore, the proposed corruption simulation algorithms and the robust learning method can be potentially applied to bone marrow smear and general pathology areas.",
isbn="978-3-030-59722-1"
}

@article{CAMPANELLA2018142,
title = "Towards machine learned quality control: A benchmark for sharpness quantification in digital pathology",
journal = "Computerized Medical Imaging and Graphics",
volume = "65",
pages = "142 - 151",
year = "2018",
note = "Advances in Biomedical Image Processing",
issn = "0895-6111",
doi = "https://doi.org/10.1016/j.compmedimag.2017.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S0895611117300800",
author = "Gabriele Campanella and Arjun R. Rajanna and Lorraine Corsale and Peter J. Schüffler and Yukako Yagi and Thomas J. Fuchs",
keywords = "Computational pathology, Digital pathology, Quality control, Machine learning, Deep learning, Quantitative blur detection",
abstract = "Pathology is on the verge of a profound change from an analog and qualitative to a digital and quantitative discipline. This change is mostly driven by the high-throughput scanning of microscope slides in modern pathology departments, reaching tens of thousands of digital slides per month. The resulting vast digital archives form the basis of clinical use in digital pathology and allow large scale machine learning in computational pathology. One of the most crucial bottlenecks of high-throughput scanning is quality control (QC). Currently, digital slides are screened manually to detected out-of-focus regions, to compensate for the limitations of scanner software. We present a solution to this problem by introducing a benchmark dataset for blur detection, an in-depth comparison of state-of-the art sharpness descriptors and their prediction performance within a random forest framework. Furthermore, we show that convolution neural networks, like residual networks, can be used to train blur detectors from scratch. We thoroughly evaluate the accuracy of feature based and deep learning based approaches for sharpness classification (99.74% accuracy) and regression (MSE 0.004) and additionally compare them to domain experts in a comprehensive human perception study. Our pipeline outputs spacial heatmaps enabling to quantify and localize blurred areas on a slide. Finally, we tested the proposed framework in the clinical setting and demonstrate superior performance over the state-of-the-art QC pipeline comprising commercial software and human expert inspection by reducing the error rate from 17% to 4.7%."
}

@article{farahani2015whole,
  title={Whole slide imaging in pathology: advantages, limitations, and emerging perspectives},
  author={Farahani, Navid and Parwani, Anil V and Pantanowitz, Liron},
  journal={Pathol Lab Med Int},
  volume={7},
  number={23-33},
  pages={4321},
  year={2015}
}

@article{Taqi2018ARO,
  title={A review of artifacts in histopathology},
  author={S. Taqi and S. Sami and Lateef Begum Sami and S. Zaki},
  journal={Journal of Oral and Maxillofacial Pathology : JOMFP},
  year={2018},
  volume={22},
  pages={279 - 279}
}

@article{datasheet,
  author    = {Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III and Kate Crawford},
  title     = {Datasheets for Datasets},
  year      = {2020},
  url       = {https://arxiv.org/pdf/1803.09010},
  archivePrefix = {arXiv},
  eprint    = {1803.09010},
}



%%% Kurt

@inproceedings{phan2021adversarial,
  title={Adversarial Imaging Pipelines},
  author={Phan, Buu and Mannan, Fahim and Heide, Felix},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16051--16061},
  year={2021}
}
@ARTICLE{SSIM,
       author = {{Wang}, Z. and {Bovik}, A.~C. and {Sheikh}, H.~R. and {Simoncelli}, E.~P.},
        title = "{Image Quality Assessment: From Error Visibility to Structural Similarity}",
      journal = {IEEE Transactions on Image Processing},
         year = 2004,
        month = apr,
       volume = {13},
       number = {4},
        pages = {600-612},
          doi = {10.1109/TIP.2003.819861},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004ITIP...13..600W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{carlini2017evaluating,
      title={Towards Evaluating the Robustness of Neural Networks}, 
      author={Nicholas Carlini and David Wagner},
      year={2017},
      eprint={1608.04644},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{syu2018learning,
  title={Learning deep convolutional networks for demosaicing},
  author={Syu, Nai-Sheng and Chen, Yu-Sheng and Chuang, Yung-Yu},
  journal={arXiv preprint arXiv:1802.03769},
  year={2018}
}
@misc{ratnasingam2019deep,
      title={Deep Camera: A Fully Convolutional Neural Network for Image Signal Processing}, 
      author={Sivalogeswaran Ratnasingam},
      year={2019},
      eprint={1908.09191},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}
@misc{syu2018learning,
      title={Learning Deep Convolutional Networks for Demosaicing}, 
      author={Nai-Sheng Syu and Yu-Sheng Chen and Yung-Yu Chuang},
      year={2018},
      eprint={1802.03769},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
% Generating raw data

@inproceedings{plotz2017benchmarking,
  title={Benchmarking denoising algorithms with real photographs},
  author={Plotz, Tobias and Roth, Stefan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1586--1595},
  year={2017}
}

% Demosaicing

@article{losson2010comparison,
  title={Comparison of color demosaicing methods},
  author={Losson, Olivier and Macaire, Ludovic and Yang, Yanqin},
  journal={Advances in Imaging and Electron Physics},
  volume={162},
  pages={173--265},
  year={2010},
  publisher={Elsevier}
}


@inproceedings{malvar2004high,
  title={High-quality linear interpolation for demosaicing of Bayer-patterned color images},
  author={Malvar, Henrique S and He, Li-wei and Cutler, Ross},
  booktitle={2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={3},
  pages={iii--485},
  year={2004},
  organization={IEEE}
}

@article{menon2006demosaicing,
  title={Demosaicing with directional filtering and a posteriori decision},
  author={Menon, Daniele and Andriani, Stefano and Calvagno, Giancarlo},
  journal={IEEE Transactions on Image Processing},
  volume={16},
  number={1},
  pages={132--141},
  year={2006},
  publisher={IEEE}
}

@inproceedings{li2008image,
  title={Image demosaicing: A systematic survey},
  author={Li, Xin and Gunturk, Bahadir and Zhang, Lei},
  booktitle={Visual Communications and Image Processing 2008},
  volume={6822},
  pages={68221J},
  year={2008},
  organization={International Society for Optics and Photonics}
}

% Denoising


@article{goyal2020image,
  title={Image denoising review: From classical to state-of-the-art approaches},
  author={Goyal, Bhawna and Dogra, Ayush and Agrawal, Sunil and Sohi, BS and Sharma, Apoorav},
  journal={Information Fusion},
  volume={55},
  pages={220--244},
  year={2020},
  publisher={Elsevier}
}

@article{tian2020deep,
  title={Deep learning on image denoising: An overview},
  author={Tian, Chunwei and Fei, Lunke and Zheng, Wenxian and Xu, Yong and Zuo, Wangmeng and Lin, Chia-Wen},
  journal={Neural Networks},
  year={2020},
  publisher={Elsevier}
}

% Color analysis

@article{bankhead2014analyzing,
  title={Analyzing fluorescence microscopy images with ImageJ},
  author={Bankhead, Peter},
  journal={ImageJ},
  volume={1},
  number={195},
  pages={10--1109},
  year={2014}
}

@book{hunt2011measuring,
  title={Measuring colour},
  author={Hunt, Robert William Gainer and Pointer, Michael R},
  year={2011},
  publisher={John Wiley \& Sons}
}

@book{rowlands2017physics,
  title={Physics of digital photography},
  author={Rowlands, Andy},
  year={2017},
  publisher={IOP Publishing}
}

@misc{bayer1976color,
  title={Color imaging array},
  author={Bayer, Bryce E},
  year={1976},
  month=jul # "~20",
  publisher={Google Patents},
  note={US Patent 3,971,065}
}

% Raw for Deep Learning

@inproceedings{plotz2017benchmarking,
  title={Benchmarking denoising algorithms with real photographs},
  author={Plotz, Tobias and Roth, Stefan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1586--1595},
  year={2017}
}

@article{zhou2019awgn,
  title={When awgn-based denoiser meets real noises},
  author={Zhou, Yuqian and Jiao, Jianbo and Huang, Haibin and Wang, Yang and Wang, Jue and Shi, Honghui and Huang, Thomas},
  journal={arXiv preprint arXiv:1904.03485},
  year={2019}
}

@article{DBLP:journals/corr/abs-1811-11127,
  author    = {Tim Brooks and
               Ben Mildenhall and
               Tianfan Xue and
               Jiawen Chen and
               Dillon Sharlet and
               Jonathan T. Barron},
  title     = {Unprocessing Images for Learned Raw Denoising},
  journal   = {CoRR},
  volume    = {abs/1811.11127},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.11127},
  archivePrefix = {arXiv},
  eprint    = {1811.11127},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-11127.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{chen2018learning,
      title={Learning to See in the Dark}, 
      author={Chen Chen and Qifeng Chen and Jia Xu and Vladlen Koltun},
      year={2018},
      eprint={1805.01934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dermatologytimes,
  title = {Dermatology Times September 2020},
  howpublished = {\url{https://cdn.sanity.io/files/0vv8moc6/dermatologytimes/4ba31530532b36aaeb80506db61bb5691d841d06.pdf}},
  note = {Accessed: 2021-08-28}
}

@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={nature},
  volume={542},
  number={7639},
  pages={115--118},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{hendrycks2019robustness,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Dan Hendrycks and Thomas Dietterich},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@book{palmer1999vision,
  title={Vision science: Photons to phenomenology},
  author={Palmer, Stephen E},
  year={1999},
  publisher={MIT press}
}
